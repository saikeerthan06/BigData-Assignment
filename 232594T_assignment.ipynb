{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c716d114",
   "metadata": {},
   "source": [
    "# EGT305 Big Data Assignment - Developed & Created by Sai Keerthan (232594T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d42e3",
   "metadata": {},
   "source": [
    "**Business Context**:\n",
    "Government organisation would like to know the status of the job market in the country. \n",
    "\n",
    "**Project Objectives**:\n",
    "Tasked me with: \n",
    "1. exploring the data from a survey and provide findings to the government organisation\n",
    "2. Building a ML Model to predict the salary of a person based on his features. \n",
    "\n",
    "**Dataset Provided**:\n",
    "1. `Employee_dataset.csv`\n",
    "2. `Employee_salaries.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c685cd",
   "metadata": {},
   "source": [
    "## Import Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20adfaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system level imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# data manipulation and visualization libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px # might not use due to size of the dataset\n",
    "\n",
    "# modelling | Machine Learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "# --------------------------------------------------------------\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "\n",
    "# supress any warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") # filter out the warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf3a05b",
   "metadata": {},
   "source": [
    "## Initialise the CWD and Dataset Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b088d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/saikeerthan/NYP-AI/Year3/Big_Data/third_assignment\n"
     ]
    }
   ],
   "source": [
    "# get the current working directory and print it out \n",
    "\n",
    "print(f\"Current Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d7bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate both the dataset paths \n",
    "\n",
    "employee_df = os.path.join(os.getcwd(), \"Employee_dataset.csv\")\n",
    "salary_df = os.path.join(os.getcwd(), \"Employee_salaries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfb9d458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee Dataset Path: /Users/saikeerthan/NYP-AI/Year3/Big_Data/third_assignment/Employee_dataset.csv\n",
      "Salary Dataset Path: /Users/saikeerthan/NYP-AI/Year3/Big_Data/third_assignment/Employee_salaries.csv\n"
     ]
    }
   ],
   "source": [
    "if employee_df and salary_df:\n",
    "    print(f\"Employee Dataset Path: {employee_df}\")\n",
    "    print(f\"Salary Dataset Path: {salary_df}\")\n",
    "else:\n",
    "    print(\"Dataset paths are not set correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7205bc1b",
   "metadata": {},
   "source": [
    "The dataset paths have been correctly instantiated, we will now proceed onto the preliminary Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2fa4b6",
   "metadata": {},
   "source": [
    "## Non-PySpark Data Cleaning & Modelling:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e463e9",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec1a1cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow pandas to read df \n",
    "employee_df = pd.read_csv(employee_df)\n",
    "salary_df = pd.read_csv(salary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a2612b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobId</th>\n",
       "      <th>companyId</th>\n",
       "      <th>jobRole</th>\n",
       "      <th>education</th>\n",
       "      <th>major</th>\n",
       "      <th>industry</th>\n",
       "      <th>yearsExperience</th>\n",
       "      <th>distanceFromCBD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JOB1362684407687</td>\n",
       "      <td>COMP37</td>\n",
       "      <td>CFO</td>\n",
       "      <td>MASTERS</td>\n",
       "      <td>MATH</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>10.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JOB1362684407688</td>\n",
       "      <td>COMP19</td>\n",
       "      <td>CEO</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>WEB</td>\n",
       "      <td>3.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>JOB1362685407682</td>\n",
       "      <td>COMP56</td>\n",
       "      <td>VICE_PRESIDENT</td>\n",
       "      <td>BACHELORS</td>\n",
       "      <td>CHEMISTRY</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>19.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>JOB1362685407683</td>\n",
       "      <td>COMP24</td>\n",
       "      <td>CTO</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>12.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>JOB1362685407684</td>\n",
       "      <td>COMP23</td>\n",
       "      <td>JUNIOR</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>16.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>JOB1362685407685</td>\n",
       "      <td>COMP3</td>\n",
       "      <td>CFO</td>\n",
       "      <td>MASTERS</td>\n",
       "      <td>NONE</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>JOB1362685407686</td>\n",
       "      <td>COMP59</td>\n",
       "      <td>JUNIOR</td>\n",
       "      <td>BACHELORS</td>\n",
       "      <td>NONE</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   jobId companyId         jobRole    education      major  \\\n",
       "0       JOB1362684407687    COMP37             CFO      MASTERS       MATH   \n",
       "1       JOB1362684407688    COMP19             CEO  HIGH_SCHOOL       NONE   \n",
       "2                    NaN       NaN             NaN          NaN        NaN   \n",
       "3                    NaN       NaN             NaN          NaN        NaN   \n",
       "4                    NaN       NaN             NaN          NaN        NaN   \n",
       "...                  ...       ...             ...          ...        ...   \n",
       "999995  JOB1362685407682    COMP56  VICE_PRESIDENT    BACHELORS  CHEMISTRY   \n",
       "999996  JOB1362685407683    COMP24             CTO  HIGH_SCHOOL       NONE   \n",
       "999997  JOB1362685407684    COMP23          JUNIOR  HIGH_SCHOOL       NONE   \n",
       "999998  JOB1362685407685     COMP3             CFO      MASTERS       NONE   \n",
       "999999  JOB1362685407686    COMP59          JUNIOR    BACHELORS       NONE   \n",
       "\n",
       "         industry  yearsExperience  distanceFromCBD  \n",
       "0          HEALTH             10.0             83.0  \n",
       "1             WEB              3.0             73.0  \n",
       "2             NaN              NaN              NaN  \n",
       "3             NaN              NaN              NaN  \n",
       "4             NaN              NaN              NaN  \n",
       "...           ...              ...              ...  \n",
       "999995     HEALTH             19.0             94.0  \n",
       "999996    FINANCE             12.0             35.0  \n",
       "999997  EDUCATION             16.0             81.0  \n",
       "999998     HEALTH              6.0              5.0  \n",
       "999999  EDUCATION             20.0             11.0  \n",
       "\n",
       "[1000000 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show employee df \n",
    "\n",
    "employee_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0d682bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current size of the Employee Dataset:  (1000000, 8)\n",
      "\n",
      "\n",
      "Current Dtypes of the Employee Dataset: \n",
      " jobId               object\n",
      "companyId           object\n",
      "jobRole             object\n",
      "education           object\n",
      "major               object\n",
      "industry            object\n",
      "yearsExperience    float64\n",
      "distanceFromCBD    float64\n",
      "dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the data about the employee df \n",
    "\n",
    "print(\"Current size of the Employee Dataset: \", employee_df.shape)\n",
    "print(\"\\n\")\n",
    "print(\"Current Dtypes of the Employee Dataset: \\n\", employee_df.dtypes)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6043784",
   "metadata": {},
   "source": [
    "We will do the same for `Employee_salaries.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b1ae0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobId</th>\n",
       "      <th>salaryInThousands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JOB1362684407687</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JOB1362684407688</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JOB1362684407689</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JOB1362684407690</td>\n",
       "      <td>142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB1362684407691</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>JOB1362685407682</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>JOB1362685407683</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>JOB1362685407684</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>JOB1362685407685</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>JOB1362685407686</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   jobId  salaryInThousands\n",
       "0       JOB1362684407687              130.0\n",
       "1       JOB1362684407688              101.0\n",
       "2       JOB1362684407689              137.0\n",
       "3       JOB1362684407690              142.0\n",
       "4       JOB1362684407691              163.0\n",
       "...                  ...                ...\n",
       "999995  JOB1362685407682               88.0\n",
       "999996  JOB1362685407683              160.0\n",
       "999997  JOB1362685407684               64.0\n",
       "999998  JOB1362685407685              149.0\n",
       "999999  JOB1362685407686               88.0\n",
       "\n",
       "[1000000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68688e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current size of the Salary Dataset:  (1000000, 2)\n",
      "\n",
      "\n",
      "Current Dtypes of the Salary Dataset: \n",
      " jobId                 object\n",
      "salaryInThousands    float64\n",
      "dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the data about the employee df \n",
    "\n",
    "print(\"Current size of the Salary Dataset: \", salary_df.shape)\n",
    "print(\"\\n\")\n",
    "print(\"Current Dtypes of the Salary Dataset: \\n\", salary_df.dtypes)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33372ea",
   "metadata": {},
   "source": [
    "We will have to merge the datasets together, however, before we merge, we will clean each data individually, and then merge together as we do not want to bring out the unclean data and merge them together. The process will flow like this: \n",
    "\n",
    "1. Basic Cleaning for each dataset first \n",
    "2. More complicated cleaning (if needed) on the merged dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c59c84",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097e1d3a",
   "metadata": {},
   "source": [
    "#### `Employee_dataset.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56170d53",
   "metadata": {},
   "source": [
    "#### Missing Values & Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9709e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in Employee Dataset: \n",
      " jobId              105\n",
      "companyId          148\n",
      "jobRole            165\n",
      "education          186\n",
      "major              207\n",
      "industry           214\n",
      "yearsExperience    198\n",
      "distanceFromCBD    166\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Sum of the missing values in the Employee Dataset: 1389\n"
     ]
    }
   ],
   "source": [
    "# print out the missing values in the employee df\n",
    "print(\"Missing Values in Employee Dataset: \\n\", employee_df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "print(f\"Sum of the missing values in the Employee Dataset: {employee_df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69c6da9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Missing Values in Employee Dataset: \n",
      " jobId              0.0105\n",
      "companyId          0.0148\n",
      "jobRole            0.0165\n",
      "education          0.0186\n",
      "major              0.0207\n",
      "industry           0.0214\n",
      "yearsExperience    0.0198\n",
      "distanceFromCBD    0.0166\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# calculate the percentage of missing values against the entire size of the df \n",
    "missing_percentage = (employee_df.isnull().sum() / employee_df.shape[0]) * 100\n",
    "print(\"Percentage of Missing Values in Employee Dataset: \\n\", missing_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee5d12",
   "metadata": {},
   "source": [
    "Since the loss of data is negligible, we will drop these missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed40ddbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in Employee Dataset after dropping: \n",
      " jobId              0\n",
      "companyId          0\n",
      "jobRole            0\n",
      "education          0\n",
      "major              0\n",
      "industry           0\n",
      "yearsExperience    0\n",
      "distanceFromCBD    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Sum of the missing values in the Employee Dataset after dropping: 0\n"
     ]
    }
   ],
   "source": [
    "# drop missing values in employee df \n",
    "employee_df.dropna(inplace=True)\n",
    "\n",
    "# print out the missing values in the employee df after dropping the missing values\n",
    "print(\"Missing Values in Employee Dataset after dropping: \\n\", employee_df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "print(f\"Sum of the missing values in the Employee Dataset after dropping: {employee_df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1349793f",
   "metadata": {},
   "source": [
    "The dataset now does not have any missing values, we will no move onto seeing if there are any duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ef45f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows in the Employee Dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates in the employee df\n",
    "duplicates = employee_df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows in the Employee Dataset: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0769c38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobId</th>\n",
       "      <th>companyId</th>\n",
       "      <th>jobRole</th>\n",
       "      <th>education</th>\n",
       "      <th>major</th>\n",
       "      <th>industry</th>\n",
       "      <th>yearsExperience</th>\n",
       "      <th>distanceFromCBD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JOB1362684407687</td>\n",
       "      <td>COMP37</td>\n",
       "      <td>CFO</td>\n",
       "      <td>MASTERS</td>\n",
       "      <td>MATH</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>10.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JOB1362684407688</td>\n",
       "      <td>COMP19</td>\n",
       "      <td>CEO</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>WEB</td>\n",
       "      <td>3.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JOB1362684407697</td>\n",
       "      <td>COMP56</td>\n",
       "      <td>JANITOR</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JOB1362684407698</td>\n",
       "      <td>COMP7</td>\n",
       "      <td>CEO</td>\n",
       "      <td>MASTERS</td>\n",
       "      <td>PHYSICS</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>7.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>JOB1362684407699</td>\n",
       "      <td>COMP4</td>\n",
       "      <td>JUNIOR</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>OIL</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>JOB1362685407682</td>\n",
       "      <td>COMP56</td>\n",
       "      <td>VICE_PRESIDENT</td>\n",
       "      <td>BACHELORS</td>\n",
       "      <td>CHEMISTRY</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>19.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>JOB1362685407683</td>\n",
       "      <td>COMP24</td>\n",
       "      <td>CTO</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>12.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>JOB1362685407684</td>\n",
       "      <td>COMP23</td>\n",
       "      <td>JUNIOR</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>16.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>JOB1362685407685</td>\n",
       "      <td>COMP3</td>\n",
       "      <td>CFO</td>\n",
       "      <td>MASTERS</td>\n",
       "      <td>NONE</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>JOB1362685407686</td>\n",
       "      <td>COMP59</td>\n",
       "      <td>JUNIOR</td>\n",
       "      <td>BACHELORS</td>\n",
       "      <td>NONE</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999699 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   jobId companyId         jobRole    education      major  \\\n",
       "0       JOB1362684407687    COMP37             CFO      MASTERS       MATH   \n",
       "1       JOB1362684407688    COMP19             CEO  HIGH_SCHOOL       NONE   \n",
       "10      JOB1362684407697    COMP56         JANITOR  HIGH_SCHOOL       NONE   \n",
       "11      JOB1362684407698     COMP7             CEO      MASTERS    PHYSICS   \n",
       "12      JOB1362684407699     COMP4          JUNIOR         NONE       NONE   \n",
       "...                  ...       ...             ...          ...        ...   \n",
       "999995  JOB1362685407682    COMP56  VICE_PRESIDENT    BACHELORS  CHEMISTRY   \n",
       "999996  JOB1362685407683    COMP24             CTO  HIGH_SCHOOL       NONE   \n",
       "999997  JOB1362685407684    COMP23          JUNIOR  HIGH_SCHOOL       NONE   \n",
       "999998  JOB1362685407685     COMP3             CFO      MASTERS       NONE   \n",
       "999999  JOB1362685407686    COMP59          JUNIOR    BACHELORS       NONE   \n",
       "\n",
       "         industry  yearsExperience  distanceFromCBD  \n",
       "0          HEALTH             10.0             83.0  \n",
       "1             WEB              3.0             73.0  \n",
       "10         HEALTH             24.0             30.0  \n",
       "11      EDUCATION              7.0             79.0  \n",
       "12            OIL              8.0             29.0  \n",
       "...           ...              ...              ...  \n",
       "999995     HEALTH             19.0             94.0  \n",
       "999996    FINANCE             12.0             35.0  \n",
       "999997  EDUCATION             16.0             81.0  \n",
       "999998     HEALTH              6.0              5.0  \n",
       "999999  EDUCATION             20.0             11.0  \n",
       "\n",
       "[999699 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employee_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adecee09",
   "metadata": {},
   "source": [
    "With this, the basic cleaning has been finished for the `Employee_dataset.csv`, we will now move onto doing the basic cleaning for the `Employee_salaries.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691463d",
   "metadata": {},
   "source": [
    "#### `Employee_salaries.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19263d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in Salary Dataset: \n",
      " jobId                223\n",
      "salaryInThousands    229\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Sum of the missing values in the Salary Dataset: 452\n"
     ]
    }
   ],
   "source": [
    "# check for missing values in the salary df\n",
    "print(\"Missing Values in Salary Dataset: \\n\", salary_df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "print(f\"Sum of the missing values in the Salary Dataset: {salary_df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22a3079a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Missing Values in Employee Dataset: \n",
      " jobId                0.0223\n",
      "salaryInThousands    0.0229\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# calculate the percentage of missing values against the entire size of the df \n",
    "missing_percentage = (salary_df.isnull().sum() / salary_df.shape[0]) * 100\n",
    "print(\"Percentage of Missing Values in Employee Dataset: \\n\", missing_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79437f4",
   "metadata": {},
   "source": [
    "Similar to `Employee_dataset.csv`, the percentage is negligible due to the vast amount of data present in the df, therefore it is safe to drop these missing values without risking any data loss or integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3fb2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in Salary Dataset after dropping: \n",
      " jobId                0\n",
      "salaryInThousands    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Sum of the missing values in the Salary Dataset after dropping: 0\n"
     ]
    }
   ],
   "source": [
    "# drop missing values in salary df \n",
    "salary_df.dropna(inplace=True)\n",
    "# print out the missing values in the salary df after dropping the missing values\n",
    "print(\"Missing Values in Salary Dataset after dropping: \\n\", salary_df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "print(f\"Sum of the missing values in the Salary Dataset after dropping: {salary_df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf6b4d",
   "metadata": {},
   "source": [
    "Missing values have been removed from the salary df, now it's time to check for duplicates in the salary df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcfef11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows in the Employee Dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates in the employee df\n",
    "duplicates = employee_df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows in the Employee Dataset: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e76a4",
   "metadata": {},
   "source": [
    "Similarly, there is no duplicated values in the Salary df, this concludes basic cleaning for both the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93dcf64",
   "metadata": {},
   "source": [
    "There is still more cleaning do to (Checking for outliers, structural errors), but it is better to merge the dataset first than perform more advanced cleaning. Here is the justification: \n",
    "\n",
    "1. Contextual Outlier Detection\n",
    "\t•\tOutliers are often only apparent when you have all relevant fields together.\n",
    "\t•\tExample: A salary might look reasonable in isolation, but is an outlier when paired with a junior job title or low years of experience.\n",
    "\n",
    "2. Cross-Feature Consistency Checks\n",
    "\t•\tAdvanced cleaning often requires comparing values across multiple columns, which may only exist after merging.\n",
    "\t•\tExample: Ensuring education level aligns with jobRole or that industry matches salaryInThousands.\n",
    "\n",
    "3. Category Normalization\n",
    "\t•\tHarmonizing category labels (like job roles, majors, industries) is easier once you have the full combined set of categories from both datasets.\n",
    "\n",
    "4. Avoiding Premature Data Loss\n",
    "\t•\tCleaning before merging can lead to removing data that would otherwise be valid in context.\n",
    "\t•\tExample: A value might appear as an outlier in one dataset but is justified when paired with a field from the other dataset.\n",
    "\n",
    "5. Efficient Error Detection\n",
    "\t•\tStructural errors and inconsistencies, such as duplicate jobId rows with conflicting information, become much more visible post-merge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6478f0",
   "metadata": {},
   "source": [
    "However, before we merge, there might be some jobIds which are present in `Employee_salaries.csv` which are not present in `Employee_dataset.csv`, therefore it is best we check which jobIds match and then merge because: \n",
    "\n",
    "When merging two datasets, it is generally best to keep only those records (rows) where the `jobId` appears in both dataframes. This is typically done using an `inner join`. Here’s why:\n",
    "\n",
    "- **Ensures Complete Data:**  \n",
    "  Merging only on matching `jobId`s guarantees that each row in the merged dataset has both employee details and salary information. This leads to a dataset where every record is fully usable for analysis.\n",
    "\n",
    "- **Prevents Unnecessary Missing Values:**  \n",
    "  Including rows where the `jobId` is missing from one side introduces missing values (`NaN`) in important columns. These incomplete records add complexity and can lower the quality of subsequent analyses.\n",
    "\n",
    "- **Simplifies Downstream Processing:**  \n",
    "  A merged dataset without extraneous missing values is easier to clean, analyze, and use for machine learning.\n",
    "\n",
    "Therefore, we will now check the matching jobIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb22e808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobIds in Salary Dataset not present in Employee Dataset: 297\n"
     ]
    }
   ],
   "source": [
    "# create sets for both the datasets to check if there are any common columns\n",
    "salary_ids = set(salary_df['jobId'])\n",
    "employee_ids = set(employee_df['jobId'])\n",
    "\n",
    "# find the rows in salary which are not present in employee \n",
    "id_in_salary = salary_ids - employee_ids\n",
    "\n",
    "# present numbers \n",
    "print(f\"Number of jobIds in Salary Dataset not present in Employee Dataset: {len(id_in_salary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fad72e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in Salary Dataset not present in Employee Dataset: \n",
      "                    jobId  salaryInThousands\n",
      "2       JOB1362684407689              137.0\n",
      "3       JOB1362684407690              142.0\n",
      "4       JOB1362684407691              163.0\n",
      "5       JOB1362684407692              113.0\n",
      "53      JOB1362684407740              193.0\n",
      "...                  ...                ...\n",
      "999809  JOB1362685407496              116.0\n",
      "999846  JOB1362685407533              104.0\n",
      "999847  JOB1362685407534              152.0\n",
      "999848  JOB1362685407535              168.0\n",
      "999868  JOB1362685407555              129.0\n",
      "\n",
      "[297 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# show the rows in salary df which are not present in employee df\n",
    "salary_not_in_employee = salary_df[salary_df['jobId'].isin(id_in_salary)]\n",
    "print(\"Rows in Salary Dataset not present in Employee Dataset: \\n\", salary_not_in_employee)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5538ddb2",
   "metadata": {},
   "source": [
    "Since the rows which are in `Employee_salaries.csv` and not in `Employee_dataset.csv` are not so much as compared to the overall size of the salary df, it is safe to drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c16b833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salary Dataset after dropping rows not present in Employee Dataset: \n",
      "               jobId  salaryInThousands\n",
      "0  JOB1362684407687              130.0\n",
      "1  JOB1362684407688              101.0\n",
      "2  JOB1362684407697              102.0\n",
      "3  JOB1362684407698              144.0\n",
      "4  JOB1362684407699               79.0\n"
     ]
    }
   ],
   "source": [
    "# drops the rows in salary df which are not present in employee df\n",
    "salary_df = salary_df[~salary_df['jobId'].isin(id_in_salary)]\n",
    "salary_df = salary_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Salary Dataset after dropping rows not present in Employee Dataset: \\n\", salary_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34842733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobId</th>\n",
       "      <th>salaryInThousands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JOB1362684407687</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JOB1362684407688</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JOB1362684407697</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JOB1362684407698</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB1362684407699</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999469</th>\n",
       "      <td>JOB1362685407682</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999470</th>\n",
       "      <td>JOB1362685407683</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999471</th>\n",
       "      <td>JOB1362685407684</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999472</th>\n",
       "      <td>JOB1362685407685</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999473</th>\n",
       "      <td>JOB1362685407686</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999474 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   jobId  salaryInThousands\n",
       "0       JOB1362684407687              130.0\n",
       "1       JOB1362684407688              101.0\n",
       "2       JOB1362684407697              102.0\n",
       "3       JOB1362684407698              144.0\n",
       "4       JOB1362684407699               79.0\n",
       "...                  ...                ...\n",
       "999469  JOB1362685407682               88.0\n",
       "999470  JOB1362685407683              160.0\n",
       "999471  JOB1362685407684               64.0\n",
       "999472  JOB1362685407685              149.0\n",
       "999473  JOB1362685407686               88.0\n",
       "\n",
       "[999474 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4738c1d",
   "metadata": {},
   "source": [
    "To ensure each record in our merged dataset has both employee details and salary information, we remove salary records whose `jobId` does not exist in the employee dataset. Since this represents a very small fraction of the data, it will not significantly affect our analysis, and helps avoid missing values and incomplete records later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ade1c",
   "metadata": {},
   "source": [
    "We can now join both datasets to have a `master_df.csv`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7086543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master Dataset after merging Employee and Salary Datasets: \n",
      "               jobId companyId  jobRole    education    major   industry  \\\n",
      "0  JOB1362684407687    COMP37      CFO      MASTERS     MATH     HEALTH   \n",
      "1  JOB1362684407688    COMP19      CEO  HIGH_SCHOOL     NONE        WEB   \n",
      "2  JOB1362684407697    COMP56  JANITOR  HIGH_SCHOOL     NONE     HEALTH   \n",
      "3  JOB1362684407698     COMP7      CEO      MASTERS  PHYSICS  EDUCATION   \n",
      "4  JOB1362684407699     COMP4   JUNIOR         NONE     NONE        OIL   \n",
      "\n",
      "   yearsExperience  distanceFromCBD  salaryInThousands  \n",
      "0             10.0             83.0              130.0  \n",
      "1              3.0             73.0              101.0  \n",
      "2             24.0             30.0              102.0  \n",
      "3              7.0             79.0              144.0  \n",
      "4              8.0             29.0               79.0  \n"
     ]
    }
   ],
   "source": [
    "master_df = pd.merge(employee_df, salary_df, on='jobId', how='inner')\n",
    "\n",
    "print(\"Master Dataset after merging Employee and Salary Datasets: \\n\", master_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9815a1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current size of the Master Dataset:  (999474, 9)\n",
      "\n",
      "\n",
      "Current Dtypes of the Master Dataset: \n",
      " jobId                 object\n",
      "companyId             object\n",
      "jobRole               object\n",
      "education             object\n",
      "major                 object\n",
      "industry              object\n",
      "yearsExperience      float64\n",
      "distanceFromCBD      float64\n",
      "salaryInThousands    float64\n",
      "dtype: object\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobId</th>\n",
       "      <th>companyId</th>\n",
       "      <th>jobRole</th>\n",
       "      <th>education</th>\n",
       "      <th>major</th>\n",
       "      <th>industry</th>\n",
       "      <th>yearsExperience</th>\n",
       "      <th>distanceFromCBD</th>\n",
       "      <th>salaryInThousands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JOB1362684407687</td>\n",
       "      <td>COMP37</td>\n",
       "      <td>CFO</td>\n",
       "      <td>MASTERS</td>\n",
       "      <td>MATH</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>10.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JOB1362684407688</td>\n",
       "      <td>COMP19</td>\n",
       "      <td>CEO</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>WEB</td>\n",
       "      <td>3.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JOB1362684407697</td>\n",
       "      <td>COMP56</td>\n",
       "      <td>JANITOR</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JOB1362684407698</td>\n",
       "      <td>COMP7</td>\n",
       "      <td>CEO</td>\n",
       "      <td>MASTERS</td>\n",
       "      <td>PHYSICS</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>7.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB1362684407699</td>\n",
       "      <td>COMP4</td>\n",
       "      <td>JUNIOR</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>OIL</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999469</th>\n",
       "      <td>JOB1362685407682</td>\n",
       "      <td>COMP56</td>\n",
       "      <td>VICE_PRESIDENT</td>\n",
       "      <td>BACHELORS</td>\n",
       "      <td>CHEMISTRY</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>19.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999470</th>\n",
       "      <td>JOB1362685407683</td>\n",
       "      <td>COMP24</td>\n",
       "      <td>CTO</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>12.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999471</th>\n",
       "      <td>JOB1362685407684</td>\n",
       "      <td>COMP23</td>\n",
       "      <td>JUNIOR</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>16.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999472</th>\n",
       "      <td>JOB1362685407685</td>\n",
       "      <td>COMP3</td>\n",
       "      <td>CFO</td>\n",
       "      <td>MASTERS</td>\n",
       "      <td>NONE</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999473</th>\n",
       "      <td>JOB1362685407686</td>\n",
       "      <td>COMP59</td>\n",
       "      <td>JUNIOR</td>\n",
       "      <td>BACHELORS</td>\n",
       "      <td>NONE</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999474 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   jobId companyId         jobRole    education      major  \\\n",
       "0       JOB1362684407687    COMP37             CFO      MASTERS       MATH   \n",
       "1       JOB1362684407688    COMP19             CEO  HIGH_SCHOOL       NONE   \n",
       "2       JOB1362684407697    COMP56         JANITOR  HIGH_SCHOOL       NONE   \n",
       "3       JOB1362684407698     COMP7             CEO      MASTERS    PHYSICS   \n",
       "4       JOB1362684407699     COMP4          JUNIOR         NONE       NONE   \n",
       "...                  ...       ...             ...          ...        ...   \n",
       "999469  JOB1362685407682    COMP56  VICE_PRESIDENT    BACHELORS  CHEMISTRY   \n",
       "999470  JOB1362685407683    COMP24             CTO  HIGH_SCHOOL       NONE   \n",
       "999471  JOB1362685407684    COMP23          JUNIOR  HIGH_SCHOOL       NONE   \n",
       "999472  JOB1362685407685     COMP3             CFO      MASTERS       NONE   \n",
       "999473  JOB1362685407686    COMP59          JUNIOR    BACHELORS       NONE   \n",
       "\n",
       "         industry  yearsExperience  distanceFromCBD  salaryInThousands  \n",
       "0          HEALTH             10.0             83.0              130.0  \n",
       "1             WEB              3.0             73.0              101.0  \n",
       "2          HEALTH             24.0             30.0              102.0  \n",
       "3       EDUCATION              7.0             79.0              144.0  \n",
       "4             OIL              8.0             29.0               79.0  \n",
       "...           ...              ...              ...                ...  \n",
       "999469     HEALTH             19.0             94.0               88.0  \n",
       "999470    FINANCE             12.0             35.0              160.0  \n",
       "999471  EDUCATION             16.0             81.0               64.0  \n",
       "999472     HEALTH              6.0              5.0              149.0  \n",
       "999473  EDUCATION             20.0             11.0               88.0  \n",
       "\n",
       "[999474 rows x 9 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print data about the master df\n",
    "print(\"Current size of the Master Dataset: \", master_df.shape)\n",
    "print(\"\\n\")\n",
    "print(\"Current Dtypes of the Master Dataset: \\n\", master_df.dtypes)\n",
    "print(\"\\n\")\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef130f",
   "metadata": {},
   "source": [
    "#### `master_df.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "accd7034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in Master Dataset: \n",
      " jobId                0\n",
      "companyId            0\n",
      "jobRole              0\n",
      "education            0\n",
      "major                0\n",
      "industry             0\n",
      "yearsExperience      0\n",
      "distanceFromCBD      0\n",
      "salaryInThousands    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Sum of the missing values in the Master Dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# check for missing values in the master df \n",
    "print(\"Missing Values in Master Dataset: \\n\", master_df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "print(f\"Sum of the missing values in the Master Dataset: {master_df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a539eab",
   "metadata": {},
   "source": [
    "No Missing values in the master dataset, let us now see for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f49aa7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows in the Master Dataset:  0\n"
     ]
    }
   ],
   "source": [
    "# duplicates in the master df\n",
    "print(\"Number of duplicate rows in the Master Dataset: \", master_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38bc8b6",
   "metadata": {},
   "source": [
    "Basic Cleaning for the Master Dataset is done, we will now move onto printing out the executive summary to perform more complex cleaning (fixing structural errors, outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4e66e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>num_unique</th>\n",
       "      <th>example_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jobId</td>\n",
       "      <td>999474</td>\n",
       "      <td>[JOB1362684407687, JOB1362684407688, JOB136268...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>companyId</td>\n",
       "      <td>63</td>\n",
       "      <td>[COMP37, COMP19, COMP56, COMP7, COMP4, COMP54,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jobRole</td>\n",
       "      <td>9</td>\n",
       "      <td>[CFO, CEO, JANITOR, JUNIOR, CTO, VICE_PRESIDEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>education</td>\n",
       "      <td>5</td>\n",
       "      <td>[MASTERS, HIGH_SCHOOL, NONE, BACHELORS, DOCTORAL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>major</td>\n",
       "      <td>9</td>\n",
       "      <td>[MATH, NONE, PHYSICS, BIOLOGY, LITERATURE, CHE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>industry</td>\n",
       "      <td>8</td>\n",
       "      <td>[HEALTH, WEB, EDUCATION, OIL, FINANCE, AUTO, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yearsExperience</td>\n",
       "      <td>25</td>\n",
       "      <td>[10.0, 3.0, 24.0, 7.0, 8.0, 21.0, 13.0, 1.0, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>distanceFromCBD</td>\n",
       "      <td>102</td>\n",
       "      <td>[83.0, 73.0, 30.0, 79.0, 29.0, 26.0, 81.0, 8.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>salaryInThousands</td>\n",
       "      <td>281</td>\n",
       "      <td>[130.0, 101.0, 102.0, 144.0, 79.0, 193.0, 47.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              column  num_unique  \\\n",
       "0              jobId      999474   \n",
       "1          companyId          63   \n",
       "2            jobRole           9   \n",
       "3          education           5   \n",
       "4              major           9   \n",
       "5           industry           8   \n",
       "6    yearsExperience          25   \n",
       "7    distanceFromCBD         102   \n",
       "8  salaryInThousands         281   \n",
       "\n",
       "                                      example_values  \n",
       "0  [JOB1362684407687, JOB1362684407688, JOB136268...  \n",
       "1  [COMP37, COMP19, COMP56, COMP7, COMP4, COMP54,...  \n",
       "2  [CFO, CEO, JANITOR, JUNIOR, CTO, VICE_PRESIDEN...  \n",
       "3  [MASTERS, HIGH_SCHOOL, NONE, BACHELORS, DOCTORAL]  \n",
       "4  [MATH, NONE, PHYSICS, BIOLOGY, LITERATURE, CHE...  \n",
       "5  [HEALTH, WEB, EDUCATION, OIL, FINANCE, AUTO, S...  \n",
       "6  [10.0, 3.0, 24.0, 7.0, 8.0, 21.0, 13.0, 1.0, 2...  \n",
       "7  [83.0, 73.0, 30.0, 79.0, 29.0, 26.0, 81.0, 8.0...  \n",
       "8  [130.0, 101.0, 102.0, 144.0, 79.0, 193.0, 47.0...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print unique summary for master dataset\n",
    "executive_summary = pd.DataFrame({\n",
    "    \"column\": master_df.columns,\n",
    "    \"num_unique\": [master_df[c].nunique(dropna=False) for c in master_df.columns],\n",
    "    \"example_values\": [master_df[c].unique()[:10] for c in master_df.columns]  # Show first 10 unique values\n",
    "})\n",
    "executive_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b29ff78",
   "metadata": {},
   "source": [
    "We have printed out the executive summary to gain a overview of the columns and their unique values, now, we will narrow down to clean the most relevant columns first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4d770",
   "metadata": {},
   "source": [
    "##### `jobRole` Column Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5f1d4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cfo', 'ceo', 'janitor', 'junior', 'cto', 'vice_president',\n",
       "       'senior', 'manager', 'president'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure proper capitalisation of the column \n",
    "master_df['jobRole'] = master_df['jobRole'].str.lower().str.strip()\n",
    "\n",
    "master_df['jobRole'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673062d",
   "metadata": {},
   "source": [
    "##### Check the Value Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed56be9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobRole\n",
      "senior            125830\n",
      "vice_president    125168\n",
      "manager           125062\n",
      "cto               124986\n",
      "janitor           124909\n",
      "ceo               124703\n",
      "junior            124519\n",
      "cfo               124296\n",
      "president              1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(master_df['jobRole'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e343fa2",
   "metadata": {},
   "source": [
    "The `president` column looks suspicious, as there is so much more entires for `vice_president`, let us investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c690acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobId</th>\n",
       "      <th>companyId</th>\n",
       "      <th>jobRole</th>\n",
       "      <th>education</th>\n",
       "      <th>major</th>\n",
       "      <th>industry</th>\n",
       "      <th>yearsExperience</th>\n",
       "      <th>distanceFromCBD</th>\n",
       "      <th>salaryInThousands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>935203</th>\n",
       "      <td>JOB1362685343310</td>\n",
       "      <td>COMP0</td>\n",
       "      <td>president</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>GOVERNMENT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   jobId companyId    jobRole education major    industry  \\\n",
       "935203  JOB1362685343310     COMP0  president      NONE  NONE  GOVERNMENT   \n",
       "\n",
       "        yearsExperience  distanceFromCBD  salaryInThousands  \n",
       "935203              1.0              1.0               81.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the row(s) with 'president'\n",
    "master_df[master_df['jobRole'] == 'president']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "609cc417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobId</th>\n",
       "      <th>companyId</th>\n",
       "      <th>jobRole</th>\n",
       "      <th>education</th>\n",
       "      <th>major</th>\n",
       "      <th>industry</th>\n",
       "      <th>yearsExperience</th>\n",
       "      <th>distanceFromCBD</th>\n",
       "      <th>salaryInThousands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>JOB1362684407718</td>\n",
       "      <td>COMP0</td>\n",
       "      <td>cfo</td>\n",
       "      <td>BACHELORS</td>\n",
       "      <td>PHYSICS</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>18.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>JOB1362684407924</td>\n",
       "      <td>COMP0</td>\n",
       "      <td>cfo</td>\n",
       "      <td>MASTERS</td>\n",
       "      <td>MATH</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>JOB1362684407980</td>\n",
       "      <td>COMP0</td>\n",
       "      <td>cfo</td>\n",
       "      <td>DOCTORAL</td>\n",
       "      <td>LITERATURE</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>8.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>JOB1362684407997</td>\n",
       "      <td>COMP0</td>\n",
       "      <td>senior</td>\n",
       "      <td>BACHELORS</td>\n",
       "      <td>CHEMISTRY</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>17.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>JOB1362684408026</td>\n",
       "      <td>COMP0</td>\n",
       "      <td>janitor</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>WEB</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999175</th>\n",
       "      <td>JOB1362685407377</td>\n",
       "      <td>COMP0</td>\n",
       "      <td>ceo</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>22.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>183.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999184</th>\n",
       "      <td>JOB1362685407386</td>\n",
       "      <td>COMP0</td>\n",
       "      <td>manager</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>OIL</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999270</th>\n",
       "      <td>JOB1362685407475</td>\n",
       "      <td>COMP0</td>\n",
       "      <td>vice_president</td>\n",
       "      <td>MASTERS</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>24.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999287</th>\n",
       "      <td>JOB1362685407492</td>\n",
       "      <td>COMP0</td>\n",
       "      <td>janitor</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>WEB</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999425</th>\n",
       "      <td>JOB1362685407638</td>\n",
       "      <td>COMP0</td>\n",
       "      <td>junior</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>16.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15663 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   jobId companyId         jobRole    education       major  \\\n",
       "23      JOB1362684407718     COMP0             cfo    BACHELORS     PHYSICS   \n",
       "216     JOB1362684407924     COMP0             cfo      MASTERS        MATH   \n",
       "269     JOB1362684407980     COMP0             cfo     DOCTORAL  LITERATURE   \n",
       "286     JOB1362684407997     COMP0          senior    BACHELORS   CHEMISTRY   \n",
       "315     JOB1362684408026     COMP0         janitor  HIGH_SCHOOL        NONE   \n",
       "...                  ...       ...             ...          ...         ...   \n",
       "999175  JOB1362685407377     COMP0             ceo  HIGH_SCHOOL        NONE   \n",
       "999184  JOB1362685407386     COMP0         manager         NONE        NONE   \n",
       "999270  JOB1362685407475     COMP0  vice_president      MASTERS    BUSINESS   \n",
       "999287  JOB1362685407492     COMP0         janitor         NONE        NONE   \n",
       "999425  JOB1362685407638     COMP0          junior         NONE        NONE   \n",
       "\n",
       "         industry  yearsExperience  distanceFromCBD  salaryInThousands  \n",
       "23         HEALTH             18.0             32.0              132.0  \n",
       "216     EDUCATION             20.0             20.0              180.0  \n",
       "269     EDUCATION              8.0             83.0              101.0  \n",
       "286     EDUCATION             17.0             97.0               86.0  \n",
       "315           WEB             21.0              1.0              163.0  \n",
       "...           ...              ...              ...                ...  \n",
       "999175    FINANCE             22.0             32.0              183.0  \n",
       "999184        OIL             15.0             14.0              104.0  \n",
       "999270  EDUCATION             24.0             52.0              131.0  \n",
       "999287        WEB              5.0              1.0               82.0  \n",
       "999425    FINANCE             16.0             91.0               97.0  \n",
       "\n",
       "[15663 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out values with COMP0 as companyID in the df \n",
    "master_df[(master_df['companyId'] == 'COMP0')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7895777",
   "metadata": {},
   "source": [
    "This president entry is very suspicious, as it does not make sense for the president to make lesser money than the vice president, with only one year of experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ddd6bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the row where jobRole is 'president'\n",
    "master_df = master_df[master_df['jobRole'] != 'president']\n",
    "\n",
    "master_df = master_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c04bec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cfo' 'ceo' 'janitor' 'junior' 'cto' 'vice_president' 'senior' 'manager']\n",
      "jobRole\n",
      "senior            125830\n",
      "vice_president    125168\n",
      "manager           125062\n",
      "cto               124986\n",
      "janitor           124909\n",
      "ceo               124703\n",
      "junior            124519\n",
      "cfo               124296\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Confirm it's been deleted\n",
    "print(master_df['jobRole'].unique())\n",
    "print(master_df['jobRole'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7771a9f7",
   "metadata": {},
   "source": [
    "With that out of the way, let us continue on with finding out even more outliers in the jobRole with specific roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "712b6595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [jobId, companyId, jobRole, education, major, industry, yearsExperience, distanceFromCBD, salaryInThousands]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Find janitors with unusually high salaries\n",
    "high_salary_janitors = master_df[(master_df['jobRole'] == 'janitor') & (master_df['salaryInThousands'] > 200)]\n",
    "print(high_salary_janitors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b52ad404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobId</th>\n",
       "      <th>companyId</th>\n",
       "      <th>jobRole</th>\n",
       "      <th>education</th>\n",
       "      <th>major</th>\n",
       "      <th>industry</th>\n",
       "      <th>yearsExperience</th>\n",
       "      <th>distanceFromCBD</th>\n",
       "      <th>salaryInThousands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>JOB1362684407753</td>\n",
       "      <td>COMP15</td>\n",
       "      <td>cto</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>WEB</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>JOB1362684407802</td>\n",
       "      <td>COMP44</td>\n",
       "      <td>cfo</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>JOB1362684407817</td>\n",
       "      <td>COMP29</td>\n",
       "      <td>cto</td>\n",
       "      <td>DOCTORAL</td>\n",
       "      <td>ENGINEERING</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>JOB1362684407862</td>\n",
       "      <td>COMP5</td>\n",
       "      <td>cfo</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>WEB</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>JOB1362684407878</td>\n",
       "      <td>COMP41</td>\n",
       "      <td>ceo</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999157</th>\n",
       "      <td>JOB1362685407360</td>\n",
       "      <td>COMP26</td>\n",
       "      <td>cfo</td>\n",
       "      <td>BACHELORS</td>\n",
       "      <td>PHYSICS</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999243</th>\n",
       "      <td>JOB1362685407449</td>\n",
       "      <td>COMP26</td>\n",
       "      <td>cfo</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999394</th>\n",
       "      <td>JOB1362685407608</td>\n",
       "      <td>COMP12</td>\n",
       "      <td>ceo</td>\n",
       "      <td>DOCTORAL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>WEB</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>177.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999401</th>\n",
       "      <td>JOB1362685407615</td>\n",
       "      <td>COMP57</td>\n",
       "      <td>cfo</td>\n",
       "      <td>DOCTORAL</td>\n",
       "      <td>MATH</td>\n",
       "      <td>WEB</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999460</th>\n",
       "      <td>JOB1362685407674</td>\n",
       "      <td>COMP51</td>\n",
       "      <td>cfo</td>\n",
       "      <td>MASTERS</td>\n",
       "      <td>NONE</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>124.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29872 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   jobId companyId jobRole    education        major industry  \\\n",
       "56      JOB1362684407753    COMP15     cto  HIGH_SCHOOL         NONE      WEB   \n",
       "103     JOB1362684407802    COMP44     cfo  HIGH_SCHOOL         NONE   HEALTH   \n",
       "115     JOB1362684407817    COMP29     cto     DOCTORAL  ENGINEERING     AUTO   \n",
       "160     JOB1362684407862     COMP5     cfo         NONE         NONE      WEB   \n",
       "176     JOB1362684407878    COMP41     ceo         NONE         NONE  FINANCE   \n",
       "...                  ...       ...     ...          ...          ...      ...   \n",
       "999157  JOB1362685407360    COMP26     cfo    BACHELORS      PHYSICS     AUTO   \n",
       "999243  JOB1362685407449    COMP26     cfo  HIGH_SCHOOL         NONE   HEALTH   \n",
       "999394  JOB1362685407608    COMP12     ceo     DOCTORAL         NONE      WEB   \n",
       "999401  JOB1362685407615    COMP57     cfo     DOCTORAL         MATH      WEB   \n",
       "999460  JOB1362685407674    COMP51     cfo      MASTERS         NONE  FINANCE   \n",
       "\n",
       "        yearsExperience  distanceFromCBD  salaryInThousands  \n",
       "56                  1.0             13.0               96.0  \n",
       "103                 1.0             34.0               99.0  \n",
       "115                 1.0             96.0              146.0  \n",
       "160                 0.0             31.0              123.0  \n",
       "176                 1.0             99.0               93.0  \n",
       "...                 ...              ...                ...  \n",
       "999157              0.0             68.0               91.0  \n",
       "999243              0.0             59.0              103.0  \n",
       "999394              1.0             57.0              177.0  \n",
       "999401              0.0             29.0              162.0  \n",
       "999460              1.0             99.0              124.0  \n",
       "\n",
       "[29872 rows x 9 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Find executives (ceo, cfo, cto) with very low yearsExperience\n",
    "junior_execs = master_df[\n",
    "    (master_df['jobRole'].isin(['ceo', 'cfo', 'cto'])) &\n",
    "    (master_df['yearsExperience'] < 2)\n",
    "]\n",
    "junior_execs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64db279",
   "metadata": {},
   "source": [
    "##### `education` Column Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bd34b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MASTERS' 'HIGH_SCHOOL' 'NONE' 'BACHELORS' 'DOCTORAL']\n"
     ]
    }
   ],
   "source": [
    "print(master_df['education'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f21cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df['education'] = master_df['education'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84fca116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['masters' 'high_school' 'none' 'bachelors' 'doctoral']\n"
     ]
    }
   ],
   "source": [
    "print(master_df['education'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d53170ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education\n",
      "high_school    236862\n",
      "none           236714\n",
      "bachelors      175405\n",
      "doctoral       175271\n",
      "masters        175221\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(master_df['education'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e423516d",
   "metadata": {},
   "source": [
    "##### `major` Column Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ca120f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MATH' 'NONE' 'PHYSICS' 'BIOLOGY' 'LITERATURE' 'CHEMISTRY' 'COMPSCI'\n",
      " 'BUSINESS' 'ENGINEERING']\n"
     ]
    }
   ],
   "source": [
    "print(master_df['major'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d51b7e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df['major'] = master_df['major'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "361dc739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['math' 'none' 'physics' 'biology' 'literature' 'chemistry' 'compsci'\n",
      " 'business' 'engineering']\n"
     ]
    }
   ],
   "source": [
    "print(master_df['major'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d751b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "major\n",
      "none           532060\n",
      "chemistry       58841\n",
      "literature      58644\n",
      "engineering     58568\n",
      "business        58498\n",
      "physics         58381\n",
      "compsci         58352\n",
      "biology         58351\n",
      "math            57778\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(master_df['major'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0fefa53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>major</th>\n",
       "      <th>biology</th>\n",
       "      <th>business</th>\n",
       "      <th>chemistry</th>\n",
       "      <th>compsci</th>\n",
       "      <th>engineering</th>\n",
       "      <th>literature</th>\n",
       "      <th>math</th>\n",
       "      <th>none</th>\n",
       "      <th>physics</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bachelors</th>\n",
       "      <td>19610</td>\n",
       "      <td>19450</td>\n",
       "      <td>19672</td>\n",
       "      <td>19567</td>\n",
       "      <td>19664</td>\n",
       "      <td>19409</td>\n",
       "      <td>19254</td>\n",
       "      <td>19484</td>\n",
       "      <td>19295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doctoral</th>\n",
       "      <td>19395</td>\n",
       "      <td>19534</td>\n",
       "      <td>19567</td>\n",
       "      <td>19318</td>\n",
       "      <td>19417</td>\n",
       "      <td>19577</td>\n",
       "      <td>19351</td>\n",
       "      <td>19705</td>\n",
       "      <td>19407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high_school</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>236862</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>masters</th>\n",
       "      <td>19346</td>\n",
       "      <td>19514</td>\n",
       "      <td>19602</td>\n",
       "      <td>19467</td>\n",
       "      <td>19487</td>\n",
       "      <td>19658</td>\n",
       "      <td>19173</td>\n",
       "      <td>19295</td>\n",
       "      <td>19679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>236714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "major        biology  business  chemistry  compsci  engineering  literature  \\\n",
       "education                                                                     \n",
       "bachelors      19610     19450      19672    19567        19664       19409   \n",
       "doctoral       19395     19534      19567    19318        19417       19577   \n",
       "high_school        0         0          0        0            0           0   \n",
       "masters        19346     19514      19602    19467        19487       19658   \n",
       "none               0         0          0        0            0           0   \n",
       "\n",
       "major         math    none  physics  \n",
       "education                            \n",
       "bachelors    19254   19484    19295  \n",
       "doctoral     19351   19705    19407  \n",
       "high_school      0  236862        0  \n",
       "masters      19173   19295    19679  \n",
       "none             0  236714        0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(master_df['education'], master_df['major'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33f90c6",
   "metadata": {},
   "source": [
    "To ensure data consistency, we checked the relationship between `major` and `education` using a crosstab. All non-tertiary education levels (`high_school` and `none`) only have 'none' as their major, while higher education levels have valid fields of study as majors. This confirms that the columns are logically consistent and well-cleaned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8b4b9",
   "metadata": {},
   "source": [
    "##### `industry` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86f24c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HEALTH' 'WEB' 'EDUCATION' 'OIL' 'FINANCE' 'AUTO' 'SERVICE']\n"
     ]
    }
   ],
   "source": [
    "print(master_df['industry'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c747eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df['industry'] = master_df['industry'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d75dd65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['health' 'web' 'education' 'oil' 'finance' 'auto' 'service']\n"
     ]
    }
   ],
   "source": [
    "print(master_df['industry'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b8c5d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "industry\n",
      "web          143141\n",
      "auto         142878\n",
      "finance      142798\n",
      "education    142736\n",
      "oil          142689\n",
      "health       142674\n",
      "service      142557\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(master_df['industry'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2fd71c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(master_df['industry'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deeffb8",
   "metadata": {},
   "source": [
    "- Listed all unique values to check for typos, synonyms, and formatting inconsistencies.\n",
    "- Standardized formatting for clarity and consistency.\n",
    "- Checked value distribution for rare or suspicious entries.\n",
    "- Handled any missing or unusual categories if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f50aa",
   "metadata": {},
   "source": [
    "##### `yearsExperience` Column Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82d16531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    999473.000000\n",
      "mean         11.992349\n",
      "std           7.212440\n",
      "min           0.000000\n",
      "25%           6.000000\n",
      "50%          12.000000\n",
      "75%          18.000000\n",
      "max          24.000000\n",
      "Name: yearsExperience, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(master_df['yearsExperience'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d42fd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative values: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Negative values:\", (master_df['yearsExperience'] < 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "971e8ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero years: 39822\n"
     ]
    }
   ],
   "source": [
    "print(\"Zero years:\", (master_df['yearsExperience'] == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "15333cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing:\", master_df['yearsExperience'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64e88bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 values: yearsExperience\n",
      "15.0    40298\n",
      "1.0     40248\n",
      "9.0     40222\n",
      "3.0     40172\n",
      "22.0    40171\n",
      "8.0     40090\n",
      "6.0     40068\n",
      "18.0    40067\n",
      "17.0    40052\n",
      "7.0     40028\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 values:\", master_df['yearsExperience'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "150beadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs0AAAGJCAYAAABxbg5mAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAATEJJREFUeJzt3Qd4VOX2/v1FKKF3SUCq9I6AFEUEwdDkgICHJoQuHPDQBOQcpKooSFOaSleQomCh9w7SpQgIiAKHjkDoATLvtZ73t+c/k7aTkDCZyfdzXUOY2c/s2dOSe55Ze+1kDofDIQAAAACi5Bf1IgAAAACK0AwAAADYIDQDAAAANgjNAAAAgA1CMwAAAGCD0AwAAADYIDQDAAAANgjNAAAAgA1CMwAAAGCD0AwgSkOHDpVkyZI9lduqUaOGOVk2btxobvu77757Krffrl07yZ8/vyRmt2/flk6dOklgYKB5bHr16uXpTUo0vv76aylWrJikTJlSMmfOLL7KG16ngK8iNANJxKxZs0zQsk6pU6eWXLlySZ06deSzzz6TW7duxcvtnD9/3oTtAwcOSGKTmLctJj766CPzPHbr1s2ExDZt2kQYs2PHDvHz85OBAwdGuo5PPvnEPP/Lli0TX3Hs2DETJgsWLChfffWVfPnll7YfBKM6Xbx48aluOwDvkcLTGwDg6Ro+fLgUKFBAHj58aAKCzujqjOXYsWPlp59+kjJlyjjHDho0SN57771YB9Nhw4aZ2bBy5crF+HqrV6+WhBbdtmnYCgsLk8Rs/fr1UqVKFRkyZEiUY6pWrSpvv/22jBkzRt566y0pWbKkc9lff/1lnv8333xTGjRoIL5CX8P63E2YMEEKFSoUo+tMmTJF0qdPH+HyxD5L7Q2vU8BXEZqBJKZevXpSsWJF53mdkdQw9vrrr8s//vEPOXr0qKRJk8YsS5EihTklpLt370ratGklVapU4kn6tX5id/nyZSlRooTtuI8//lh+/PFHE563bNniLLF55513zP3UcPk0WM/t03hcYht4mzVrJtmzZxdvcefOHUmXLp1XvE4BX0V5BgB59dVX5f333zczkd988020Nc1r1qyRatWqmYCiM3VFixaV//znP84ZvxdeeMH8v3379s6vvLWkQGnNcqlSpWTv3r1SvXp1E6is64avabY8fvzYjNE6Xg0NGuzPnj3rNkZnjvXr+fBc12m3bZHVimpQ6du3r+TJk0f8/f3Nff3000/F4XC4jdP19OjRQ3744Qdz/3SszvCuXLkyxqGvY8eOEhAQYMpmypYtK7Nnz45Q33369GlTVmFt+59//hnp+jJlymSC8bZt22TatGnmsiVLlsjPP/9sAnXOnDnNbOX48ePNdupt6m1ryL5+/brbujR866y0lvLo/dISiBEjRpjnJfxjHdVzu2fPHlMGpCFVP5DpNx0dOnSI0WMzefJks41627oN3bt3lxs3bjiX63Nmzbw/88wz5nHR1+2TCg4ONo+Lfoh0pfcjS5Ys5lsL17KnzZs3m8cvW7ZskjFjRmnbtm2Ex1KtWLFCXn75ZfNazpAhg3lsjxw54jZGX4v63jp16pTUr1/fjGvdunWUr9OYPpd6Pf1wvHXrVqlUqZIZ+9xzz8mcOXMibKc+xr179zbX0cc+d+7c5j5dvXrVOebBgwfmsdfZfR2j75P+/fubywFfxEwzAEPrYzXkaJlE586dIx2jf9z1j66WcOjX/PqH8uTJkyacqeLFi5vLBw8eLF26dDHhQL344ovOdVy7ds3Mdrdo0cKUD+gf+Oh8+OGHJpQMGDDAhEsNB7Vr1zZ1ydaMeEzEZNtcaTDWgL5hwwYTaLWcY9WqVdKvXz/53//+J+PGjXMbr0Fk8eLF8q9//cuEHK0Tb9q0qZw5c8YEqajcu3fPBE59HDV4a6BctGiRCUcaXHr27Gm2XWuYNcRoeNEgb4XEqFglGPq41apVy6xH76uGKaU/NfDpB4h///vfJpBPnDhR9u/fb55Pa0ZTx2iA69Onj/mp30roYxgSEiKjR492u83Inlt9zoKCgsy2aqmPftjSsK+PlR0Nv1pOo8+31nEfP37clFXs3r3buY36etDQpx8KrJIL1xKjqPz9998RLtNvVazZav3QofdVw7PWiSdPnly++OIL8/7Q50IDvCt97vS6us3WduqHUOsDj9Lr6fo0eGttuc7E6zj9EKqPu2sYfvTokRmny/SDWnQz9jF9LpW+znSWXV/Tui0zZswwr7UKFSo4S3l0h1N9f+gHBv1wU758eROWtXzr3Llz5sOPBnV9f+jrXt9P+ho9dOiQeV/8/vvv5gMk4HMcAJKEmTNn6vSoY/fu3VGOyZQpk+P55593nh8yZIi5jmXcuHHm/JUrV6Jch65fx+jthffKK6+YZVOnTo10mZ4sGzZsMGOfffZZR0hIiPPyhQsXmssnTJjgvCxfvnyO4OBg23VGt216fV2P5YcffjBjP/jgA7dxzZo1cyRLlsxx8uRJ52U6LlWqVG6X/frrr+byzz//3BGd8ePHm3HffPON87LQ0FBH1apVHenTp3e777p9DRo0cMTUn3/+6UiXLp0ja9asjpQpUzoOHTpkLt+yZYu5zblz57qNX7lyZYTL7969G2G9b7/9tiNt2rSO+/fv2z63S5YssX3dReby5cvmMQ0KCnI8fvzYefnEiRPN+mbMmBHhdRrd6zL82MhORYsWdRu7atUq52vgjz/+MM9H48aNI31fVahQwTxvllGjRpnLf/zxR3P+1q1bjsyZMzs6d+7sdv2LFy+a953r5fpa1Ou+9957tq/T2DyXej29bPPmzW6Ps7+/v6Nv377OywYPHmzGLV68OMLth4WFmZ9ff/21w8/Pz9y+K33+9brbtm2LcF3A21GeAcBJZ+mi66JhzcLpV/Zx3RlJZ6d1Riym9Cthnbm16CyZlhcsX75cEpKuX2cXdebOlc7yak7Wr9ld6Wyoli5YdLZTv6b/448/bG9HS09atmzpvExnBvV2dcZv06ZNcb4P+fLlM1+f66yqzhRr+YTSmWwt4XjttdfMDKJ10tlGfQ3o7LrFdTZfXxs6TmchdZZUu1bYPbfWa2bp0qVm59OYWrt2rYSGhpqdVLUbiEW/BdHH9Um7f3z//fem1Mj1NHPmTLcxOkOus7j6DUWTJk1MOYPONkdGZ1tdZ3R1Zlxnrq3Xqa5fvznQ59n1MdfXWOXKld0ec9d12InNc6m0Jt76lkXpNwBaduT6OtXHRkuE3njjjQi3Z82a6+3q7LK2+XO9XS31UpHdH8DbUZ4BwElDWo4cOaJc3rx5c1Mjq72C9at2/dpfw4QGWddgE51nn302Vjv9FS5cOMIfba2hjKqeN77oV+v6FbxrYFcaFKzlrvLmzRthHVr7Gllda/jb0fsY/vGL6nZiy6rjdt3588SJE3Lz5s0on2trxzqrJEe7qGipgpZkuNJ12D23r7zyiilT0TIL/epeS1EaN24srVq1MiE7Ktb91kDnStevdbhP+rho3XVMdgTU0gj9kKjlQPPmzYvyMQv/OtXAqh/urNepPubKCpXh6QcBVxq4tRTHTmyey5i+TrWWWp8zu9vV8o2oSoTC3y7gCwjNAAytVdQ/vtG17NJZR93hSWeRdKZPd3RbsGCBCQJa66mzZnZiU4ccU1EdgEV3VovJNsWHqG4n/E6DiYF+S6Aha+7cuZEut4KQzoxq6NVAp7OtOpOus6379u0ztdLhv22I7Lm1DlCzc+dOsyOi1oVrnay2xNPLImv7lphoXbAVALVm1/UbgdiwHiuta9ZvFsIL36VGP1DE5INoTJ/L+H6d6u2WLl3atKqMjO4UCPgaQjMA5x9zpTsfRUf/kOsMs570D6YecOO///2vCdJaohDfRxC0Zuhc/7jrzkyuO3vpTJlrRwWLzkbqrKQlNtumpQ1aIqAlCa6zzVZJgi6PD7qegwcPmhDiGpLi+3ZcafjV+/bSSy9F+yFGd2LTnft0pz2dmbXojmaxpf2l9aQ7duqMrXaDmD9/vvnWIjLW/dad6lyfQy3Z0NvX11pC0+4pWm6iJQ26E+WoUaNMyYI1ex/+dVqzZk23b20uXLhgul8oq3RHA258bntMn8vYrvPw4cO2Y3799Vfze+BpHTUU8DRqmgGYr961jZh2brBaW8W044B1kBCrzZS20lKRhdi40M4IrnXWOmupYUS7NLj+AddZSw1UFq2hDd+aLjbbpmFHZ6q1C4ErLTHQkOB6+09Cb0cPMqMz9q6dEz7//HMzC6szvfHtn//8p7lv+pyHp7dtPT7WrKTrLKQ+xtoGLqb0a//ws5jhXzOR0WCppRjahcT1+tOnTzffiDyNg7PobLp2P9H2f/oBUbtbaMeJyLZbj0LoWrOtXTH0sbReJ/phVGfs9UNmZLXdV65cSdDnMja0NEMDsXYkCc96LvR2tYuMHmwlso4w+oED8DXMNANJjO7AprOY+gf10qVLJjDrTko6s6ctpfTr96joV/RanqGBRcfr19YaoLT2UltjWQFWd/6aOnWqmaHVoKo7Omkgj4usWbOadeuMn26vthjTEhLXtng6W6lhum7duuaPudZkar9p1x3zYrttDRs2NDOHOouudam6Y5SWoGh9q+6cFn7dcaU7kOnOZdr2S3scazDT+6KtwvS+hq+pjg8axHUHt5EjR5paXd3hTXdi09lS3cFL261pnbrOruosvgZF3TFRPyzoNxKx+SpfA6e+RnSGVh8z/QCkQUsDpDULG1VZgR54R2uh9XnV9mY666zr0plebWn3JPQxjqw0RHeo01Z5+r7Q29IdKbXlmtIdBbUmW3ua66yzK/0wobOu+vqztlNft7rdSu+vBmlt7ajr07Z8eh81lGupk84Uh/+AFp/PZWxoW0V9fLRtoZbS6E6F+oFZfz/oe0ffC3o/Fi5cKF27djXfMun2a3jX3y16uZbhuNbRAz7B0+07ADwdVmss66TtvAIDAx2vvfaaad/m2tosqpZz69atczRq1MiRK1cuc3392bJlS8fvv//udj1ts1WiRAlHihQp3Fq8aVuykiVLRrp9UbWc+/bbbx0DBw505MiRw5EmTRrTcu2vv/6KcP0xY8aY9nTaPuull15y7NmzJ8I6o9u28K28rDZhvXv3NvdTW7YVLlzYMXr0aGfbLYuup3v37hG2KapWeOFdunTJ0b59e0f27NnN41q6dOlI2+LFtuWc6+O4aNGiCMu+/PJL0ypNH9cMGTKY2+3fv7/j/PnzzjHaOqxKlSpmjD4OutxqxabrtkT13O7bt8+8RvLmzWueG30eX3/9dfP8xIS2mCtWrJh5/AMCAhzdunVzXL9+3W1MfLWcs+6Tvhf0sS5fvrzj4cOHbtfX14O2WtuxY4fb+2rTpk2OLl26OLJkyWJa07Vu3dpx7dq1CLev669Tp45pM5c6dWpHwYIFHe3atXN7PPQ1o60CIxPZ6zSmz2VUr5/I3ie67T169DDvKX1N5s6d29z21atXnWO0xd4nn3xinnd9bvW+6zYMGzbMcfPmTZtnAvA+yfQfTwd3AAC8kXVQET3gCjOrgG+jphkAAACwQWgGAAAAbBCaAQAAABvUNAMAAAA2mGkGAAAAbBCaAQAAABsc3CSe6CFwz58/bw5EwCFFAQAAEh+tStaDLOXKlUv8/GI3d0xojicamPPkyePpzQAAAICNs2fPmqPZxgahOZ5Yh7rVJ0EPlwoAAIDEJSQkxExyWrktNgjN8cQqydDATGgGAABIvOJSSsuOgAAAAIANQjMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQjMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQjMAAADgLYfR/vjjj2XgwIHSs2dPGT9+vLns/v370rdvX5k/f748ePBA6tSpI5MnT5aAgADn9c6cOSPdunWTDRs2SPr06SU4OFhGjhwpKVL8v7u2ceNG6dOnjxw5csQcb3zQoEHSrl07t9ufNGmSjB49Wi5evChly5aVzz//XCpVqvQUHwEAgCfo35GrV6/G+3qzZ88uefPmjff1AkjCoXn37t3yxRdfSJkyZdwu7927tyxbtkwWLVokmTJlkh49ekiTJk1k27ZtZvnjx4+lQYMGEhgYKNu3b5cLFy5I27ZtJWXKlPLRRx+ZMadPnzZjunbtKnPnzpV169ZJp06dJGfOnCaEqwULFphQPXXqVKlcubIJ7brs+PHjkiNHDkmKvPGPSEJts+KPn3fjtYHoXhtFixWX+/fuxvu6U6dJK8ePHeX1gQj4neSdkjkcDocnN+D27dtSvnx5M4P8wQcfSLly5UxovXnzpjzzzDMyb948adasmRl77NgxKV68uOzYsUOqVKkiK1askNdff13Onz/vnH3W4DtgwAC5cuWKpEqVyvxfg/fhw4edt9miRQu5ceOGrFy50pzXoPzCCy/IxIkTzfmwsDAzI/3OO+/Ie++9F6P7ERISYoK9bnfGjBnFmyXkHxF//9Ty/fffmQ8t8Uk/MDVt9qY8uH9PEkJCbbc3fojQb338/f3jfb0Jte6Efm0QjLzbvn37pEKFCpLt9b6SMlueeFvvw2tn5drSMbJ3717zNy6+eeP7mzCX8H9jFb+TEi6veXymuXv37mYmuHbt2iY0W/QXzcOHD83llmLFipkXgRWa9Wfp0qXdyjV0hljLNbQU4/nnnzdjXNdhjenVq5f5f2hoqLktLQ2x+Pn5mevodaP7xaIn1yfBV+gvYn0zx/cfkfvnjsiN9dPMB52EEt/bnNDb7ZUfIpL5iTjC4n+9CbzuhHhtWMFoy5Yt5gN9fCK8PF362vAPLCSS1ENXAr4HEyrMedusbUL9jU3o30nZ+b3h2dCstcr6KV/LM8LT2mKdKc6cObPb5RqQdZk1xjUwW8utZdGN0ZB77949uX79uinziGyMzmxHReumhw0bJp6UUL8ojh49miB/RPTNLA5HgvyiuPfHHrm55ZsE+cOXUNvtjR8irMc5IZ/DhNrmhHhtPL59XSRZMnnrrbck3hFensqHCOv3XUJJiPXrOhMidCXk+zuhwpw3fsuYUH9jE/p3kn8CTfJ4UyD3WGg+e/as2elvzZo1kjp1avE2OjOtddAWDeFa0uErX+8kpAQLtgmMDxH/73FOyOcwobY5IYQ9uJ0gz+HTCC8abuPzj1SC/05KyG84EkCCfqD6P970/k7ox8PbvmX0tt9JCf1YpPaSkhKPhWYtibh8+bJbrZfO+G7evNnUFq9atcqUTmjtsets86VLl8yOf0p/7tq1y229utxaZv20LnMdo3UsadKkkeTJk5tTZGOsdURGZzwS6qtTT3+9Y/3BxtPhrR8i4J3hxZt/J3nT77uECi/e+js6oT9getO3jE/j+fOmSZ6HCfRB3qdCc61ateTQoUNul7Vv397ULevOezprq10wtNtF06ZNzXLtZqGzGVWrVjXn9eeHH35owrfV5UJnrjUQlyhRwjlm+fLlbrejY6x1aAmI7gSit9O4cWPnjoB6Xrt1JHYELgCJoWwgIb9yfhrfcCQUb9zmhORN3yR58zYnlJRe9EHep0JzhgwZpFSpUm6XpUuXTrJly+a8vGPHjqYEImvWrCYIazcLDbu6E6AKCgoy4bhNmzYyatQoU7+sPZh150JrFlhbzenMdf/+/aVDhw6yfv16WbhwoemoYdHb0P7OFStWNL2ZtXvHnTt3TIgHAF/yNMoGAMAXebx7RnTGjRtnOlnoTLPrwU0sWlaxdOlS0y1Dw7SGbg2/w4cPd44pUKCACcja83nChAmSO3dumTZtmrNHs2revLlpUTd48GATvLXtnbajC79zIAB4u4T+mhwAfFWiCs165D5XuoOgHqlPT1HJly9fhPKL8GrUqCH79++PdoyWYnhDOQYAxAe+cgaA2PGL5XgAAAAgySE0AwAAADYIzQAAAIANQjMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQjMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQjMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQjMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQjMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQjMAAABgg9AMAAAAJObQPGXKFClTpoxkzJjRnKpWrSorVqxwLq9Ro4YkS5bM7dS1a1e3dZw5c0YaNGggadOmlRw5cki/fv3k0aNHbmM2btwo5cuXF39/fylUqJDMmjUrwrZMmjRJ8ufPL6lTp5bKlSvLrl27EvCeAwAAwJt4NDTnzp1bPv74Y9m7d6/s2bNHXn31VWnUqJEcOXLEOaZz585y4cIF52nUqFHOZY8fPzaBOTQ0VLZv3y6zZ882gXjw4MHOMadPnzZjatasKQcOHJBevXpJp06dZNWqVc4xCxYskD59+siQIUNk3759UrZsWalTp45cvnz5KT4aAAAASKw8GpobNmwo9evXl8KFC0uRIkXkww8/lPTp08vOnTudY3QGOTAw0HnSGWnL6tWr5bfffpNvvvlGypUrJ/Xq1ZMRI0aYWWMN0mrq1KlSoEABGTNmjBQvXlx69OghzZo1k3HjxjnXM3bsWBPO27dvLyVKlDDX0dudMWPGU35EAAAAkBglmppmnTWeP3++3Llzx5RpWObOnSvZs2eXUqVKycCBA+Xu3bvOZTt27JDSpUtLQECA8zKdIQ4JCXHOVuuY2rVru92WjtHLlYZrnel2HePn52fOW2Mi8+DBA3M7ricAAAD4phSe3oBDhw6ZkHz//n0zy7xkyRIz26tatWol+fLlk1y5csnBgwdlwIABcvz4cVm8eLFZfvHiRbfArKzzuiy6MRpy7927J9evXzeBPbIxx44di3K7R44cKcOGDYunRwEAAACJmcdDc9GiRU2t8c2bN+W7776T4OBg2bRpkwnOXbp0cY7TGeWcOXNKrVq15NSpU1KwYEGPbrfOemsdtEVDeJ48eTy6TQAAAPDR0JwqVSrT0UJVqFBBdu/eLRMmTJAvvvgiwljtaqFOnjxpQrPWOIfvcnHp0iXzU5dZP63LXMdobXSaNGkkefLk5hTZGGsdkdFOHHoCAACA70s0Nc2WsLAwUy8cGZ2RVjrjrLSsQ8s7XLtcrFmzxgRiq8RDx6xbt85tPTrGqpvW0K5h3XWMboOed62tBgAAQNKVwtMlDtrxIm/evHLr1i2ZN2+e6ams7eC0BEPPa3eNbNmymZrm3r17S/Xq1U1vZxUUFGTCcZs2bUwrOq1fHjRokHTv3t05C6x9nSdOnCj9+/eXDh06yPr162XhwoWybNky53ZomYWWhVSsWFEqVaok48ePNzskajcNAAAAwKOhWWeI27Zta/ovZ8qUyYRhDcyvvfaanD17VtauXesMsFov3LRpUxOKLVpWsXTpUunWrZuZFU6XLp0Jv8OHD3eO0XZzGpA1cGvZh/aGnjZtmumgYWnevLlcuXLF9HfW4K3t61auXBlh50AAAAAkTR4NzdOnT49ymYZk3SHQjnbXWL58ebRj9MiC+/fvj3aM9m/WEwAAAJDoa5oBAACAxIbQDAAAANggNAMAAAA2CM0AAACADUIzAAAAYIPQDAAAANggNAMAAAA2CM0AAACADUIzAAAAYIPQDAAAANggNAMAAAA2CM0AAACADUIzAAAAYIPQDAAAANggNAMAAAA2CM0AAACADUIzAAAAYIPQDAAAANggNAMAAAA2CM0AAACADUIzAAAAYIPQDAAAANggNAMAAAA2CM0AAACADUIzAAAAkJhD85QpU6RMmTKSMWNGc6pataqsWLHCufz+/fvSvXt3yZYtm6RPn16aNm0qly5dclvHmTNnpEGDBpI2bVrJkSOH9OvXTx49euQ2ZuPGjVK+fHnx9/eXQoUKyaxZsyJsy6RJkyR//vySOnVqqVy5suzatSsB7zkAAAC8iUdDc+7cueXjjz+WvXv3yp49e+TVV1+VRo0ayZEjR8zy3r17y88//yyLFi2STZs2yfnz56VJkybO6z9+/NgE5tDQUNm+fbvMnj3bBOLBgwc7x5w+fdqMqVmzphw4cEB69eolnTp1klWrVjnHLFiwQPr06SNDhgyRffv2SdmyZaVOnTpy+fLlp/yIAAAAIDHyaGhu2LCh1K9fXwoXLixFihSRDz/80Mwo79y5U27evCnTp0+XsWPHmjBdoUIFmTlzpgnHulytXr1afvvtN/nmm2+kXLlyUq9ePRkxYoSZNdYgraZOnSoFChSQMWPGSPHixaVHjx7SrFkzGTdunHM79DY6d+4s7du3lxIlSpjr6Mz1jBkzPPbYAAAAIPFINDXNOms8f/58uXPnjinT0Nnnhw8fSu3atZ1jihUrJnnz5pUdO3aY8/qzdOnSEhAQ4ByjM8QhISHO2Wod47oOa4y1Dg3XeluuY/z8/Mx5a0xkHjx4YG7H9QQAAADf5PHQfOjQITO7rPXGXbt2lSVLlpjZ3osXL0qqVKkkc+bMbuM1IOsypT9dA7O13FoW3RgNuffu3ZOrV6+awB7ZGGsdkRk5cqRkypTJecqTJ88TPhIAAABIrDwemosWLWpqjX/55Rfp1q2bBAcHm5KLxG7gwIGmhMQ6nT171tObBAAAgASSQjxMZ5O1o4XSuuXdu3fLhAkTpHnz5qZ04saNG26zzdo9IzAw0Pxff4bvcmF113AdE77jhp7Xbh1p0qSR5MmTm1NkY6x1REZnxvUEAAAA3+fxmebwwsLCTL2wBuiUKVPKunXrnMuOHz9uWsxpzbPSn1re4drlYs2aNSYQa4mHNcZ1HdYYax0a2vW2XMfoNuh5awwAAACSthSeLnHQjhe6c9+tW7dk3rx5pqeytoPTOuGOHTuaVnBZs2Y1Qfidd94xQbZKlSrm+kFBQSYct2nTRkaNGmVqkAcNGmR6O1uzwFonPXHiROnfv7906NBB1q9fLwsXLpRly5Y5t0NvQ8tCKlasKJUqVZLx48ebHRK1mwYAAADg0dCsM8Rt27aVCxcumJCsBzrRwPzaa6+Z5doWTjtZ6EFNdPZZu15MnjzZeX0tq1i6dKmphdYwnS5dOhN+hw8f7hyj7eY0IGvPZy370N7Q06ZNM+uyaCnIlStXTH9nDd7avm7lypURdg4EAABA0uTR0Kx9mKOjR+fTnst6ikq+fPlk+fLl0a6nRo0asn///mjHaP9mPQEAAACJvqYZAAAASGwIzQAAAIANQjMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQjMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQjMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQjMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQjMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQjMAAABgg9AMAAAA2CA0AwAAAAkRmv/444+4XA0AAABIOqG5UKFCUrNmTfnmm2/k/v37cb7xkSNHygsvvCAZMmSQHDlySOPGjeX48eNuY2rUqCHJkiVzO3Xt2tVtzJkzZ6RBgwaSNm1as55+/frJo0eP3MZs3LhRypcvL/7+/mb7Z82aFWF7Jk2aJPnz55fUqVNL5cqVZdeuXXG+bwAAAEjioXnfvn1SpkwZ6dOnjwQGBsrbb78dp4C5adMm6d69u+zcuVPWrFkjDx8+lKCgILlz547buM6dO8uFCxecp1GjRjmXPX782ATm0NBQ2b59u8yePdsE4sGDBzvHnD592ozRoH/gwAHp1auXdOrUSVatWuUcs2DBAnN/hgwZYu5f2bJlpU6dOnL58uW4PEQAAABI6qG5XLlyMmHCBDl//rzMmDHDBNlq1apJqVKlZOzYsXLlypUYrWflypXSrl07KVmypAmpGnZ11njv3r1u43QGWcO5dcqYMaNz2erVq+W3334zs966XfXq1ZMRI0aYWWMN0mrq1KlSoEABGTNmjBQvXlx69OghzZo1k3HjxjnXo9ut4bx9+/ZSokQJcx29Xb1/AAAASNqeaEfAFClSSJMmTWTRokXyySefyMmTJ+Xdd9+VPHnySNu2bU2Yjo2bN2+an1mzZnW7fO7cuZI9e3YTygcOHCh37951LtuxY4eULl1aAgICnJfpDHFISIgcOXLEOaZ27dpu69QxernScK1B3XWMn5+fOW+NCe/BgwfmNlxPAAAA8E1PFJr37Nkj//rXvyRnzpxmplYD86lTp0yphc5CN2rUKMbrCgsLM2UTL730kgnHllatWplZ5A0bNpjA/PXXX8tbb73lXH7x4kW3wKys87osujEadO/duydXr141ZR6RjbHWEVk9dqZMmZwn/aAAAAAA35QiLlfSgDxz5kyz0179+vVlzpw55qfOziothdBSC92pLqa0tvnw4cOydetWt8u7dOni/L/OKGtAr1WrlgnnBQsWFE/RAK810BYN4ARnAAAA3xSn0DxlyhTp0KGDqUfWEBsZ7WIxffr0GK1Pa4yXLl0qmzdvlty5c0c7VrtaKC0F0dCsNc7hd0K8dOmS+anLrJ/WZa5jtDY6TZo0kjx5cnOKbIy1jvC0C4eeAAAA4PviVJ5x4sQJM9MaVWBWqVKlkuDg4GjX43A4TGBesmSJrF+/3sxQ29HuF8q67apVq8qhQ4fculxoeYgGYt2hzxqzbt06t/XoGL3c2tYKFSq4jdFyET1vjQEAAEDSFafQrKUZuvNfeHqZtnyLTUmG1ivPmzfP9GrW+mE9aZ2x0hIM7YShO+n9+eef8tNPP5kdDKtXr25a3iltUafhuE2bNvLrr7+aNnKDBg0y67ZmgrWvsx6QpX///nLs2DGZPHmyLFy4UHr37u3cFi21+Oqrr8z2Hz16VLp162Za32k3DQAAACRtcQrNuhOcdrOIrCTjo48+ilWZh3bM0AOY6MyxddKeydYM8Nq1a00wLlasmPTt21eaNm0qP//8s3MdWlahpR36U2eFdSdBDdbDhw93jtEZ7GXLlpnZZW1tp63npk2bZjpoWJo3by6ffvqp6e+sret0Rltb4oXfORAAAABJT5xqmrWXcmSlFPny5TPLYkrLM6KjO9bpAVDs6O0uX7482jEazPfv3x/tGC0V0RMAAADwxDPNOqN88ODBCJdreUS2bNniskoAAADAt0Jzy5Yt5d///rfpnaz9jfWkO/L17NlTWrRoEf9bCQAAAHhbeYbunKc75mm/ZD0qoNVtQmuJY1PTDAAAAPhsaNYd9HRnPQ3PWpKhvY71wCNaWwwAAAD4mjiFZkuRIkXMCQAAAPBlcQrNWsOsh8nWg3/oQUW0NMOV1jcDAAAASTo06w5/GpobNGggpUqVkmTJksX/lgEAAADeHJrnz59vjqhXv379+N8iAAAAwBdazumOgIUKFYr/rQEAAAB8JTTr4awnTJhge0Q/AAAAIMmWZ2zdutUc2GTFihVSsmRJSZkypdvyxYsXx9f2AQAAAN4ZmjNnzixvvPFG/G8NAAAA4CuheebMmfG/JQAAAIAv1TSrR48eydq1a+WLL76QW7dumcvOnz8vt2/fjs/tAwAAALxzpvmvv/6SunXrypkzZ+TBgwfy2muvSYYMGeSTTz4x56dOnRr/WwoAAAB400yzHtykYsWKcv36dUmTJo3zcq1z1qMEAgAAAJLUZ5q3bNki27dvN/2aXeXPn1/+97//xde2AQAAAN470xwWFiaPHz+OcPm5c+dMmQYAAAAgST00BwUFyfjx453nkyVLZnYAHDJkCIfWBgAAgM+JU3nGmDFjpE6dOlKiRAm5f/++tGrVSk6cOCHZs2eXb7/9Nv63EgAAAPC20Jw7d2759ddfZf78+XLw4EEzy9yxY0dp3bq1246BAAAAQJINzeaKKVLIW2+9Fb9bAwAAAPhKaJ4zZ060y9u2bRvX7QEAAAB8IzRrn2ZXDx8+lLt375oWdGnTpiU0AwAAwKfEqXuGHtTE9aQ1zcePH5dq1aqxIyAAAAB8TpxCc2QKFy4sH3/8cYRZ6OiMHDlSXnjhBdPbOUeOHNK4cWMTvl1pd47u3btLtmzZJH369NK0aVO5dOmS2xg9nHeDBg3MLLeup1+/fvLo0SO3MRs3bpTy5cuLv7+/FCpUSGbNmhVheyZNmmQO0JI6dWqpXLmy7Nq1K9aPAwAAAHxPvIVma+fA8+fPx3j8pk2bTCDeuXOnrFmzxpR5aA/oO3fuOMf07t1bfv75Z1m0aJEZr+tv0qSJc7keZEUDc2hoqDlK4ezZs00gHjx4sHPM6dOnzZiaNWvKgQMHpFevXtKpUydZtWqVc8yCBQukT58+ptf0vn37pGzZsqat3uXLl+PlsQEAAEASq2n+6aef3M47HA65cOGCTJw4UV566aUYr2flypVu5zXs6kzx3r17pXr16nLz5k2ZPn26zJs3T1599VUzZubMmVK8eHETtKtUqSKrV6+W3377TdauXSsBAQFSrlw5GTFihAwYMECGDh1q6qynTp0qBQoUMP2llV5/69atMm7cOBOM1dixY6Vz587Svn17c16vs2zZMpkxY4a89957cXmYAAAAkJRDs5ZRuNIjAj7zzDMm2FrBNC40JKusWbOanxqedfa5du3azjHFihWTvHnzyo4dO0xo1p+lS5c2gdmiQbhbt25y5MgRef75580Y13VYY3TGWekstd7WwIEDncv9/PzMdfS6kXnw4IE5WUJCQuJ8vwEAAOCDoTksLCzeN0TXqSFWZ6pLlSplLrt48aKZKc6cObPbWA3Iuswa4xqYreXWsujGaNC9d++e2ZlRyzwiG3Ps2LEo67GHDRv2xPcbAAAASaym+UlobfPhw4fNUQa9gc5K68y4dTp79qynNwkAAACJaaZZd5iLKa0VttOjRw9ZunSpbN682Ryi2xIYGGhKJ27cuOE226zdM3SZNSZ8lwuru4brmPAdN/R8xowZzWG/kydPbk6RjbHWEZ524dATAAAAfF+cQvP+/fvNSeuNixYtai77/fffTfDUtm6utc7R0R0I33nnHVmyZIlpCac767mqUKGCpEyZUtatW2dazSltSact5qpWrWrO688PP/zQdLnQnQiVduLQQFyiRAnnmOXLl7utW8dY69ASEL0tvR2rXlvLRfS8BnoAAAAkbXEKzQ0bNjS9lbW9W5YsWcxlWhesnSdefvll6du3b4xLMrQzxo8//mjWZ9UgZ8qUycwA68+OHTuamW3dOVCDsIZsDbu6E6DSFnUajtu0aSOjRo0y6xg0aJBZtzUT3LVrV9PZo3///tKhQwdZv369LFy40HTHsOhtBAcHS8WKFaVSpUoyfvx40/rO6qYBAACApCtOoVk7ZGirNyswK/3/Bx98YEJsTEPzlClTzM8aNWq4Xa5t5dq1a2f+r23htJOFzjRrtwrtejF58mTnWJ3d1tIO7ZahYTpdunQm/A4fPtw5RmewNSBrz+cJEyaYEpBp06Y5282p5s2by5UrV0x/Zw3e2rpOW+KF3zkQAAAASU+cQrN2ndCAGZ5eduvWrRivR8sz7OjR+fRIfXqKSr58+SKUX4SnwVxLSqKjpRiUYwAAACBeume88cYbpmxh8eLFcu7cOXP6/vvvTSmF69H6AAAAgCQ706xHy3v33XelVatWZmdAs6IUKUxoHj16dHxvIwAAAOB9oTlt2rSmrlgD8qlTp8xlBQsWNPXEAAAAgK95ooObXLhwwZwKFy5sAnNMapQBAACAJBGar127JrVq1ZIiRYpI/fr1TXBWWp4R084ZAAAAgE+HZm3dpgcd0YOMaKmGa9s2bdMGAAAASFKvadYezatWrXI75LXSMo2//vorvrYNAAAA8N6ZZj1SnusMs+Xvv/92HoUPAAAASNKhWQ+VPWfOHOf5ZMmSSVhYmDmMdc2aNeNz+wAAAADvLM/QcKw7Au7Zs0dCQ0Olf//+cuTIETPTvG3btvjfSgAAAMDbZppLlSolv//+u1SrVk0aNWpkyjX0SIB6mGrt1wwAAAAk6ZlmPQJg3bp1zVEB//vf/ybMVgEAAADePNOsreYOHjyYMFsDAAAA+Ep5xltvvSXTp0+P/60BAAAAfGVHwEePHsmMGTNk7dq1UqFCBXMIbVdjx46Nr+0DAAAAvCs0//HHH5I/f345fPiwlC9f3lymOwS60vZzAAAAQJINzXrEvwsXLsiGDRuch83+7LPPJCAgIKG2DwAAAPCummaHw+F2fsWKFabdHAAAAODL4rQjYFQhGgAAAJCkHpq1Xjl8zTI1zAAAAPB1KWI7s9yuXTvx9/c35+/fvy9du3aN0D1j8eLF8buVAAAAgLeE5uDg4Aj9mgEAAABfF6vQPHPmzITbEgAAAMAXdwQEAAAAkgJCMwAAAGCD0AwAAAAk5tC8efNmadiwoeTKlcu0rvvhhx/clmunDqvNnXWqW7eu25i///5bWrduLRkzZpTMmTNLx44d5fbt225jDh48KC+//LKkTp1a8uTJI6NGjYqwLYsWLZJixYqZMaVLl5bly5cn0L0GAACAt/FoaNajCZYtW1YmTZoU5RgNyXrobuv07bffui3XwHzkyBFZs2aNLF261ATxLl26OJeHhIRIUFCQ5MuXT/bu3SujR4+WoUOHypdffukcs337dmnZsqUJ3Pv375fGjRub0+HDhxPongMAAMBnu2fEt3r16plTdLQndGBgYKTLjh49KitXrpTdu3dLxYoVzWWff/651K9fXz799FMzgz137lwJDQ2VGTNmSKpUqaRkyZJy4MABGTt2rDNcT5gwwYTzfv36mfMjRowwIXzixIkyderUeL/fAAAA8C6JvqZ548aNkiNHDilatKh069ZNrl275ly2Y8cOU5JhBWZVu3Zt8fPzk19++cU5pnr16iYwW+rUqSPHjx+X69evO8fo9VzpGL08Kg8ePDCz2K4nAAAA+KZEHZp19nfOnDmybt06+eSTT2TTpk1mZvrx48dm+cWLF02gdpUiRQrJmjWrWWaNCQgIcBtjnbcbYy2PzMiRIyVTpkzOk9ZKAwAAwDd5tDzDTosWLZz/153zypQpIwULFjSzz7Vq1fLotg0cOFD69OnjPK8zzQRnAAAA35SoZ5rDe+655yR79uxy8uRJc15rnS9fvuw25tGjR6ajhlUHrT8vXbrkNsY6bzcmqlpqq9ZaO3a4ngAAAOCbvCo0nzt3ztQ058yZ05yvWrWq3Lhxw3TFsKxfv17CwsKkcuXKzjHaUePhw4fOMbqTn9ZIZ8mSxTlGS0Bc6Ri9HAAAAPBoaNZ+ytrJQk/q9OnT5v9nzpwxy7Sbxc6dO+XPP/80obZRo0ZSqFAhs5OeKl68uKl77ty5s+zatUu2bdsmPXr0MGUd2jlDtWrVyuwEqO3ktDXdggULTLcM19KKnj17mi4cY8aMkWPHjpmWdHv27DHrAgAAADwamjWYPv/88+akNMjq/wcPHizJkyc3ByX5xz/+IUWKFDGht0KFCrJlyxZTGmHRlnJ6UBKtcdZWc9WqVXPrwaw76a1evdoEcr1+3759zfpdezm/+OKLMm/ePHM97Rv93XffmQOtlCpV6ik/IgAAAEiMPLojYI0aNcThcES5fNWqVbbr0E4ZGnijozsQatiOzptvvmlOAAAAgFfXNAMAAACeQGgGAAAAbBCaAQAAABuEZgAAAMAGoRkAAACwQWgGAAAAbBCaAQAAABuEZgAAAMAGoRkAAACwQWgGAAAAbBCaAQAAABuEZgAAAMAGoRkAAACwQWgGAAAAbBCaAQAAABuEZgAAAMAGoRkAAACwQWgGAAAAbBCaAQAAABuEZgAAAMAGoRkAAACwQWgGAAAAbBCaAQAAABuEZgAAAMAGoRkAAABIzKF58+bN0rBhQ8mVK5ckS5ZMfvjhB7flDodDBg8eLDlz5pQ0adJI7dq15cSJE25j/v77b2ndurVkzJhRMmfOLB07dpTbt2+7jTl48KC8/PLLkjp1asmTJ4+MGjUqwrYsWrRIihUrZsaULl1ali9fnkD3GgAAAN7Go6H5zp07UrZsWZk0aVKkyzXcfvbZZzJ16lT55ZdfJF26dFKnTh25f/++c4wG5iNHjsiaNWtk6dKlJoh36dLFuTwkJESCgoIkX758snfvXhk9erQMHTpUvvzyS+eY7du3S8uWLU3g3r9/vzRu3NicDh8+nMCPAAAAALxBCk/eeL169cwpMjrLPH78eBk0aJA0atTIXDZnzhwJCAgwM9ItWrSQo0ePysqVK2X37t1SsWJFM+bzzz+X+vXry6effmpmsOfOnSuhoaEyY8YMSZUqlZQsWVIOHDggY8eOdYbrCRMmSN26daVfv37m/IgRI0wInzhxognsAAAASNoSbU3z6dOn5eLFi6Ykw5IpUyapXLmy7Nixw5zXn1qSYQVmpeP9/PzMzLQ1pnr16iYwW3S2+vjx43L9+nXnGNfbscZYtxOZBw8emFls1xMAAAB8U6INzRqYlc4su9Lz1jL9mSNHDrflKVKkkKxZs7qNiWwdrrcR1RhreWRGjhxpQrx10lppAAAA+KZEG5oTu4EDB8rNmzedp7Nnz3p6kwAAAJDUQnNgYKD5eenSJbfL9by1TH9evnzZbfmjR49MRw3XMZGtw/U2ohpjLY+Mv7+/6djhegIAAIBvSrShuUCBAia0rlu3znmZ1g1rrXLVqlXNef1548YN0xXDsn79egkLCzO1z9YY7ajx8OFD5xjdya9o0aKSJUsW5xjX27HGWLcDAACApM2joVn7KWsnCz1ZO//p/8+cOWP6Nvfq1Us++OAD+emnn+TQoUPStm1b0xFD28Gp4sWLm64XnTt3ll27dsm2bdukR48eprOGjlOtWrUyOwFqOzltTbdgwQLTLaNPnz7O7ejZs6fpwjFmzBg5duyYaUm3Z88esy4AAADAoy3nNJjWrFnTed4KssHBwTJr1izp37+/6eWsreF0RrlatWom3OoBSCzaUk7Dba1atUzXjKZNm5rezhbdSW/16tXSvXt3qVChgmTPnt0cMMW1l/OLL74o8+bNM+3t/vOf/0jhwoVNW7tSpUo9tccCAAAAiZdHQ3ONGjVMP+ao6Gzz8OHDzSkq2ilDA290ypQpI1u2bIl2zJtvvmlOAAAAgNfUNAMAAACJBaEZAAAAsEFoBgAAAGwQmgEAAAAbhGYAAADABqEZAAAAsEFoBgAAAGwQmgEAAAAbhGYAAADABqEZAAAAsEFoBgAAAGwQmgEAAAAbhGYAAADABqEZAAAAsEFoBgAAAGwQmgEAAAAbhGYAAADABqEZAAAAsEFoBgAAAGwQmgEAAAAbhGYAAADABqEZAAAAsEFoBgAAAGwQmgEAAAAbhGYAAADAm0Pz0KFDJVmyZG6nYsWKOZffv39funfvLtmyZZP06dNL06ZN5dKlS27rOHPmjDRo0EDSpk0rOXLkkH79+smjR4/cxmzcuFHKly8v/v7+UqhQIZk1a9ZTu48AAABI/BJ1aFYlS5aUCxcuOE9bt251Luvdu7f8/PPPsmjRItm0aZOcP39emjRp4lz++PFjE5hDQ0Nl+/btMnv2bBOIBw8e7Bxz+vRpM6ZmzZpy4MAB6dWrl3Tq1ElWrVr11O8rAAAAEqcUksilSJFCAgMDI1x+8+ZNmT59usybN09effVVc9nMmTOlePHisnPnTqlSpYqsXr1afvvtN1m7dq0EBARIuXLlZMSIETJgwAAzi50qVSqZOnWqFChQQMaMGWPWodfXYD5u3DipU6fOU7+/AAAASHwS/UzziRMnJFeuXPLcc89J69atTbmF2rt3rzx8+FBq167tHKulG3nz5pUdO3aY8/qzdOnSJjBbNAiHhITIkSNHnGNc12GNsdYRlQcPHpj1uJ4AAADgmxJ1aK5cubIpp1i5cqVMmTLFlFK8/PLLcuvWLbl48aKZKc6cObPbdTQg6zKlP10Ds7XcWhbdGA3B9+7di3LbRo4cKZkyZXKe8uTJE2/3GwAAAIlLoi7PqFevnvP/ZcqUMSE6X758snDhQkmTJo1Ht23gwIHSp08f53kN2QRnAAAA35SoZ5rD01nlIkWKyMmTJ02ds+7gd+PGDbcx2j3DqoHWn+G7aVjn7cZkzJgx2mCunTZ0jOsJAAAAvsmrQvPt27fl1KlTkjNnTqlQoYKkTJlS1q1b51x+/PhxU/NctWpVc15/Hjp0SC5fvuwcs2bNGhNwS5Qo4Rzjug5rjLUOAAAAIFGH5nfffde0kvvzzz9Ny7g33nhDkidPLi1btjR1xB07djQlEhs2bDA7BrZv396EXe2coYKCgkw4btOmjfz666+mjdygQYNMb2edKVZdu3aVP/74Q/r37y/Hjh2TyZMnm/IPbWcHAAAAJPqa5nPnzpmAfO3aNXnmmWekWrVqpp2c/l9pWzg/Pz9zUBPtZqFdLzT0WjRgL126VLp162bCdLp06SQ4OFiGDx/uHKPt5pYtW2ZC8oQJEyR37twybdo02s0BAADAO0Lz/Pnzo12eOnVqmTRpkjlFRXccXL58ebTrqVGjhuzfvz/O2wkAAADflqjLMwAAAIDEgNAMAAAA2CA0AwAAADYIzQAAAIANQjMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQjMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQjMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQjMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQjMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQjMAAABgg9AMAAAA2CA0hzNp0iTJnz+/pE6dWipXriy7du3y9CYBAADAwwjNLhYsWCB9+vSRIUOGyL59+6Rs2bJSp04duXz5sqc3DQAAAB5EaHYxduxY6dy5s7Rv315KlCghU6dOlbRp08qMGTM8vWkAAADwoBSevPHEJDQ0VPbu3SsDBw50Xubn5ye1a9eWHTt2RBj/4MEDc7LcvHnT/AwJCXkq23v79u3/fzsunpSw0Pvxuu6H184myLoTar3eum62+emsm21+Ouv2xm1OyHWzzU9n3WyzD2zz3+ecueZpZCjrNhwOR6yvm8wRl2v5oPPnz8uzzz4r27dvl6pVqzov79+/v2zatEl++eUXt/FDhw6VYcOGeWBLAQAA8CTOnj0ruXPnjtV1mGmOI52R1vpnS1hYmPz999+SLVs2SZYs2VP5pJQnTx7zpGfMmDHBbw9PB8+r7+E59U08r76H5zRpPKcOh0Nu3boluXLlivX6CM3/J3v27JI8eXK5dOmS2+V6PjAwMMJ4f39/c3KVOXNmedr0RcCb2/fwvPoenlPfxPPqe3hOff85zZQpU5zWw46A/ydVqlRSoUIFWbdundvssZ53LdcAAABA0sNMswsttwgODpaKFStKpUqVZPz48XLnzh3TTQMAAABJF6HZRfPmzeXKlSsyePBguXjxopQrV05WrlwpAQEBkthoaYj2kw5fIgLvxvPqe3hOfRPPq+/hOfU9/vH8nNI9AwAAALBBTTMAAABgg9AMAAAA2CA0AwAAADYIzQAAAIANQrOXmjRpkuTPn19Sp04tlStXll27dnl6kxBHekh2PYqk66lYsWKe3izE0ubNm6Vhw4bmKFP6HP7www9uy3Wfa+3MkzNnTkmTJo3Url1bTpw44bHtxZM/p+3atYvw3q1bt67Hthf2Ro4cKS+88IJkyJBBcuTIIY0bN5bjx4+7jbl//750797dHOE3ffr00rRp0wgHPoP3Pa81atSI8H7t2rVrrG6H0OyFFixYYHpKaxuVffv2SdmyZaVOnTpy+fJlT28a4qhkyZJy4cIF52nr1q2e3iTEkvZ01/eifqCNzKhRo+Szzz6TqVOnyi+//CLp0qUz71v9Aw3vfE6VhmTX9+633377VLcRsbNp0yYTiHfu3Clr1qyRhw8fSlBQkHmuLb1795aff/5ZFi1aZMafP39emjRp4tHtxpM/r6pz585u71f9vRwr2nIO3qVSpUqO7t27O88/fvzYkStXLsfIkSM9ul2ImyFDhjjKli3r6c1APNJfrUuWLHGeDwsLcwQGBjpGjx7tvOzGjRsOf39/x7fffuuhrcSTPKcqODjY0ahRI49tE57c5cuXzXO7adMm5/syZcqUjkWLFjnHHD161IzZsWOHB7cUT/K8qldeecXRs2dPx5NgptnLhIaGyt69e81XuxY/Pz9zfseOHR7dNsSdfk2vXwE/99xz0rp1azlz5oynNwnx6PTp0+aASa7v20yZMpnSKt633m3jxo3m6+CiRYtKt27d5Nq1a57eJMTCzZs3zc+sWbOan/r3VWcpXd+rWi6XN29e3qte/Lxa5s6dK9mzZ5dSpUrJwIED5e7du7FaL0cE9DJXr16Vx48fRzhKoZ4/duyYx7YLcafBadasWeaPrn5dNGzYMHn55Zfl8OHDpj4L3k8Ds4rsfWstg/fR0gz92r5AgQJy6tQp+c9//iP16tUz4Sp58uSe3jzYCAsLk169eslLL71kQpTS92OqVKkkc+bMbmN5r3qPyJ5X1apVK8mXL5+ZoDp48KAMGDDA1D0vXrw4xusmNAMepn9kLWXKlDEhWt/YCxculI4dO3p02wBErUWLFs7/ly5d2rx/CxYsaGafa9Wq5dFtgz2tgdXJCfYhSRrPa5cuXdzer7pTtr5P9QOvvm9jgvIML6NfK+gMRvg9efV8YGCgx7YL8UdnOIoUKSInT5709KYgnljvTd63vk3Lq/R3NO/dxK9Hjx6ydOlS2bBhg+TOndt5ub4ftQzyxo0bbuN5r3qHqJ7XyOgElYrN+5XQ7GX0a6MKFSrIunXr3L6K0PNVq1b16LYhfty+fdt88tVPwfAN+vW9/sF1fd+GhISYLhq8b33HuXPnTE0z793ES/fp1GC1ZMkSWb9+vXlvutK/rylTpnR7r+pX+LqfCe9V731eI3PgwAHzMzbvV8ozvJC2mwsODpaKFStKpUqVZPz48aatSvv27T29aYiDd9991/SC1ZIMbW2krQT124SWLVt6etMQyw87rjMWuvOf/lLWHVF0JyKtsfvggw+kcOHC5hf6+++/b2rrtJ8ovO851ZPuf6A9fPUDkX7Q7d+/vxQqVMi0EkTi/ep+3rx58uOPP5p9Rqw6Zd0xV/un608ti9O/s/ocZ8yYUd555x0TmKtUqeLpzUccn1d9f+ry+vXrm/7bWtOsrQWrV69uyqpi7Il6b8BjPv/8c0fevHkdqVKlMi3odu7c6elNQhw1b97ckTNnTvNcPvvss+b8yZMnPb1ZiKUNGzaYFkfhT9qWzGo79/777zsCAgJMq7latWo5jh8/7unNRhyf07t37zqCgoIczzzzjGlRli9fPkfnzp0dFy9e9PRmIxqRPZ96mjlzpnPMvXv3HP/6178cWbJkcaRNm9bxxhtvOC5cuODR7caTPa9nzpxxVK9e3ZE1a1bz+7dQoUKOfv36OW7evOmIjWT/d2MAAAAAokBNMwAAAGCD0AwAAADYIDQDAAAANgjNAAAAgA1CMwAAAGCD0AwAAADYIDQDAAAANgjNAAAAgA1CMwD4qB9++MEc1lkPy66H8fZG+fPnl/Hjx3t6MwCA0AwArvQgqbVr15Y6depEWDZ58mTJnDmznDt3TrzB22+/Lc2aNZOzZ8/KiBEjogylyZIli3D6+OOPJTHYvXu3dOnSxdObAQDCYbQBIBwNmaVLl5ZPPvnEBE91+vRpc9mUKVOkTZs28Xp7Dx8+lJQpU8brOm/fvi0ZMmSQ9evXS82aNaMcp6G5Y8eO0rlzZ7fL9brp0qUTTwkNDZVUqVJ57PYBIDxmmgEgnDx58siECRPk3XffNWFZ5xY0WAYFBcnzzz8v9erVk/Tp00tAQIAJ0FevXnVed+XKlVKtWjUzI50tWzZ5/fXX5dSpU87lf/75p5nJXbBggbzyyiuSOnVqmTt3rvz111/SsGFDyZIliwmrJUuWlOXLl0e5jdevX5e2bdua8WnTpjXbdOLECbNs48aNJvSqV1991dyeXhYVHRsYGOh2sgLz8OHDJVeuXHLt2jXn+AYNGpggHhYWZs7r+vXDhG5DmjRp5LnnnpPvvvsuwgeRf/7zn+ZxyZo1qzRq1Mg8FpZ27dpJ48aN5cMPPzS3V7Ro0UjLM27cuCGdOnWSZ555RjJmzGju36+//upcPnToUClXrpx8/fXX5rqZMmWSFi1ayK1bt5xjdLtHjRplSlf8/f0lb9685nZjuq0AkiZCMwBEIjg4WGrVqiUdOnSQiRMnyuHDh+WLL74wIU2D8549e0xAvnTpkglYljt37kifPn3M8nXr1omfn5+88cYbzoBpee+996Rnz55y9OhRUwrSvXt3efDggWzevFkOHTpkZrk1mEdFQ6bexk8//SQ7duwwwb5+/fpm1vrFF1+U48ePm3Hff/+9XLhwwVwWF//9739N+NSgqiZNmiTbt2+X2bNnm/tmef/996Vp06YmwLZu3doEVb1vSrdJ76OG8y1btsi2bdvMfatbt66ZUbbo46XbvWbNGlm6dGmk2/Pmm2/K5cuXZcWKFbJ3714pX768eZ7+/vtv5xj9kKL13LoOPW3atMmt3GTgwIHmvG7zb7/9JvPmzTMfgGKzrQCSIC3PAABEdOnSJUf27Nkdfn5+jiVLljhGjBjhCAoKchtz9uxZLXFzHD9+PNJ1XLlyxSw/dOiQOX/69Glzfvz48W7jSpcu7Rg6dGiMtuv3338369i2bZvzsqtXrzrSpEnjWLhwoTl//fp1M2bDhg3RritfvnyOVKlSOdKlS+d22rx5s3PMqVOnHBkyZHAMGDDA3MbcuXPd1qG307VrV7fLKleu7OjWrZv5/9dff+0oWrSoIywszLn8wYMHZl2rVq0y54ODgx0BAQHm8vDbN27cOPP/LVu2ODJmzOi4f/++25iCBQs6vvjiC/P/IUOGONKmTesICQlxLu/Xr5/ZHqWX+/v7O7766qtIH4+YbCuApCmFp0M7ACRWOXLkMDXNOmuppQNaRrFhw4ZIZ4B1drNIkSKmRGLw4MHyyy+/mLINa4b5zJkzUqpUKef4ihUrul3/3//+t3Tr1k1Wr15tdkTUWdsyZcpEul06g5siRQqpXLmy8zItBdGSBmt2Nzb69etnZq5dPfvss87/a7nFp59+ah6L5s2bS6tWrSKso2rVqhHOHzhwwPxfZ59PnjzpLBmx3L9/3610RWvGo6tj1vVorbbeV1f37t1zW4/OjLveVs6cOc3stNLHR2f0dXY6qtuIybYCSHoIzQAQDQ2nelIa2LTuWEsnwtNgpnR5vnz55KuvvjK1uRqaNSyH/2o//E52Wv6gZQHLli0zwXnkyJEyZswYeeeddyShZc+e3dT3RkfLRrR1ndb2Pnr0yPmYxIQ+bhUqVDAfOsLT2mSL3Y6Huh59nCOrz9b6Y0v4nSq15tr68KI11/GxrQCSHmqaASCGtH72yJEjZiZTQ6brSQOf7iynNbmDBg0yM5nFixc3O+zFZgfErl27yuLFi6Vv374meEdG16vBVWezLdZtlyhRQuKb7rSo26RhVWfMI2tft3PnzgjndTutx01n4HXmPvzjpjvqxZSu5+LFiyawh1+PBv+YKFy4sAnOWj8d1W3Ex7YC8D2EZgCIId1ZT3c4a9mypekfrF/Xr1q1Stq3by+PHz82nSy0dODLL780X/FruzfdKTAm9OAjui7t1rFv3z5TBmKFzsiCn3Z00DZxW7duNSUFb731limp0MtjSztLaBh1PYWEhJhl2pNay0Z0dl27gsycOVM++uijCCF50aJFMmPGDPn9999lyJAhsmvXLunRo4dZpjsGaqjVbdOd6/Q+agDXkpTY9LzWshUt+9BSGZ2N11lv3SlRd1bUnSJjQruVDBgwQPr37y9z5swxz6Hel+nTp8frtgLwPYRmAIghLbfQbgoakLX9nNbgatjV0gDtJKGn+fPnm64OWpLRu3dvGT16dIzWrevUUK5BWTs1aH20HkwlKhpetYxAW9ppkNT98bRFXVz6PWsNtpY9uJ40VOo6tda5UqVKzgCsJSQaojWkaymDZdiwYea+ax22htFvv/3WOeutLfG0vENbuzVp0sTcR23hp3XC2jYuprTMQu9j9erVzQcVfYy0S4e267O6X8SEds3QmXy937otWqdt1TzH17YC8D0c3AQA8EQ0zC5ZssTMAAOAr2KmGQAAALBBaAYAAABs0HIOAPBEqPIDkBQw0wwAAADYIDQDAAAANgjNAAAAgA1CMwAAAGCD0AwAAADYIDQDAAAANgjNAAAAgA1CMwAAACDR+/8ABhiVAi13zF0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(master_df['yearsExperience'], bins=25, edgecolor='k')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Years of Experience')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83300351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 0\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "Q1 = master_df['yearsExperience'].quantile(0.25)\n",
    "Q3 = master_df['yearsExperience'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = master_df[(master_df['yearsExperience'] < lower_bound) | (master_df['yearsExperience'] > upper_bound)]\n",
    "print(f\"Number of outliers: {len(outliers)}\")\n",
    "print(outliers['yearsExperience'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04024b0",
   "metadata": {},
   "source": [
    "- No negative or missing values were found.\n",
    "- The distribution of values (0–24 years) is plausible for the job market context.\n",
    "- Zero years of experience is present for 39,822 records, likely representing new industry entrants, which is realistic.\n",
    "- No cleaning required for this column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a549980",
   "metadata": {},
   "source": [
    "##### `distanceFromCBD` Column Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "05ae386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    999473.000000\n",
      "mean         49.529449\n",
      "std          28.883195\n",
      "min           0.000000\n",
      "25%          25.000000\n",
      "50%          50.000000\n",
      "75%          75.000000\n",
      "max         500.000000\n",
      "Name: distanceFromCBD, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(master_df['distanceFromCBD'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65b98427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative values: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Negative values:\", (master_df['distanceFromCBD'] < 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3b11d4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values:\", master_df['distanceFromCBD'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cabf97cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 values: distanceFromCBD\n",
      "99.0    10171\n",
      "62.0    10166\n",
      "63.0    10150\n",
      "97.0    10149\n",
      "41.0    10145\n",
      "92.0    10134\n",
      "39.0    10128\n",
      "85.0    10126\n",
      "0.0     10116\n",
      "81.0    10115\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 values:\", master_df['distanceFromCBD'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a1c5f9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 83.  73.  30.  79.  29.  26.  81.   8.  91.  43.  66.  99.  96.  62.\n",
      "  69.  63.  70.  40.   6.  23.   9.   2.  32.  78.  14.  58.  35.  17.\n",
      "  54.  93.  82.  38.  87.  76.  22.  44.  72.  25.  36.   5.  71.  65.\n",
      "  53.  13.  33.  55.  61.  98.  59.  15.  75.  56.  11.  12.  34.  21.\n",
      "  52.  48.  97.  16.  28.  94.  41.  74.  60.  95.  80.  89.  10.  50.\n",
      "   4.  68.  49.   3.  88.  47.  51.  31.  18.  92.  42.  39.  67.  84.\n",
      "  86.  46.   0. 400.  19.  20.  57.  37.  64.   1.  27.  77.  90.  45.\n",
      "  85.   7.  24. 500.]\n"
     ]
    }
   ],
   "source": [
    "print(master_df['distanceFromCBD'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6241d0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApIAAADvCAYAAABBqzRQAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJnZJREFUeJzt3Qd0VFUex/GbUEIPIF0pQXoRaSKyiBRBBKSIBRs2bKiooCu7KqBSVuzKIrIHcNUFBQEVRUVAsQCCiIh0BEUBUaQEggjJ2/O7u2/OzJCE5JpMMpPv55whzMybN++9O5P55X/fvRPneZ5nAAAAgGyKz+4DAAAAACFIAgAAwAlBEgAAAE4IkgAAAHBCkAQAAIATgiQAAACcECQBAADghCAJAAAAJwRJAAAAOCFIAjBxcXFm5MiRJj9ZsWKFOeecc0zJkiXt9q1evTrH1j1t2jS7zu3bt5uCbPPmzaZr164mMTHRHo+5c+fm9SYBiDIESSAX+YEl+FKpUiXTsWNHM3/+fBPt1q1bZwNoTgeyY8eOmUsuucT89ttv5qmnnjIvv/yyqVmzZrrLfvTRRyHHNyEhwVSuXNmcd955ZsyYMeaXX37JkW1KSUmx+6rnixUDBw4033zzjRk9erQ9xq1atTL53c8//2yGDRtmGjRoYEqUKGH/0GjZsqV59NFHzf79+wPLqf2DXxdFixY1SUlJ5qabbjI7duzI9H1arFgxU61aNdOtWzfz7LPPmuTk5DzYUyA6FM7rDQAKgocffth+iOmr7fVBqA+uCy+80Lz99tumZ8+eJpqD5KhRo+yHdq1atXJsvVu3bjXff/+9mTx5srnxxhuz9Jg777zTtG7d2qSmptrw+Pnnn5sRI0aYJ5980rz++uumU6dOgWWvvvpqc/nll9vQmZ0gqX0V7W+0O3LkiFm6dKn5+9//bm6//XYTDVSl1vvm0KFD5qqrrrIBUlauXGnGjRtnlixZYj744IPA8qeddpoZO3as/f8ff/xhX68vvPCCef/998369ettEE3vfao/ZHbv3m3/aLjrrrvsa+itt94yZ5xxRoT3GMj/CJJABHTv3j2k2nPDDTfYqtn06dOjOkjmlj179tifZcuWzfJj2rdvb/r37x9y29dff227bi+++GIbIqpWrWpvL1SokL0UZH6lNivH+PDhw7byl5dUbezbt69tt6+++spWJIOpqqo/PIKpy16BM5iCooLzZ599Zs4///xM36fDhw83ixYtsu/Riy66yIbP4sWL58r+AdGKrm0gD+jDWx9IhQsXPuEDe+jQoaZ69eq2Wla/fn3z+OOP20qmX0XSB6gu+r9PXcAKSTqnUBU5ufbaa02pUqXMd999Z7voFATUXaeqi7++zOjDWh+sZcqUsevp3LmzWbZsWeB+VVXV/Szqqve7BU/W9asPZoU+bY+OQ+/eve0HtE/b3aFDB/t/rV/rdK0ANmvWzDz99NM2hDz//POZniOpqpaOU4UKFWzbKHBcf/319j4tV7FiRft/VSX9ffXPK12zZo3d7tq1a9tu0SpVqtjH7t27N2R7tLwet2XLFru89l9h57rrrrMVz3CvvPKKOeuss2zlrFy5cubcc88NqbiJTpHwj2fp0qVNjx49zLfffpvpcdF2+KcK3HvvvXab/Iqyv40K3ldccYV93r/85S/2vuPHj5tHHnnEnH766fb1qcf87W9/M0ePHg1Zv25X+NJrQcFMx7Np06aB18bs2bPtdR0rVRX1WjuZSZMmmZ9++slWB8NDpOgPswceeOCk61HbSPh7LyOqZD/44IO2Qq72ABCKIAlEwIEDB8yvv/5qq0D6kL/11lsD3XM+hTtVPXRO4AUXXGA/MBUk9UF/zz332GX0gfzSSy/ZIKIuSd/gwYPtcyggBVfaFCq1Ln3IPvbYY/ZDW929umRG26hwoorefffdZz9It23bZgPd8uXL7TIKNepOFoUJnWOnS8OGDTNc74cffmjDmiqOCizaL3VBt2vXLhDqbr75Zrs+0fq1zuB9zS5VKXXcwgNYMG2PKpfahvvvv98899xz5sorrwwEZ4XIiRMn2v+rKubva79+/extCxYssIFdgVCPVbf5jBkzbDdseqH90ksvtefdqdtV/1e7+d3mPl1XF3yRIkVs+Nd1/YGhIO7TNig4Kuj/4x//sO2kAKjgl9l5q9puvc5kwIABdj0K3MEU4hVudZ7poEGD7G06zeChhx4yLVq0sI9X4Nc+aH/D6TWqINqrVy+7zL59++z/X331VXP33Xfb1772Sacx6BikpaWZzKhrWe0YXnXOjF7/et/psmvXLnvs9NqvU6eOfc1lldpBMnsNAQWWByDXTJ06VSnihEtCQoI3bdq0kGXnzp1r73v00UdDbu/fv78XFxfnbdmyJXDb8OHDvfj4eG/JkiXezJkz7eOefvrpkMcNHDjQ3n7HHXcEbktLS/N69OjhFS1a1Pvll18Ct2u5ESNGBK736dPHLrN169bAbTt37vRKly7tnXvuuYHb/OdevHhxlo7HmWee6VWqVMnbu3dv4Lavv/7a7ss111wTuE3r03q1/pPJyrLNmjXzypUrd0K7bNu2zV6fM2eOvb5ixYoM16HjFX6cfCkpKSfcNn36dLu82sinx+q266+/PmTZvn37eqecckrg+ubNm+0x0e2pqakhy6oNJTk52Stbtqw3aNCgkPt3797tJSYmnnB7OO27tmX8+PEht/vbOGDAgJDbV69ebW+/8cYbQ24fNmyYvX3RokWB22rWrGlv+/zzzwO3vf/++/a24sWLe99//33g9kmTJmXpNaT2UztmVYcOHdJ97zVs2ND77rvvQpb1Xw+Ztb+OafPmzbP8/EBBQUUSiIAJEybYqpUu6h5TV7CqO+ri87377ru2muhX+Xzq6lbWCx7lrWpe48aN7ajb2267zVaGwh/nCx5IoS5LXdfAA1UHM6riqPLSp08f21XrU9e5KkyffvqpOXjwYLaPgSpCmsJHXbrly5cP3K4BDDpXTfufW1Sxy2zkrX+e4Lx58+xAi+wKPm/u999/txWws88+215ftWrVCcvfcsstIddV/VU3uH9cNQ2PKnSq/sXHh/6aVhuKXkvqsldF0a+66aLXUJs2bczixYuzvR+ZbaPfPn51PPj1Ke+8807I7Y0aNTJt27YNXNc2+V3FNWrUOOF2VXQzo2OjrvvsUBe7/77T+0dVV1XudcpGdkfzn+w1BBRUBEkgAnSeW5cuXexFXab60NUHrR/qROdg6RzG8A9Lv6tY9/s0lcmUKVNsd7M+3KZOnRoIGMEUQoLDoNSrV8/+zKjrUx+w6tJUt3o4bYsCTvj0KVnhb39G61UI0jmiuUGnEWQWQhTENSBHXa06R1LnbeqYhp/7lxGdozpkyBB7CoFCpbrCdY6lKLiECw5SovMQRd2/ou5etZ1eI5nNAekHMz1f8EV/CPgDllz52x/cftomdQuHn3OoIB78+kxvH3UuqKh7Pr3b/X3PiM7VzW6Q03mj/vtOp3iojdRFvnHjRjvKOydfQ0BBxahtIA/oA1lVyWeeecYGAlUXs0tTmPgVMK0j/IMf/6MK46ZNm0yTJk0yXEYhfNasWfacSE3JpGOrwTJPPPGEvU3VqMzoHD+d66nzWc8880y7vAK3wkt65/5lNGI8K4OgfP56dX6jP4AkWFYHk2Qko9HJ6f3Bkp6M9tF13zXARhVt/eGlP6Rc6TxhhVdNFZRVP/74o/2DIDxEA6AiCeQZjYD1Kx2iUbQ7d+48oeqyYcOGwP0+jRLWAAwN7mjevLntJk+v8qWwEd5lqFAlGc37qIqWRgmrahNO26IQ7FeVshoqgrc/o/WqEpgbU8woIGqEuwb5nIy6ozWNjEZwa1CIBh1p0Exm+6pK2sKFC+0gHVU0NRhHXfXhleDs0KhotZ0GzmS2jGiCe7/qFnzJ6bku1X7aJr8S6tO8qOpiz2jC+JyigTpqxzfeeONPr0unb/jvu6xQWJesvIaAgoYgCeRRlUzdj6qs+F3XGuGrD7jgaWpEo2MVYnRel/9YnWeobnBVNDXiVx/mGgmbnuD1qeqj6xoJrOl8MqoYaQTzm2++GdL9ref4z3/+Y0cEq5tR/OAX/I0iGdE5lqrWadR58PJr1661x0L7n9M06lwTSqvrWCPbM6IwGF4R07aK373tT14dvq9+hS388eGjoLND56cqsOuPhfCKpv88CjVqB42qTu+8zpz6Rh+f3z7h+6XZBUSjx3OTztnUa0jnZPp/DAVTV76+3eZkdO6oQqSmhsoKjfTWlEeq+Ou0FACh6NoGIkAn+vuVRX3gKZCpsqMqlh/KVHFRd7emulGA0wedApYCncKQX4HSh6W6+FQF0zlbGqyiQRmaQ09TowQHMs3T995779lBORrUoO3Q+ZmaXsefFzE9eg4NUFBo1GAedZNqHj+FKk0jFBy2FKQ09YwqoppbUOfsqUqWnvHjx9tArEEYmpRdFSZNl6Ouxj/7Xd+ffPKJ7eZXGNfAFU04rfPhtO45c+ak2/3rU7j95z//aauJOs6qCmtya7WNfzzV1atzFl977TV7nqkGDKm7XBdNhaTjokB36qmn2nbT+auu1IWq14ECjAbiaLoeHVt9s4v+gNB0Oto2TUmkqWk0HY+m4FGb/vDDD7aNNb1N+B8lf4Zej3odvfjiizZM67zSL774wh47BV+9dnOT/hhQO6o99LoL/mYbDWjS5P7Bg3tEr0l/7kf1AKgarmOmttR7L6P3qZbVH04KkXofqNqq15LeTwDC5PWwcaCgTf9TrFgxOw3OxIkTA1O5+DSly9133+1Vq1bNK1KkiFe3bl07PYu/3JdffukVLlw4ZEofOX78uNe6dWv7uH379gWm/ylZsqSdwqdr165eiRIlvMqVK9vpXcKnlElvWptVq1Z53bp180qVKmUf27Fjx5DpXHyTJ0/2ateu7RUqVChL07h8+OGHXrt27ew0MGXKlPF69erlrVu3LmQZl+l//IuOW8WKFe00RaNHj/b27NlzwmPCp//Rvmq6mxo1atipmTRFUc+ePb2VK1eGPE7737JlSzs1UvAx+/HHH+1UPZqOR9PEXHLJJXa6pPDj6k+tEzz1Unrb45syZYqdckbbpOlvNKXNggULTth/tZOeV6+t008/3bv22mtP2PbsTv8Tvo1y7Ngxb9SoUV5SUpI9ztWrV7dTUf3+++8hy2n6H00zFU7rHTx4cJa2IyM6rnqP1KtXz+6vXptqE7X1gQMHMpz+R1NolS9f3rvooovs+yiz96nat0qVKt7555/vPfPMM97BgweztG1AQRSnf8LDJYDop+5vnR+YnXPBAADIDs6RBAAAgBOCJAAAAJwQJAEAAOCEcyQBAADghIokAAAAnBAkAQAAEB0TkutbGvQ1cJpIOTtfrwYAAIDI0JmP+nIGfQmCvmkr3wRJhUj/e3oBAACQf+3YscOcdtpp+SdIqhLpb5j/1XAAAADIPw4ePGgLf35uyzdB0u/OVogkSAIAAORfJzsNkcE2AAAAcEKQBAAAgBOCJAAAAJwQJAEAAOCEIAkAAAAnBEkAAAA4IUgCAADACUESAAAATgiSAAAAcEKQBAAAgBOCJAAAAJwQJAEAAOCEIAkAAAAnBEkAAAA4IUgCAADACUESAAAATgiSAAAAcEKQBAAAgBOCJAAAAJwQJAEAAOCEIAkAAAAnBEkAAAA4IUgCAADACUESAAAATgiSAAAAcEKQBAAAgBOCJAAAAJwQJAEAAOCEIAkAAAAnBEkAAAA4IUgCAADACUESAAAATgiSAAAAcEKQBAAAgBOCJAAAAJwQJAEAAOCEIAkAAAAnBEkAAAA4IUgCAADACUESAAAATgiSAAAAcEKQBAAAgBOCJAAAAJwQJAEAAOCEIAkAAAAnBEkAAAA4IUgCAADACUESAAAATgiSAAAAcEKQBAAAgBOCJAAAAJwQJAEAAOCEIAkAAAAnBEkAAAA4IUgCAADACUESAAAATgiSAAAAcEKQBAAAgBOCJAAAAJwQJAEAAOCEIAkAAAAnhd0ehsxs3rzZJCcnm1hVunRpU7du3bzeDAAAkMcIkrkQIuvVqxex56tSKs7c3LKomfTlH2b3IS9iz7tp0ybCJAAABRxBMof5lchXXnnFNGzYMNefr/j+TabhkpvNZQ9NM0fK5n6AXb9+vbnqqqtiuuIKAACyhiCZSxQiW7RokftPtDPemCXGNGzQwJhqZ+b+8wEAAPwfg20AAADghCAJAAAAJzEfJFNSUsyqVavsT0QX2g4AgPwt5oPkhg0bTMuWLe1PRBfaDgCA/I3BNgAAxLDU1FTzySefmF27dpmqVaua9u3bm0KFCuX1ZiFG2i/bFcklS5aYXr16mWrVqpm4uDgzd+7c3NkyAADwp8yePdvUqVPHdOzY0VxxxRX2p67rduR/s6Og/bIdJA8fPmyaNWtmJkyYkDtbBAAA/jSFjf79+5umTZuapUuX2vl/9VPXdXt+CiOI3vbLdtd29+7d7QUAAOTf7tChQ4eanj172p7D+Pj/1Y3OPvtse71Pnz5m2LBhpnfv3vmqmxTR1365fo7k0aNH7cV38OBBE0lHjhwJfCNLJPjP4z9vrInk8Yz1YwkAuUXn1G3fvt1Mnz49EEJ8uj58+HBzzjnn2OXOO++8PNtORH/75XqQHDt2rBk1apTJK2oI0df6Rfp527VrZ2JNXhzPWD2WAJBbNDBDmjRpku79/u3+cshfdkVR++V6kFRqvueee0IqktWrVzeRUqtWrYh+97X/XdT+88aaSB7PWD+WAJBbNLpX1q5da7tDw+n24OWQv1SNovbL9SCZkJBgL3mlePHikf3u67DnjTV5cTxj9VgCQG7RFDH6I3zMmDEh59hJWlqa7S1MSkqyyyH/iab2i/kJyQEAKGg0AOOJJ54w8+bNswMzgkf96rpuf/zxx/N8oAaiv/2yXZE8dOiQ2bJlS+D6tm3bzOrVq0358uVNjRo1cnr7AACAg379+plZs2bZ0b8amOFTJUu3637kX/2ipP2yHSRXrlxpJ8T0+ec/Dhw40EybNi1ntw4AADhT2NAUMfn5m1EQ3e2X7SCpYeae55lo0aBBA/Pll1/an4gutB0A/HkKHXk9RQxit/1i/ru2S5QoEdFBNsg5tB0AAPkbg20AAADghCAJAAAAJzHftR1pKSkp9ueqVasi8nzF928ymhZ8/YYN5sjutFx/vkh91SQAAMj/CJI5bMOGDfbnoEGDIvJ8VUrFmZtbFjWTnrjC7D4UuUFQpUuXjthzAQCA/IkgmcM0UahopLEGi0TKRRF7pv+FyLp160bwGQEAQH4U50V4Lh9913ZiYqI5cOCAKVOmTCSfGgAAADmY1xhsAwAAACcESQAAADghSAIAAMAJQRIAAABOCJIAAABwQpAEAACAE4IkAAAAnBAkAQAA4IQgCQAAACcESQAAADghSAIAAMAJQRIAAABOCJIAAABwQpAEAACAE4IkAAAAnBAkAQAA4IQgCQAAACcESQAAADghSAIAAMAJQRIAAABOCJIAAABwQpAEAACAE4IkAAAAnBAkAQAA4IQgCQAAACcESQAAADghSAIAAMAJQRIAAABOCJIAAABwQpAEAACAE4IkAAAAnBAkAQAA4IQgCQAAACcESQAAADghSAIAAMAJQRIAAABOCJIAAABwQpAEAACAE4IkAAAAnBAkAQAA4IQgCQAAACcESQAAADghSAIAAMAJQRIAAABOCJIAAABwQpAEAACAE4IkAAAAnBAkAQAA4IQgCQAAACcESQAAADghSAIAAMAJQRIAAABOCJIAAABwQpAEAACAE4IkAAAAnBAkAQAA4IQgCQAAACcESQAAADghSAIAAMAJQRIAAABOCJIAAABwQpAEAACAk8ImwjzPsz8PHjwY6acGAABAFvg5zc9t+SZIJicn25/Vq1eP9FMDAAAgm7ktMTExw/vjvJNFzRyWlpZmdu7caUqXLm3i4uIikqgVWnfs2GHKlCmT68+HnEcbRjfaL/rRhtGPNoxuB/Og/RQPFSKrVatm4uPj809FUhtz2mmnRfpp7YHnzRPdaMPoRvtFP9ow+tGG0a1MhNsvs0qkj8E2AAAAcEKQBAAAgJOYD5IJCQlmxIgR9ieiE20Y3Wi/6EcbRj/aMLol5OP2i/hgGwAAAMSGmK9IAgAAIHcQJAEAAOCEIAkAAAAnBEkAAAA4iekgOWHCBFOrVi1TrFgx06ZNG/PFF1/k9Sbh/5YsWWJ69eplZ8zXNxzNnTs35H6NAXvooYdM1apVTfHixU2XLl3M5s2bQ5b57bffzJVXXmknZy1btqy54YYbzKFDhyK8JwXT2LFjTevWre03VFWqVMn06dPHbNy4MWSZ33//3QwePNiccsopplSpUubiiy82P//8c8gyP/zwg+nRo4cpUaKEXc+9995rjh8/HuG9KZgmTpxozjjjjMAEx23btjXz588P3E/7RZdx48bZ36V33XVX4DbaMH8bOXKkbbPgS4MGDaKu/WI2SL722mvmnnvuscPlV61aZZo1a2a6detm9uzZk9ebBmPM4cOHbZso7KfnscceM88++6x54YUXzPLly03JkiVt++mN5VOI/Pbbb82CBQvMvHnzbDi96aabIrgXBdfHH39sf8EtW7bMHv9jx46Zrl272nb13X333ebtt982M2fOtMvrq1H79esXuD81NdX+Avzjjz/M559/bl566SUzbdo0+wcEcp++YUzh48svvzQrV640nTp1Mr1797bvKaH9oseKFSvMpEmT7B8GwWjD/K9x48Zm165dgcunn34afe3nxaizzjrLGzx4cOB6amqqV61aNW/s2LF5ul04kV6Gc+bMCVxPS0vzqlSp4o0fPz5w2/79+72EhARv+vTp9vq6devs41asWBFYZv78+V5cXJz3008/RXgPsGfPHtseH3/8caC9ihQp4s2cOTOwzPr16+0yS5cutdffffddLz4+3tu9e3dgmYkTJ3plypTxjh49mgd7gXLlynn/+te/aL8okpyc7NWtW9dbsGCB16FDB2/IkCH2dtow/xsxYoTXrFmzdO+LpvaLyYqk0rn+ylZ3aPB3fOv60qVL83TbcHLbtm0zu3fvDmk/fd+nTk/w208/1Z3dqlWrwDJaXu2sCiYi68CBA/Zn+fLl7U+9/1SlDG5DddnUqFEjpA2bNm1qKleuHFhGVeeDBw8GqmKIDFU2ZsyYYSvK6uKm/aKHegZUlQpuK6ENo8PmzZvtKV61a9e2vWzqqo629itsYtCvv/5qfzEGH1zR9Q0bNuTZdiFrFCIlvfbz79NPnQ8SrHDhwjbI+MsgMtLS0ux5We3atTNNmjSxt6kNihYtasN+Zm2YXhv79yH3ffPNNzY46pQRnYM1Z84c06hRI7N69WraLwoo/OvULXVth+M9mP+1adPGdkXXr1/fdmuPGjXKtG/f3qxduzaq2i8mgySAyFZE9Isv+NweRAd9gCk0qqI8a9YsM3DgQHsuFvK/HTt2mCFDhthzlDWgFNGne/fugf/r/FYFy5o1a5rXX3/dDjKNFjHZtV2hQgVTqFChE0Y36XqVKlXybLuQNX4bZdZ++hk+cEoj1TSSmzaOnNtvv90OdFq8eLEdvOFTG+gUk/3792fahum1sX8fcp8qHnXq1DEtW7a0I/E1AO6ZZ56h/aKAuj71O7BFixa2N0YX/RGgQYr6vypTtGF0KVu2rKlXr57ZsmVLVL0H42P1l6N+MS5cuDCk+03X1Y2D/C0pKcm+CYLbT+d86NxHv/30U28w/TL1LVq0yLaz/qpD7tIYKYVIdYXquKvNgun9V6RIkZA21PRAOv8nuA3VtRr8B4GqK5qKRt2riDy9f44ePUr7RYHOnTvb46+Ksn/ROeM6z87/P20YXQ4dOmS2bt1qp72LqvegF6NmzJhhR/lOmzbNjvC96aabvLJly4aMbkLejjT86quv7EUvwyeffNL+//vvv7f3jxs3zrbXm2++6a1Zs8br3bu3l5SU5B05ciSwjgsuuMBr3ry5t3z5cu/TTz+1IxcHDBiQh3tVcNx6661eYmKi99FHH3m7du0KXFJSUgLL3HLLLV6NGjW8RYsWeStXrvTatm1rL77jx497TZo08bp27eqtXr3ae++997yKFSt6w4cPz6O9Kljuv/9+O8p+27Zt9j2m65r14IMPPrD3037RJ3jUttCG+dvQoUPt71C9Bz/77DOvS5cuXoUKFewsGNHUfjEbJOW5556zjVC0aFE7HdCyZcvyepPwf4sXL7YBMvwycODAwBRADz74oFe5cmX7B0Hnzp29jRs3hqxj7969NjiWKlXKTndw3XXX2YCK3Jde2+kyderUwDIK/bfddpudUqZEiRJe3759bdgMtn37dq979+5e8eLF7S9Q/WI9duxYHuxRwXP99dd7NWvWtL8f9eGj95gfIoX2i/4gSRvmb5dddplXtWpV+x489dRT7fUtW7ZEXfvF6Z/I1T8BAAAQK2LyHEkAAADkPoIkAAAAnBAkAQAA4IQgCQAAACcESQAAADghSAIAAMAJQRIAAABOCJIAAABwQpAEEFFxcXFm7ty5JhaNHDnSVK5cOab3EQCCESQB/GnXXnutDU+6FClSxIap888/30yZMsWkpaWFLLtr1y7TvXv3LK03mgLZ+vXrzahRo8ykSZOytY+5bffu3eaOO+4wtWvXNgkJCaZ69eqmV69eZuHChYFlatWqFWi/QoUKmWrVqpkbbrjB7Nu3L7DMRx99FFgmPj7eJCYmmubNm5v77rvP7i+AgokgCSBHXHDBBTZQbN++3cyfP9907NjRDBkyxPTs2dMcP348sFyVKlVsoIk1W7dutT979+6d4T7+8ccfEd0mtUXLli3NokWLzPjx480333xj3nvvPds2gwcPDln24Ycftu33ww8/mFdffdUsWbLE3HnnnSesc+PGjWbnzp1mxYoV5q9//av58MMPTZMmTey6ARRAEf1mbwAxaeDAgV7v3r1PuH3hwoWefs1Mnjw5cJuuz5kzx/7/6NGj3uDBg70qVap4CQkJXo0aNbwxY8bY+2rWrGmX9S+6Llu2bPEuuugir1KlSl7JkiW9Vq1aeQsWLAh5Xi07evRo77rrrvNKlSrlVa9e3Zs0aVLIMjt27PAuv/xyr1y5cl6JEiW8li1besuWLQvcP3fuXK958+Z2u5KSkryRI0d6x44dS3f/R4wYEbKt/q9W/7g8+uijXtWqVb1atWrZ29esWeN17NjRK1asmFe+fHlv0KBBXnJy8gnHU/ug/UxMTPRGjRpln3/YsGF2m0899VRvypQpmbZL9+7d7XKHDh064b59+/aFHK+nnnoq5P5HHnnEa9SoUeD64sWL7X4FP05SUlK8+vXre+3atct0WwDEJiqSAHJNp06dTLNmzczs2bPTvf/ZZ581b731lnn99ddtpUuVMHWziipeMnXqVFsp868fOnTIXHjhhbZr9quvvrKVUHXVqpIW7IknnjCtWrWyy9x2223m1ltvtc/hr6NDhw7mp59+ss//9ddf2y5avxv+k08+Mddcc42tqK5bt852V0+bNs2MHj063f0YNmyY3U7RtgZ39Wo79bwLFiww8+bNM4cPHzbdunUz5cqVs/s0c+ZMW9W7/fbbQ9apKqIqf6oMPvnkk2bEiBG2uqvHLV++3Nxyyy3m5ptvNj/++GO62/Tbb7/Z6qMqjyVLljzh/rJly2bYbjoub7/9tmnTpo05meLFi9tt+eyzz8yePXtOujyAGJPXSRZA7FYk5bLLLvMaNmyYbkXyjjvu8Dp16uSlpaWl+9jgZTPTuHFj77nnngupsF111VWB61q/KnsTJ06011WdLF26tLd3795019e5c+dAZdT38ssv26piRrSd4b9SdVwqV65sK6++F1980VYUg6uE77zzjhcfH+/t3r078DjtQ2pqamAZVf3at28fuH78+HFbkZ0+fXq627N8+XK7PbNnz/ZORs9VtGhRuz5VSfW4Nm3ahFQfM6pIyvz58+19ek4ABQsVSQC5SnlQAzQyGqSzevVqU79+fXs+3gcffHDS9amaqApgw4YNbVWtVKlSdqBLeEXyjDPOCPxfz6/zFv2KmZ5TA0XKly+f7nOoQqlzBrVu/zJo0CBbaUxJScnW/jdt2tQULVo0cF3bqiptcJWwXbt2thrqV0ylcePGdlCLTwOYtC6fBsWccsopGVYB/5fDs+7ee++1x2XNmjWBgTg9evQwqampJ32s/1wZtTOA2FU4rzcAQGxTcEpKSkr3vhYtWpht27bZwTnq3r300ktNly5dzKxZszJcn0Kkuokff/xxU6dOHdu12r9//xMGsmj0eDCFHL/rWo85WVjVCOx+/fqdcF+xYsVMdqTXrZwV6W1/ZvsUrm7duvb+DRs2ZOn5KlSoYI+n/9inn37atG3b1ixevNi2ycnaWPzTEgAUHFQkAeQaneen0bwXX3xxhsuUKVPGXHbZZWby5MnmtddeM2+88YY9v08UnMIrYjoXT5XMvn372gqdKo0anZwdqlaq+uY/T3oBV9VBBavwS3CV0IUqqap46lzJ4H3SelWZzSmqtupczAkTJoQ8l2///v2ZPl4VTzly5Eimy+n+F1980Zx77rmmYsWKf3KrAUQbgiSAHHH06FE7Z6EGaqxatcqMGTPGToWjASIauJIeDSKZPn26rZpt2rTJDjxRMPQHgqjCpW5Wrdef01DVMg3eURBUILviiisyrMplZMCAAfZ5+vTpY0Pcd999ZwPs0qVL7f0PPfSQ+fe//22rkt9++62tuM2YMcM88MADf/o4XXnllbaqOXDgQLN27Vpb8dM8j1dffbXtvs5JCpEK4meddZbdv82bN9t90SAnVRuDJScn2+Os7vsvvvjCdnUrGJ5zzjkhy6krXctpXTom6pb/9ddfzcSJE3N02wFEB4IkgByhEcJVq1a14U8jqRWQFFjefPPNQHUrXOnSpc1jjz1mR1e3bt3aVhbffffdQNVPI6/Vja1JtHVOox8+NXJZAUejtVV1UwUxO3TOos7HrFSpkh0BrsrmuHHjAtupdWqEtZbRdp199tnmqaeeMjVr1vzTx6lEiRLm/ffft9VQrVvd8p07dzbPP/+8yWmahFyhXvNGDh061M73qIniFc7Dg5/Cs9pPk5Er/KtLXvuv8zCDqWqqZTQ/pY6Zur0ViBs1apTj2w8g/4vTiJu83ggAAABEHyqSAAAAcEKQBAAAgBOCJAAAAJwQJAEAAOCEIAkAAAAnBEkAAAA4IUgCAADACUESAAAATgiSAAAAcEKQBAAAgBOCJAAAAIyL/wIwaisdWc+0LgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,2))\n",
    "plt.boxplot(master_df['distanceFromCBD'], vert=False)\n",
    "plt.xlabel('Distance from CBD')\n",
    "plt.title('Boxplot of Distance from CBD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "57485a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows over 60 km: 390233\n"
     ]
    }
   ],
   "source": [
    "max_distance = 60 # include people coming in from Johor Bahru\n",
    "num_over = (master_df['distanceFromCBD'] > max_distance).sum()\n",
    "\n",
    "print(f\"Rows over {max_distance} km: {num_over}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8da314b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap values to 60 km\n",
    "master_df['distanceFromCBD'] = master_df['distanceFromCBD'].clip(upper=max_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e09e8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60., 30., 29., 26.,  8., 43., 40.,  6., 23.,  9.,  2., 32., 14.,\n",
       "       58., 35., 17., 54., 38., 22., 44., 25., 36.,  5., 53., 13., 33.,\n",
       "       55., 59., 15., 56., 11., 12., 34., 21., 52., 48., 16., 28., 41.,\n",
       "       10., 50.,  4., 49.,  3., 47., 51., 31., 18., 42., 39., 46.,  0.,\n",
       "       19., 20., 57., 37.,  1., 27., 45.,  7., 24.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df['distanceFromCBD'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d52855",
   "metadata": {},
   "source": [
    "To ensure that our analysis reflects realistic commuting patterns for Singapore and neighboring Johor Bahru (Malaysia), we evaluated the `distanceFromCBD` column for outliers and data quality:\n",
    "\n",
    "- **Analysis of unique values and descriptive statistics** revealed that a large number of records (over 390,000) had distances exceeding 60 km, which is not plausible for daily commutes to the Singapore CBD, even when accounting for cross-border workers from Johor Bahru.\n",
    "- While assignment instructions encourage removal of irrelevant or unrealistic observations, dropping all records above 60 km would eliminate almost 40% of the data, severely impacting the representativeness and statistical power of our analysis.\n",
    "- **To balance data quality with completeness, we opted to cap all values above 60 km at 60.** This approach preserves the full dataset for further analysis and machine learning, while mitigating the influence of extreme outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809f5cc",
   "metadata": {},
   "source": [
    "##### `salaryInThousands` Columnn Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8d0d8881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    9.994730e+05\n",
      "mean     1.260673e+02\n",
      "std      1.000259e+04\n",
      "min      0.000000e+00\n",
      "25%      8.800000e+01\n",
      "50%      1.140000e+02\n",
      "75%      1.410000e+02\n",
      "max      1.000000e+07\n",
      "Name: salaryInThousands, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(master_df['salaryInThousands'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "73782876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative values: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Negative values:\", (master_df['salaryInThousands'] < 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "63913a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero salaries: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Zero salaries:\", (master_df['salaryInThousands'] == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9cf391c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 0\n",
      "\n",
      "\n",
      "Top 10 values: salaryInThousands\n",
      "108.0    10466\n",
      "114.0    10403\n",
      "107.0    10368\n",
      "112.0    10355\n",
      "104.0    10286\n",
      "103.0    10282\n",
      "110.0    10261\n",
      "109.0    10241\n",
      "115.0    10222\n",
      "105.0    10213\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values:\", master_df['salaryInThousands'].isnull().sum())\n",
    "print(\"\\n\")\n",
    "print(\"Top 10 values:\", master_df['salaryInThousands'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5a5ad6",
   "metadata": {},
   "source": [
    "Check for the zero salaries and the high salary provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "174ac9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobId</th>\n",
       "      <th>companyId</th>\n",
       "      <th>jobRole</th>\n",
       "      <th>education</th>\n",
       "      <th>major</th>\n",
       "      <th>industry</th>\n",
       "      <th>yearsExperience</th>\n",
       "      <th>distanceFromCBD</th>\n",
       "      <th>salaryInThousands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>903154</th>\n",
       "      <td>JOB1362685311220</td>\n",
       "      <td>COMP34</td>\n",
       "      <td>vice_president</td>\n",
       "      <td>high_school</td>\n",
       "      <td>none</td>\n",
       "      <td>oil</td>\n",
       "      <td>11.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10000000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   jobId companyId         jobRole    education major  \\\n",
       "903154  JOB1362685311220    COMP34  vice_president  high_school  none   \n",
       "\n",
       "       industry  yearsExperience  distanceFromCBD  salaryInThousands  \n",
       "903154      oil             11.0             60.0         10000000.0  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View rows with salary of exactly 10,000,000\n",
    "million_salary = master_df[master_df['salaryInThousands'] == 1e7]\n",
    "million_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b3600c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobId</th>\n",
       "      <th>companyId</th>\n",
       "      <th>jobRole</th>\n",
       "      <th>education</th>\n",
       "      <th>major</th>\n",
       "      <th>industry</th>\n",
       "      <th>yearsExperience</th>\n",
       "      <th>distanceFromCBD</th>\n",
       "      <th>salaryInThousands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30535</th>\n",
       "      <td>JOB1362684438246</td>\n",
       "      <td>COMP44</td>\n",
       "      <td>junior</td>\n",
       "      <td>doctoral</td>\n",
       "      <td>math</td>\n",
       "      <td>auto</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495823</th>\n",
       "      <td>JOB1362684903671</td>\n",
       "      <td>COMP34</td>\n",
       "      <td>junior</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>oil</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651906</th>\n",
       "      <td>JOB1362685059763</td>\n",
       "      <td>COMP25</td>\n",
       "      <td>cto</td>\n",
       "      <td>high_school</td>\n",
       "      <td>none</td>\n",
       "      <td>auto</td>\n",
       "      <td>6.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815959</th>\n",
       "      <td>JOB1362685223816</td>\n",
       "      <td>COMP42</td>\n",
       "      <td>manager</td>\n",
       "      <td>doctoral</td>\n",
       "      <td>engineering</td>\n",
       "      <td>finance</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827986</th>\n",
       "      <td>JOB1362685235843</td>\n",
       "      <td>COMP40</td>\n",
       "      <td>vice_president</td>\n",
       "      <td>masters</td>\n",
       "      <td>engineering</td>\n",
       "      <td>web</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   jobId companyId         jobRole    education        major  \\\n",
       "30535   JOB1362684438246    COMP44          junior     doctoral         math   \n",
       "495823  JOB1362684903671    COMP34          junior         none         none   \n",
       "651906  JOB1362685059763    COMP25             cto  high_school         none   \n",
       "815959  JOB1362685223816    COMP42         manager     doctoral  engineering   \n",
       "827986  JOB1362685235843    COMP40  vice_president      masters  engineering   \n",
       "\n",
       "       industry  yearsExperience  distanceFromCBD  salaryInThousands  \n",
       "30535      auto             11.0              7.0                0.0  \n",
       "495823      oil              1.0             25.0                0.0  \n",
       "651906     auto              6.0             60.0                0.0  \n",
       "815959  finance             18.0              6.0                0.0  \n",
       "827986      web              3.0             29.0                0.0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View rows with salary of 0\n",
    "zero_salary = master_df[master_df['salaryInThousands'] == 0]\n",
    "zero_salary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b091247a",
   "metadata": {},
   "source": [
    "We inspected records with salaries of 0 and 10,000,000. The single extremely high value was judged to be an error and removed. The five entries with zero salary were likely unpaid internships or similar roles; these were also removed to maintain consistency in the analysis. \n",
    "\n",
    "We can remove these rows as they amount to around 6 rows, which does not impact the analaysis or the model training so much \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9df03213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extreme high salaries\n",
    "master_df = master_df[master_df['salaryInThousands'] <= 1000]\n",
    "\n",
    "# Remove zero salary rows (if not analyzing unpaid roles)\n",
    "master_df = master_df[master_df['salaryInThousands'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6d87cac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobId</th>\n",
       "      <th>companyId</th>\n",
       "      <th>jobRole</th>\n",
       "      <th>education</th>\n",
       "      <th>major</th>\n",
       "      <th>industry</th>\n",
       "      <th>yearsExperience</th>\n",
       "      <th>distanceFromCBD</th>\n",
       "      <th>salaryInThousands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JOB1362684407687</td>\n",
       "      <td>COMP37</td>\n",
       "      <td>cfo</td>\n",
       "      <td>masters</td>\n",
       "      <td>math</td>\n",
       "      <td>health</td>\n",
       "      <td>10.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JOB1362684407688</td>\n",
       "      <td>COMP19</td>\n",
       "      <td>ceo</td>\n",
       "      <td>high_school</td>\n",
       "      <td>none</td>\n",
       "      <td>web</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JOB1362684407697</td>\n",
       "      <td>COMP56</td>\n",
       "      <td>janitor</td>\n",
       "      <td>high_school</td>\n",
       "      <td>none</td>\n",
       "      <td>health</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JOB1362684407698</td>\n",
       "      <td>COMP7</td>\n",
       "      <td>ceo</td>\n",
       "      <td>masters</td>\n",
       "      <td>physics</td>\n",
       "      <td>education</td>\n",
       "      <td>7.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB1362684407699</td>\n",
       "      <td>COMP4</td>\n",
       "      <td>junior</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>oil</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999468</th>\n",
       "      <td>JOB1362685407682</td>\n",
       "      <td>COMP56</td>\n",
       "      <td>vice_president</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>health</td>\n",
       "      <td>19.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999469</th>\n",
       "      <td>JOB1362685407683</td>\n",
       "      <td>COMP24</td>\n",
       "      <td>cto</td>\n",
       "      <td>high_school</td>\n",
       "      <td>none</td>\n",
       "      <td>finance</td>\n",
       "      <td>12.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999470</th>\n",
       "      <td>JOB1362685407684</td>\n",
       "      <td>COMP23</td>\n",
       "      <td>junior</td>\n",
       "      <td>high_school</td>\n",
       "      <td>none</td>\n",
       "      <td>education</td>\n",
       "      <td>16.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999471</th>\n",
       "      <td>JOB1362685407685</td>\n",
       "      <td>COMP3</td>\n",
       "      <td>cfo</td>\n",
       "      <td>masters</td>\n",
       "      <td>none</td>\n",
       "      <td>health</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999472</th>\n",
       "      <td>JOB1362685407686</td>\n",
       "      <td>COMP59</td>\n",
       "      <td>junior</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>none</td>\n",
       "      <td>education</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999467 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   jobId companyId         jobRole    education      major  \\\n",
       "0       JOB1362684407687    COMP37             cfo      masters       math   \n",
       "1       JOB1362684407688    COMP19             ceo  high_school       none   \n",
       "2       JOB1362684407697    COMP56         janitor  high_school       none   \n",
       "3       JOB1362684407698     COMP7             ceo      masters    physics   \n",
       "4       JOB1362684407699     COMP4          junior         none       none   \n",
       "...                  ...       ...             ...          ...        ...   \n",
       "999468  JOB1362685407682    COMP56  vice_president    bachelors  chemistry   \n",
       "999469  JOB1362685407683    COMP24             cto  high_school       none   \n",
       "999470  JOB1362685407684    COMP23          junior  high_school       none   \n",
       "999471  JOB1362685407685     COMP3             cfo      masters       none   \n",
       "999472  JOB1362685407686    COMP59          junior    bachelors       none   \n",
       "\n",
       "         industry  yearsExperience  distanceFromCBD  salaryInThousands  \n",
       "0          health             10.0             60.0              130.0  \n",
       "1             web              3.0             60.0              101.0  \n",
       "2          health             24.0             30.0              102.0  \n",
       "3       education              7.0             60.0              144.0  \n",
       "4             oil              8.0             29.0               79.0  \n",
       "...           ...              ...              ...                ...  \n",
       "999468     health             19.0             60.0               88.0  \n",
       "999469    finance             12.0             35.0              160.0  \n",
       "999470  education             16.0             60.0               64.0  \n",
       "999471     health              6.0              5.0              149.0  \n",
       "999472  education             20.0             11.0               88.0  \n",
       "\n",
       "[999467 rows x 9 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "87ef3b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([130., 101., 102., 144.,  79., 193.,  47., 172., 126., 122.,  95.,\n",
       "        32.,  68., 105.,  76., 202., 131., 158.,  82., 159., 132., 165.,\n",
       "       100., 164., 115., 206., 183., 114., 104., 141., 119.,  91., 106.,\n",
       "       112., 116., 148., 173., 113.,  70.,  88.,  96., 118., 140., 161.,\n",
       "       111.,  55., 217.,  62.,  86.,  80., 168., 133., 129.,  89., 135.,\n",
       "        94., 169.,  90., 110., 179., 176.,  84., 162., 107., 125., 205.,\n",
       "        72.,  99., 145., 170., 180., 117., 207., 151., 108., 121., 166.,\n",
       "        49.,  75., 194.,  52., 154., 146., 171., 139., 174.,  57., 127.,\n",
       "        78., 152., 155.,  65., 123.,  48.,  42.,  50., 156., 178., 128.,\n",
       "        83.,  69.,  85.,  59., 136.,  93.,  67., 134.,  97., 160., 195.,\n",
       "       163.,  63., 153.,  74.,  73., 120., 187.,  92., 223.,  51.,  77.,\n",
       "       103., 150.,  45., 137., 143.,  34., 124., 109., 190.,  98.,  58.,\n",
       "       149., 157., 147.,  71.,  64., 167.,  46., 184.,  33.,  87., 188.,\n",
       "        60.,  23., 177.,  61., 196., 175.,  54.,  38.,  66., 185., 181.,\n",
       "       197., 248., 142.,  81., 189.,  56., 204., 214.,  53.,  39., 218.,\n",
       "       199., 192., 240., 210., 186., 201., 225.,  44.,  35.,  29.,  36.,\n",
       "        37., 200., 191., 209.,  43., 247., 229., 138., 220.,  40., 182.,\n",
       "        28., 198., 232., 203., 241., 212., 238.,  31., 213., 208., 234.,\n",
       "       243.,  41.,  24., 237., 222.,  30., 230., 231., 219., 221., 233.,\n",
       "       254., 235., 215., 211.,  27.,  25., 239., 226., 259., 216., 283.,\n",
       "       236., 224., 228.,  26.,  20., 273., 271., 255., 227., 251., 246.,\n",
       "       249., 245., 258., 244., 265., 263., 252., 269., 242., 266., 264.,\n",
       "       253., 270., 268., 250., 285., 262., 261., 272., 275., 280.,  21.,\n",
       "        22., 257., 292., 256., 288., 260.,  19., 286., 277., 276., 274.,\n",
       "       278.,  17., 267., 281.,  18., 282., 279., 294., 284., 290., 301.,\n",
       "       289., 293., 298., 287.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df[\"salaryInThousands\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52201e18",
   "metadata": {},
   "source": [
    "### Overview Summary of the Cleaned `master_df.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "75720d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>num_unique</th>\n",
       "      <th>example_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jobId</td>\n",
       "      <td>999467</td>\n",
       "      <td>[JOB1362684407687, JOB1362684407688, JOB136268...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>companyId</td>\n",
       "      <td>63</td>\n",
       "      <td>[COMP37, COMP19, COMP56, COMP7, COMP4, COMP54,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jobRole</td>\n",
       "      <td>8</td>\n",
       "      <td>[cfo, ceo, janitor, junior, cto, vice_presiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>education</td>\n",
       "      <td>5</td>\n",
       "      <td>[masters, high_school, none, bachelors, doctoral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>major</td>\n",
       "      <td>9</td>\n",
       "      <td>[math, none, physics, biology, literature, che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>industry</td>\n",
       "      <td>7</td>\n",
       "      <td>[health, web, education, oil, finance, auto, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yearsExperience</td>\n",
       "      <td>25</td>\n",
       "      <td>[10.0, 3.0, 24.0, 7.0, 8.0, 21.0, 13.0, 1.0, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>distanceFromCBD</td>\n",
       "      <td>61</td>\n",
       "      <td>[60.0, 30.0, 29.0, 26.0, 8.0, 43.0, 40.0, 6.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>salaryInThousands</td>\n",
       "      <td>279</td>\n",
       "      <td>[130.0, 101.0, 102.0, 144.0, 79.0, 193.0, 47.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              column  num_unique  \\\n",
       "0              jobId      999467   \n",
       "1          companyId          63   \n",
       "2            jobRole           8   \n",
       "3          education           5   \n",
       "4              major           9   \n",
       "5           industry           7   \n",
       "6    yearsExperience          25   \n",
       "7    distanceFromCBD          61   \n",
       "8  salaryInThousands         279   \n",
       "\n",
       "                                      example_values  \n",
       "0  [JOB1362684407687, JOB1362684407688, JOB136268...  \n",
       "1  [COMP37, COMP19, COMP56, COMP7, COMP4, COMP54,...  \n",
       "2  [cfo, ceo, janitor, junior, cto, vice_presiden...  \n",
       "3  [masters, high_school, none, bachelors, doctoral]  \n",
       "4  [math, none, physics, biology, literature, che...  \n",
       "5  [health, web, education, oil, finance, auto, s...  \n",
       "6  [10.0, 3.0, 24.0, 7.0, 8.0, 21.0, 13.0, 1.0, 2...  \n",
       "7  [60.0, 30.0, 29.0, 26.0, 8.0, 43.0, 40.0, 6.0,...  \n",
       "8  [130.0, 101.0, 102.0, 144.0, 79.0, 193.0, 47.0...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print unique summary for master dataset\n",
    "executive_summary = pd.DataFrame({\n",
    "    \"column\": master_df.columns,\n",
    "    \"num_unique\": [master_df[c].nunique(dropna=False) for c in master_df.columns],\n",
    "    \"example_values\": [master_df[c].unique()[:10] for c in master_df.columns]  # Show first 10 unique values\n",
    "})\n",
    "executive_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f69700e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 999467 entries, 0 to 999472\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   jobId              999467 non-null  object \n",
      " 1   companyId          999467 non-null  object \n",
      " 2   jobRole            999467 non-null  object \n",
      " 3   education          999467 non-null  object \n",
      " 4   major              999467 non-null  object \n",
      " 5   industry           999467 non-null  object \n",
      " 6   yearsExperience    999467 non-null  float64\n",
      " 7   distanceFromCBD    999467 non-null  float64\n",
      " 8   salaryInThousands  999467 non-null  float64\n",
      "dtypes: float64(3), object(6)\n",
      "memory usage: 76.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yearsExperience</th>\n",
       "      <th>distanceFromCBD</th>\n",
       "      <th>salaryInThousands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>999467.000000</td>\n",
       "      <td>999467.000000</td>\n",
       "      <td>999467.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.992371</td>\n",
       "      <td>41.717154</td>\n",
       "      <td>116.062697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.212443</td>\n",
       "      <td>20.083119</td>\n",
       "      <td>38.717688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>114.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>141.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>301.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       yearsExperience  distanceFromCBD  salaryInThousands\n",
       "count    999467.000000    999467.000000      999467.000000\n",
       "mean         11.992371        41.717154         116.062697\n",
       "std           7.212443        20.083119          38.717688\n",
       "min           0.000000         0.000000          17.000000\n",
       "25%           6.000000        25.000000          88.000000\n",
       "50%          12.000000        50.000000         114.000000\n",
       "75%          18.000000        60.000000         141.000000\n",
       "max          24.000000        60.000000         301.000000"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show general info about the cleaned DataFrame\n",
    "master_df.info()\n",
    "\n",
    "# Show summary statistics for numeric columns\n",
    "master_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a53c05e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts for jobRole:\n",
      "jobRole\n",
      "senior            125830\n",
      "vice_president    125166\n",
      "manager           125061\n",
      "cto               124985\n",
      "janitor           124909\n",
      "ceo               124703\n",
      "junior            124517\n",
      "cfo               124296\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for education:\n",
      "education\n",
      "high_school    236860\n",
      "none           236713\n",
      "bachelors      175405\n",
      "doctoral       175269\n",
      "masters        175220\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for major:\n",
      "major\n",
      "none           532057\n",
      "chemistry       58841\n",
      "literature      58644\n",
      "engineering     58566\n",
      "business        58498\n",
      "physics         58381\n",
      "compsci         58352\n",
      "biology         58351\n",
      "math            57777\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for industry:\n",
      "industry\n",
      "web          143140\n",
      "auto         142876\n",
      "finance      142797\n",
      "education    142736\n",
      "oil          142687\n",
      "health       142674\n",
      "service      142557\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Categorical columns to summarize\n",
    "cat_cols = ['jobRole', 'education', 'major', 'industry']\n",
    "\n",
    "for col in cat_cols:\n",
    "    print(f\"\\nValue counts for {col}:\")\n",
    "    print(master_df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ae24ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values by column:\n",
      "jobId                0\n",
      "companyId            0\n",
      "jobRole              0\n",
      "education            0\n",
      "major                0\n",
      "industry             0\n",
      "yearsExperience      0\n",
      "distanceFromCBD      0\n",
      "salaryInThousands    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Duplicated Values in Master Dataset:  0\n"
     ]
    }
   ],
   "source": [
    "# Double-check for any missing data\n",
    "print(\"\\nMissing values by column:\")\n",
    "print(master_df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "print(\"Duplicated Values in Master Dataset: \", master_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c36302",
   "metadata": {},
   "source": [
    "**After all cleaning steps, we checked the resulting DataFrame to confirm data integrity:**\n",
    "\n",
    "- No missing values remain\n",
    "- Categorical columns are standardized and have reasonable distributions\n",
    "- Numeric columns contain only plausible, in-domain values\n",
    "\n",
    "Above are the final data info, summary statistics, and category distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cddf6b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Master Dataset saved to: /Users/saikeerthan/NYP-AI/Year3/Big_Data/third_assignment/master_df.csv\n"
     ]
    }
   ],
   "source": [
    "# save the cleaned master_df to a CSV file\n",
    "master_df_path = os.path.join(os.getcwd(), \"master_df.csv\")\n",
    "master_df.to_csv(master_df_path, index=False)\n",
    "\n",
    "print(f\"Cleaned Master Dataset saved to: {master_df_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1661e02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobId</th>\n",
       "      <th>companyId</th>\n",
       "      <th>jobRole</th>\n",
       "      <th>education</th>\n",
       "      <th>major</th>\n",
       "      <th>industry</th>\n",
       "      <th>yearsExperience</th>\n",
       "      <th>distanceFromCBD</th>\n",
       "      <th>salaryInThousands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JOB1362684407687</td>\n",
       "      <td>COMP37</td>\n",
       "      <td>cfo</td>\n",
       "      <td>masters</td>\n",
       "      <td>math</td>\n",
       "      <td>health</td>\n",
       "      <td>10.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JOB1362684407688</td>\n",
       "      <td>COMP19</td>\n",
       "      <td>ceo</td>\n",
       "      <td>high_school</td>\n",
       "      <td>none</td>\n",
       "      <td>web</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JOB1362684407697</td>\n",
       "      <td>COMP56</td>\n",
       "      <td>janitor</td>\n",
       "      <td>high_school</td>\n",
       "      <td>none</td>\n",
       "      <td>health</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JOB1362684407698</td>\n",
       "      <td>COMP7</td>\n",
       "      <td>ceo</td>\n",
       "      <td>masters</td>\n",
       "      <td>physics</td>\n",
       "      <td>education</td>\n",
       "      <td>7.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB1362684407699</td>\n",
       "      <td>COMP4</td>\n",
       "      <td>junior</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>oil</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999468</th>\n",
       "      <td>JOB1362685407682</td>\n",
       "      <td>COMP56</td>\n",
       "      <td>vice_president</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>health</td>\n",
       "      <td>19.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999469</th>\n",
       "      <td>JOB1362685407683</td>\n",
       "      <td>COMP24</td>\n",
       "      <td>cto</td>\n",
       "      <td>high_school</td>\n",
       "      <td>none</td>\n",
       "      <td>finance</td>\n",
       "      <td>12.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999470</th>\n",
       "      <td>JOB1362685407684</td>\n",
       "      <td>COMP23</td>\n",
       "      <td>junior</td>\n",
       "      <td>high_school</td>\n",
       "      <td>none</td>\n",
       "      <td>education</td>\n",
       "      <td>16.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999471</th>\n",
       "      <td>JOB1362685407685</td>\n",
       "      <td>COMP3</td>\n",
       "      <td>cfo</td>\n",
       "      <td>masters</td>\n",
       "      <td>none</td>\n",
       "      <td>health</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999472</th>\n",
       "      <td>JOB1362685407686</td>\n",
       "      <td>COMP59</td>\n",
       "      <td>junior</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>none</td>\n",
       "      <td>education</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999467 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   jobId companyId         jobRole    education      major  \\\n",
       "0       JOB1362684407687    COMP37             cfo      masters       math   \n",
       "1       JOB1362684407688    COMP19             ceo  high_school       none   \n",
       "2       JOB1362684407697    COMP56         janitor  high_school       none   \n",
       "3       JOB1362684407698     COMP7             ceo      masters    physics   \n",
       "4       JOB1362684407699     COMP4          junior         none       none   \n",
       "...                  ...       ...             ...          ...        ...   \n",
       "999468  JOB1362685407682    COMP56  vice_president    bachelors  chemistry   \n",
       "999469  JOB1362685407683    COMP24             cto  high_school       none   \n",
       "999470  JOB1362685407684    COMP23          junior  high_school       none   \n",
       "999471  JOB1362685407685     COMP3             cfo      masters       none   \n",
       "999472  JOB1362685407686    COMP59          junior    bachelors       none   \n",
       "\n",
       "         industry  yearsExperience  distanceFromCBD  salaryInThousands  \n",
       "0          health             10.0             60.0              130.0  \n",
       "1             web              3.0             60.0              101.0  \n",
       "2          health             24.0             30.0              102.0  \n",
       "3       education              7.0             60.0              144.0  \n",
       "4             oil              8.0             29.0               79.0  \n",
       "...           ...              ...              ...                ...  \n",
       "999468     health             19.0             60.0               88.0  \n",
       "999469    finance             12.0             35.0              160.0  \n",
       "999470  education             16.0             60.0               64.0  \n",
       "999471     health              6.0              5.0              149.0  \n",
       "999472  education             20.0             11.0               88.0  \n",
       "\n",
       "[999467 rows x 9 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be401a1",
   "metadata": {},
   "source": [
    "### Non-PySpark ML Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b258100a",
   "metadata": {},
   "source": [
    "#### Universal Tensors and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c71eae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"salaryInThousands\"\n",
    "\n",
    "# # Categorical & numeric feature lists\n",
    "cat_cols = [\"jobRole\", \"education\", \"major\", \"industry\", \"companyId\"]\n",
    "num_cols = [\"yearsExperience\", \"distanceFromCBD\"]\n",
    "\n",
    "# X = master_df[cat_cols + num_cols]\n",
    "# y = master_df[TARGET]\n",
    "\n",
    "# subsampling:\n",
    "sample_df = master_df.sample(n=150_000, random_state=42)\n",
    "\n",
    "# Continue as before with train/val/test splits\n",
    "X = sample_df[cat_cols + num_cols]\n",
    "y = sample_df[TARGET]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7702a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bd83cfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols),   # tree models don’t need scaling\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cf1738",
   "metadata": {},
   "source": [
    "#### RandomForest Regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "25e3a2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;prep&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;cat&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;jobRole&#x27;, &#x27;education&#x27;,\n",
       "                                                   &#x27;major&#x27;, &#x27;industry&#x27;,\n",
       "                                                   &#x27;companyId&#x27;]),\n",
       "                                                 (&#x27;num&#x27;, &#x27;passthrough&#x27;,\n",
       "                                                  [&#x27;yearsExperience&#x27;,\n",
       "                                                   &#x27;distanceFromCBD&#x27;])])),\n",
       "                (&#x27;rf&#x27;,\n",
       "                 RandomForestRegressor(n_estimators=300, n_jobs=-1,\n",
       "                                       random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;prep&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;cat&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;jobRole&#x27;, &#x27;education&#x27;,\n",
       "                                                   &#x27;major&#x27;, &#x27;industry&#x27;,\n",
       "                                                   &#x27;companyId&#x27;]),\n",
       "                                                 (&#x27;num&#x27;, &#x27;passthrough&#x27;,\n",
       "                                                  [&#x27;yearsExperience&#x27;,\n",
       "                                                   &#x27;distanceFromCBD&#x27;])])),\n",
       "                (&#x27;rf&#x27;,\n",
       "                 RandomForestRegressor(n_estimators=300, n_jobs=-1,\n",
       "                                       random_state=42))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>prep: ColumnTransformer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for prep: ColumnTransformer</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(transformers=[(&#x27;cat&#x27;, OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;jobRole&#x27;, &#x27;education&#x27;, &#x27;major&#x27;, &#x27;industry&#x27;,\n",
       "                                  &#x27;companyId&#x27;]),\n",
       "                                (&#x27;num&#x27;, &#x27;passthrough&#x27;,\n",
       "                                 [&#x27;yearsExperience&#x27;, &#x27;distanceFromCBD&#x27;])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>cat</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;jobRole&#x27;, &#x27;education&#x27;, &#x27;major&#x27;, &#x27;industry&#x27;, &#x27;companyId&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>OneHotEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>num</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;yearsExperience&#x27;, &#x27;distanceFromCBD&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>passthrough</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">?<span>Documentation for RandomForestRegressor</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestRegressor(n_estimators=300, n_jobs=-1, random_state=42)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('prep',\n",
       "                 ColumnTransformer(transformers=[('cat',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['jobRole', 'education',\n",
       "                                                   'major', 'industry',\n",
       "                                                   'companyId']),\n",
       "                                                 ('num', 'passthrough',\n",
       "                                                  ['yearsExperience',\n",
       "                                                   'distanceFromCBD'])])),\n",
       "                ('rf',\n",
       "                 RandomForestRegressor(n_estimators=300, n_jobs=-1,\n",
       "                                       random_state=42))])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"prep\", preprocess),\n",
    "        (\"rf\",  RandomForestRegressor(\n",
    "                    n_estimators=300,\n",
    "                    max_depth=None,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42,\n",
    "                )\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "18c1655e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Random Forest Results -----\n",
      "RandomRF   | VAL | RMSE:    20.43 | MAE:    16.33 | MSE:     417.25 | R²:  0.724\n",
      "RandomRF   | TEST | RMSE:    20.43 | MAE:    16.38 | MSE:     417.37 | R²:  0.722\n"
     ]
    }
   ],
   "source": [
    "def evaluate(name, model, X_val, y_val, X_test, y_test):\n",
    "    for split, X_, y_ in [(\"VAL\", X_val, y_val), (\"TEST\", X_test, y_test)]:\n",
    "        preds = model.predict(X_)\n",
    "        # rmse = mean_squared_error(y_, preds, squared=False)\n",
    "        # mse  = mean_squared_error(y_, preds, squared=True)\n",
    "        mse = np.mean((y_ - preds) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae  = mean_absolute_error(y_, preds)\n",
    "        r2   = r2_score(y_, preds)\n",
    "        print(f\"{name:<10} | {split} | RMSE: {rmse:8.2f} | MAE: {mae:8.2f} \"\n",
    "              f\"| MSE: {mse:10.2f} | R²: {r2:6.3f}\")\n",
    "\n",
    "print(\"----- Random Forest Results -----\")\n",
    "evaluate(\"RandomRF\", rf_model, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e1c44",
   "metadata": {},
   "source": [
    "#### XGBoost Regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "13c264b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cc9a5e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;prep&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;cat&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;jobRole&#x27;, &#x27;education&#x27;,\n",
       "                                                   &#x27;major&#x27;, &#x27;industry&#x27;,\n",
       "                                                   &#x27;companyId&#x27;]),\n",
       "                                                 (&#x27;num&#x27;, &#x27;passthrough&#x27;,\n",
       "                                                  [&#x27;yearsExperience&#x27;,\n",
       "                                                   &#x27;distanceFromCBD&#x27;])])),\n",
       "                (&#x27;xgb&#x27;,\n",
       "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                              colsample_bylevel=None, colsample_bynode=None,\n",
       "                              colsample_bytree=0.8...\n",
       "                              feature_types=None, feature_weights=None,\n",
       "                              gamma=None, grow_policy=None,\n",
       "                              importance_type=None,\n",
       "                              interaction_constraints=None, learning_rate=0.05,\n",
       "                              max_bin=None, max_cat_threshold=None,\n",
       "                              max_cat_to_onehot=None, max_delta_step=None,\n",
       "                              max_depth=6, max_leaves=None,\n",
       "                              min_child_weight=None, missing=nan,\n",
       "                              monotone_constraints=None, multi_strategy=None,\n",
       "                              n_estimators=600, n_jobs=-1,\n",
       "                              num_parallel_tree=None, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;prep&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;cat&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;jobRole&#x27;, &#x27;education&#x27;,\n",
       "                                                   &#x27;major&#x27;, &#x27;industry&#x27;,\n",
       "                                                   &#x27;companyId&#x27;]),\n",
       "                                                 (&#x27;num&#x27;, &#x27;passthrough&#x27;,\n",
       "                                                  [&#x27;yearsExperience&#x27;,\n",
       "                                                   &#x27;distanceFromCBD&#x27;])])),\n",
       "                (&#x27;xgb&#x27;,\n",
       "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                              colsample_bylevel=None, colsample_bynode=None,\n",
       "                              colsample_bytree=0.8...\n",
       "                              feature_types=None, feature_weights=None,\n",
       "                              gamma=None, grow_policy=None,\n",
       "                              importance_type=None,\n",
       "                              interaction_constraints=None, learning_rate=0.05,\n",
       "                              max_bin=None, max_cat_threshold=None,\n",
       "                              max_cat_to_onehot=None, max_delta_step=None,\n",
       "                              max_depth=6, max_leaves=None,\n",
       "                              min_child_weight=None, missing=nan,\n",
       "                              monotone_constraints=None, multi_strategy=None,\n",
       "                              n_estimators=600, n_jobs=-1,\n",
       "                              num_parallel_tree=None, ...))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>prep: ColumnTransformer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for prep: ColumnTransformer</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(transformers=[(&#x27;cat&#x27;, OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;jobRole&#x27;, &#x27;education&#x27;, &#x27;major&#x27;, &#x27;industry&#x27;,\n",
       "                                  &#x27;companyId&#x27;]),\n",
       "                                (&#x27;num&#x27;, &#x27;passthrough&#x27;,\n",
       "                                 [&#x27;yearsExperience&#x27;, &#x27;distanceFromCBD&#x27;])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>cat</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;jobRole&#x27;, &#x27;education&#x27;, &#x27;major&#x27;, &#x27;industry&#x27;, &#x27;companyId&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>OneHotEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>num</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;yearsExperience&#x27;, &#x27;distanceFromCBD&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>passthrough</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.0.0/python/python_api.html#xgboost.XGBRegressor\">?<span>Documentation for XGBRegressor</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             feature_weights=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.05, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "             max_leaves=None, min_child_weight=None, missing=nan,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=600,\n",
       "             n_jobs=-1, num_parallel_tree=None, ...)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('prep',\n",
       "                 ColumnTransformer(transformers=[('cat',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['jobRole', 'education',\n",
       "                                                   'major', 'industry',\n",
       "                                                   'companyId']),\n",
       "                                                 ('num', 'passthrough',\n",
       "                                                  ['yearsExperience',\n",
       "                                                   'distanceFromCBD'])])),\n",
       "                ('xgb',\n",
       "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                              colsample_bylevel=None, colsample_bynode=None,\n",
       "                              colsample_bytree=0.8...\n",
       "                              feature_types=None, feature_weights=None,\n",
       "                              gamma=None, grow_policy=None,\n",
       "                              importance_type=None,\n",
       "                              interaction_constraints=None, learning_rate=0.05,\n",
       "                              max_bin=None, max_cat_threshold=None,\n",
       "                              max_cat_to_onehot=None, max_delta_step=None,\n",
       "                              max_depth=6, max_leaves=None,\n",
       "                              min_child_weight=None, missing=nan,\n",
       "                              monotone_constraints=None, multi_strategy=None,\n",
       "                              n_estimators=600, n_jobs=-1,\n",
       "                              num_parallel_tree=None, ...))])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"prep\", preprocess),\n",
    "        (\"xgb\",  xgb_reg),\n",
    "    ]\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "27fc68c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- XGBoost Results -----\n",
      "XGBoost    | VAL | RMSE:    19.28 | MAE:    15.59 | MSE:     371.81 | R²:  0.754\n",
      "XGBoost    | TEST | RMSE:    19.28 | MAE:    15.62 | MSE:     371.82 | R²:  0.753\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n----- XGBoost Results -----\")\n",
    "evaluate(\"XGBoost \", xgb_model, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2223d526",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "aabae1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;prep&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;cat&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;jobRole&#x27;, &#x27;education&#x27;,\n",
       "                                                   &#x27;major&#x27;, &#x27;industry&#x27;,\n",
       "                                                   &#x27;companyId&#x27;]),\n",
       "                                                 (&#x27;num&#x27;, &#x27;passthrough&#x27;,\n",
       "                                                  [&#x27;yearsExperience&#x27;,\n",
       "                                                   &#x27;distanceFromCBD&#x27;])])),\n",
       "                (&#x27;lr&#x27;, LinearRegression(n_jobs=-1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;prep&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;cat&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;jobRole&#x27;, &#x27;education&#x27;,\n",
       "                                                   &#x27;major&#x27;, &#x27;industry&#x27;,\n",
       "                                                   &#x27;companyId&#x27;]),\n",
       "                                                 (&#x27;num&#x27;, &#x27;passthrough&#x27;,\n",
       "                                                  [&#x27;yearsExperience&#x27;,\n",
       "                                                   &#x27;distanceFromCBD&#x27;])])),\n",
       "                (&#x27;lr&#x27;, LinearRegression(n_jobs=-1))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>prep: ColumnTransformer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for prep: ColumnTransformer</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(transformers=[(&#x27;cat&#x27;, OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;jobRole&#x27;, &#x27;education&#x27;, &#x27;major&#x27;, &#x27;industry&#x27;,\n",
       "                                  &#x27;companyId&#x27;]),\n",
       "                                (&#x27;num&#x27;, &#x27;passthrough&#x27;,\n",
       "                                 [&#x27;yearsExperience&#x27;, &#x27;distanceFromCBD&#x27;])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>cat</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;jobRole&#x27;, &#x27;education&#x27;, &#x27;major&#x27;, &#x27;industry&#x27;, &#x27;companyId&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>OneHotEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>num</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;yearsExperience&#x27;, &#x27;distanceFromCBD&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>passthrough</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LinearRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression(n_jobs=-1)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('prep',\n",
       "                 ColumnTransformer(transformers=[('cat',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['jobRole', 'education',\n",
       "                                                   'major', 'industry',\n",
       "                                                   'companyId']),\n",
       "                                                 ('num', 'passthrough',\n",
       "                                                  ['yearsExperience',\n",
       "                                                   'distanceFromCBD'])])),\n",
       "                ('lr', LinearRegression(n_jobs=-1))])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = Pipeline([\n",
    "    (\"prep\", preprocess),  # OneHotEncoder + numeric passthrough\n",
    "    (\"lr\", LinearRegression(n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Train on train set\n",
    "linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6826dbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Linear Regression Results -----\n",
      "LinearReg  | VAL | RMSE:    20.09 | MAE:    16.18 | MSE:     403.80 | R²:  0.733\n",
      "LinearReg  | TEST | RMSE:    19.97 | MAE:    16.13 | MSE:     398.82 | R²:  0.735\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on val and test sets\n",
    "print(\"----- Linear Regression Results -----\")\n",
    "evaluate(\"LinearReg\", linear_model, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18e5c1b",
   "metadata": {},
   "source": [
    "**RF,XGB & LR Model Metrics**\n",
    "\n",
    "1. Validation Set Results\n",
    "\n",
    "| Model           | RMSE   | MAE    | MSE     | R²    |\n",
    "|-----------------|:------:|:------:|:-------:|:-----:|\n",
    "| Random Forest   | 20.43  | 16.33  | 417.25  | 0.724 |\n",
    "| XGBoost         | 19.28  | 15.59  | 371.81  | 0.754 |\n",
    "| Linear Reg.     | 20.09  | 16.18  | 403.80  | 0.733 |\n",
    "\n",
    "2. Test Set Results\n",
    "\n",
    "| Model           | RMSE   | MAE    | MSE     | R²    |\n",
    "|-----------------|:------:|:------:|:-------:|:-----:|\n",
    "| Random Forest   | 20.43  | 16.38  | 417.37  | 0.722 |\n",
    "| XGBoost         | 19.28  | 15.62  | 371.82  | 0.753 |\n",
    "| Linear Reg.     | 19.97  | 16.13  | 398.82  | 0.735 |\n",
    "\n",
    "3. Explanation of Test Set Metrics\n",
    "\n",
    "- **RMSE (Root Mean Squared Error):**\n",
    "  - Shows the average prediction error size, **heavily penalizing large errors**.\n",
    "  - For example, a Random Forest RMSE of **20.43** means the model’s salary predictions are, on average, about **$20,430** off from the actual value.\n",
    "\n",
    "- **MAE (Mean Absolute Error):**\n",
    "  - The **average absolute difference** between the predicted and true salaries.\n",
    "  - A MAE of **16.38** for Random Forest means the typical error is about **$16,380**—this metric is less sensitive to outliers than RMSE.\n",
    "\n",
    "- **MSE (Mean Squared Error):**\n",
    "  - The **mean of squared prediction errors** (in thousands squared).\n",
    "  - Useful during model training and for comparing models, but not directly interpretable in salary units.\n",
    "\n",
    "- **R² (R-squared):**\n",
    "  - **Represents the proportion of variance** in the salary that the model is able to explain.\n",
    "  - An R² of **0.722** for Random Forest means the model explains **72.2% of the variability** in actual salaries. Higher values indicate better model performance.\n",
    "\n",
    "\n",
    "- The XGBoost model delivered the lowest prediction errors (RMSE: 19.28, MAE: 15.62) and the highest explanatory power (R²: 0.753), making it the best performer for salary prediction in this study.\n",
    "- Linear Regression was nearly as strong, demonstrating that a well-processed linear model can capture much of the salary variation in the data.\n",
    "- Random Forest performed robustly, with slightly higher error but strong overall accuracy.\n",
    "- For context, salary is difficult to predict due to factors not captured in the dataset (bonuses, negotiation, company policy, etc.), so RMSE and MAE values of \\$16–20k and R² scores above 0.7 are strong results for real-world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bf930b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['jobId', 'companyId', 'jobRole', 'education', 'major', 'industry',\n",
       "       'yearsExperience', 'distanceFromCBD', 'salaryInThousands'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ee0fb2",
   "metadata": {},
   "source": [
    "#### Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "23bec4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learners = [\n",
    "    (\"linreg\",  LinearRegression(n_jobs=-1)),\n",
    "    (\"rf\",      RandomForestRegressor(\n",
    "                    n_estimators=300,\n",
    "                    max_depth=None,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1)),\n",
    "    (\"xgb\",     xgb.XGBRegressor(\n",
    "                    n_estimators=400,\n",
    "                    learning_rate=0.05,\n",
    "                    max_depth=6,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective=\"reg:squarederror\",\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "61fb0c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Meta-learner  (another XGB works well, but ridge is fast) ----\n",
    "meta_learner = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42)\n",
    "\n",
    "stack_model = StackingRegressor(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    passthrough=False,            # True if you want raw features fed to meta\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "ensemble_pipeline = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"stack\", stack_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "612ef60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-4 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-4 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-4 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-4 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-4 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;prep&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;cat&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;jobRole&#x27;, &#x27;education&#x27;,\n",
       "                                                   &#x27;major&#x27;, &#x27;industry&#x27;,\n",
       "                                                   &#x27;companyId&#x27;]),\n",
       "                                                 (&#x27;num&#x27;, &#x27;passthrough&#x27;,\n",
       "                                                  [&#x27;yearsExperience&#x27;,\n",
       "                                                   &#x27;distanceFromCBD&#x27;])])),\n",
       "                (&#x27;stack&#x27;,\n",
       "                 StackingRegressor(estimators=[(&#x27;linreg&#x27;,\n",
       "                                                LinearRegression(n_jobs=-1)),\n",
       "                                               (&#x27;rf&#x27;,\n",
       "                                                RandomForestRegressor(n_estimators=300,\n",
       "                                                                      n_jobs...\n",
       "                                                                gamma=None,\n",
       "                                                                grow_policy=None,\n",
       "                                                                importance_type=None,\n",
       "                                                                interaction_constraints=None,\n",
       "                                                                learning_rate=0.05,\n",
       "                                                                max_bin=None,\n",
       "                                                                max_cat_threshold=None,\n",
       "                                                                max_cat_to_onehot=None,\n",
       "                                                                max_delta_step=None,\n",
       "                                                                max_depth=4,\n",
       "                                                                max_leaves=None,\n",
       "                                                                min_child_weight=None,\n",
       "                                                                missing=nan,\n",
       "                                                                monotone_constraints=None,\n",
       "                                                                multi_strategy=None,\n",
       "                                                                n_estimators=200,\n",
       "                                                                n_jobs=-1,\n",
       "                                                                num_parallel_tree=None, ...),\n",
       "                                   n_jobs=-1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;prep&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;cat&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;jobRole&#x27;, &#x27;education&#x27;,\n",
       "                                                   &#x27;major&#x27;, &#x27;industry&#x27;,\n",
       "                                                   &#x27;companyId&#x27;]),\n",
       "                                                 (&#x27;num&#x27;, &#x27;passthrough&#x27;,\n",
       "                                                  [&#x27;yearsExperience&#x27;,\n",
       "                                                   &#x27;distanceFromCBD&#x27;])])),\n",
       "                (&#x27;stack&#x27;,\n",
       "                 StackingRegressor(estimators=[(&#x27;linreg&#x27;,\n",
       "                                                LinearRegression(n_jobs=-1)),\n",
       "                                               (&#x27;rf&#x27;,\n",
       "                                                RandomForestRegressor(n_estimators=300,\n",
       "                                                                      n_jobs...\n",
       "                                                                gamma=None,\n",
       "                                                                grow_policy=None,\n",
       "                                                                importance_type=None,\n",
       "                                                                interaction_constraints=None,\n",
       "                                                                learning_rate=0.05,\n",
       "                                                                max_bin=None,\n",
       "                                                                max_cat_threshold=None,\n",
       "                                                                max_cat_to_onehot=None,\n",
       "                                                                max_delta_step=None,\n",
       "                                                                max_depth=4,\n",
       "                                                                max_leaves=None,\n",
       "                                                                min_child_weight=None,\n",
       "                                                                missing=nan,\n",
       "                                                                monotone_constraints=None,\n",
       "                                                                multi_strategy=None,\n",
       "                                                                n_estimators=200,\n",
       "                                                                n_jobs=-1,\n",
       "                                                                num_parallel_tree=None, ...),\n",
       "                                   n_jobs=-1))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>prep: ColumnTransformer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for prep: ColumnTransformer</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(transformers=[(&#x27;cat&#x27;, OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;jobRole&#x27;, &#x27;education&#x27;, &#x27;major&#x27;, &#x27;industry&#x27;,\n",
       "                                  &#x27;companyId&#x27;]),\n",
       "                                (&#x27;num&#x27;, &#x27;passthrough&#x27;,\n",
       "                                 [&#x27;yearsExperience&#x27;, &#x27;distanceFromCBD&#x27;])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>cat</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;jobRole&#x27;, &#x27;education&#x27;, &#x27;major&#x27;, &#x27;industry&#x27;, &#x27;companyId&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>OneHotEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>num</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;yearsExperience&#x27;, &#x27;distanceFromCBD&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>passthrough</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>stack: StackingRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.StackingRegressor.html\">?<span>Documentation for stack: StackingRegressor</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>StackingRegressor(estimators=[(&#x27;linreg&#x27;, LinearRegression(n_jobs=-1)),\n",
       "                              (&#x27;rf&#x27;,\n",
       "                               RandomForestRegressor(n_estimators=300,\n",
       "                                                     n_jobs=-1,\n",
       "                                                     random_state=42)),\n",
       "                              (&#x27;xgb&#x27;,\n",
       "                               XGBRegressor(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=0.8, device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric=No...\n",
       "                                               feature_weights=None, gamma=None,\n",
       "                                               grow_policy=None,\n",
       "                                               importance_type=None,\n",
       "                                               interaction_constraints=None,\n",
       "                                               learning_rate=0.05, max_bin=None,\n",
       "                                               max_cat_threshold=None,\n",
       "                                               max_cat_to_onehot=None,\n",
       "                                               max_delta_step=None, max_depth=4,\n",
       "                                               max_leaves=None,\n",
       "                                               min_child_weight=None,\n",
       "                                               missing=nan,\n",
       "                                               monotone_constraints=None,\n",
       "                                               multi_strategy=None,\n",
       "                                               n_estimators=200, n_jobs=-1,\n",
       "                                               num_parallel_tree=None, ...),\n",
       "                  n_jobs=-1)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>linreg</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LinearRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression(n_jobs=-1)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">?<span>Documentation for RandomForestRegressor</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestRegressor(n_estimators=300, n_jobs=-1, random_state=42)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>xgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\" ><label for=\"sk-estimator-id-31\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.0.0/python/python_api.html#xgboost.XGBRegressor\">?<span>Documentation for XGBRegressor</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             feature_weights=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.05, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "             max_leaves=None, min_child_weight=None, missing=nan,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "             n_jobs=-1, num_parallel_tree=None, ...)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" ><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.0.0/python/python_api.html#xgboost.XGBRegressor\">?<span>Documentation for XGBRegressor</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             feature_weights=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.05, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=4,\n",
       "             max_leaves=None, min_child_weight=None, missing=nan,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=200,\n",
       "             n_jobs=-1, num_parallel_tree=None, ...)</pre></div> </div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('prep',\n",
       "                 ColumnTransformer(transformers=[('cat',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['jobRole', 'education',\n",
       "                                                   'major', 'industry',\n",
       "                                                   'companyId']),\n",
       "                                                 ('num', 'passthrough',\n",
       "                                                  ['yearsExperience',\n",
       "                                                   'distanceFromCBD'])])),\n",
       "                ('stack',\n",
       "                 StackingRegressor(estimators=[('linreg',\n",
       "                                                LinearRegression(n_jobs=-1)),\n",
       "                                               ('rf',\n",
       "                                                RandomForestRegressor(n_estimators=300,\n",
       "                                                                      n_jobs...\n",
       "                                                                gamma=None,\n",
       "                                                                grow_policy=None,\n",
       "                                                                importance_type=None,\n",
       "                                                                interaction_constraints=None,\n",
       "                                                                learning_rate=0.05,\n",
       "                                                                max_bin=None,\n",
       "                                                                max_cat_threshold=None,\n",
       "                                                                max_cat_to_onehot=None,\n",
       "                                                                max_delta_step=None,\n",
       "                                                                max_depth=4,\n",
       "                                                                max_leaves=None,\n",
       "                                                                min_child_weight=None,\n",
       "                                                                missing=nan,\n",
       "                                                                monotone_constraints=None,\n",
       "                                                                multi_strategy=None,\n",
       "                                                                n_estimators=200,\n",
       "                                                                n_jobs=-1,\n",
       "                                                                num_parallel_tree=None, ...),\n",
       "                                   n_jobs=-1))])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Train ----\n",
    "ensemble_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0bd7fdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stacked Ensemble Results -----\n",
      "StackEnsemble| VAL | RMSE:    19.32 | MAE:    15.62 | MSE:     373.10 | R²:  0.753\n",
      "StackEnsemble| TEST | RMSE:    19.26 | MAE:    15.62 | MSE:     370.91 | R²:  0.753\n"
     ]
    }
   ],
   "source": [
    "def evaluate(name, model, Xv, yv, Xt, yt):\n",
    "    for split, X_, y_ in [(\"VAL\", Xv, yv), (\"TEST\", Xt, yt)]:\n",
    "        p = model.predict(X_)\n",
    "        mse = np.mean((y_ - p) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae  = mean_absolute_error(y_, p)\n",
    "        r2   = r2_score(y_, p)\n",
    "        print(f\"{name:<12}| {split} | RMSE: {rmse:8.2f} | MAE: {mae:8.2f} | \"\n",
    "              f\"MSE: {mse:10.2f} | R²: {r2:6.3f}\")\n",
    "\n",
    "print(\"----- Stacked Ensemble Results -----\")\n",
    "evaluate(\"StackEnsemble\", ensemble_pipeline,\n",
    "         X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41a10c2",
   "metadata": {},
   "source": [
    "#### Ensemble K Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cf843ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best XGB params: {'xgb__subsample': 0.7, 'xgb__n_estimators': 800, 'xgb__max_depth': 5, 'xgb__learning_rate': 0.02, 'xgb__colsample_bytree': 0.8}\n",
      "\n",
      "----- Tuned XGBoost -----\n",
      "XGB-Tuned   | VAL | RMSE    19.22 | MAE    15.55 | MSE     369.30 | R²  0.756\n",
      "XGB-Tuned   | TEST | RMSE    19.18 | MAE    15.57 | MSE     368.06 | R²  0.755\n",
      "\n",
      "----- CV Stacked Ensemble -----\n",
      "Stack-CV    | VAL | RMSE    19.28 | MAE    15.59 | MSE     371.90 | R²  0.754\n",
      "Stack-CV    | TEST | RMSE    19.22 | MAE    15.60 | MSE     369.41 | R²  0.754\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 0. Imports & settings\n",
    "# -----------------------------------------------------------\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "import xgboost as xgb\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1.  Sample (optional) & feature / label setup\n",
    "# -----------------------------------------------------------\n",
    "N_SAMPLE = 150_000               # set to None to use full dataset\n",
    "df = master_df.sample(n=N_SAMPLE, random_state=42) if N_SAMPLE else master_df\n",
    "\n",
    "TARGET   = \"salaryInThousands\"\n",
    "cat_cols = [\"jobRole\", \"education\", \"major\", \"industry\", \"companyId\"]\n",
    "num_cols = [\"yearsExperience\", \"distanceFromCBD\"]\n",
    "\n",
    "X = df[cat_cols + num_cols]\n",
    "y = df[TARGET]\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2.  Train / Val / Test split\n",
    "# -----------------------------------------------------------\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test     = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3.  Preprocessing (reuse in all models)\n",
    "# -----------------------------------------------------------\n",
    "preprocess = ColumnTransformer(\n",
    "    [(\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), cat_cols),\n",
    "     (\"num\", \"passthrough\", num_cols)]\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4.  Hyper-parameter search for XGBoost\n",
    "# -----------------------------------------------------------\n",
    "xgb_pipe = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"xgb\", xgb.XGBRegressor(\n",
    "              objective=\"reg:squarederror\",\n",
    "              n_jobs=-1,\n",
    "              random_state=42))\n",
    "])\n",
    "\n",
    "param_dist = {\n",
    "    \"xgb__n_estimators\":   [400, 600, 800],\n",
    "    \"xgb__max_depth\":      [5, 6, 7, 8],\n",
    "    \"xgb__learning_rate\":  [0.05, 0.03, 0.02],\n",
    "    \"xgb__subsample\":      [0.7, 0.8, 0.9],\n",
    "    \"xgb__colsample_bytree\":[0.7, 0.8, 0.9],\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    xgb_pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "best_xgb = search.best_estimator_\n",
    "print(\"Best XGB params:\", search.best_params_)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5.  CV-Stacked Ensemble\n",
    "# -----------------------------------------------------------\n",
    "base_learners = [\n",
    "    (\"linreg\", LinearRegression(n_jobs=-1)),\n",
    "    (\"rf\",     RandomForestRegressor(\n",
    "                   n_estimators=300, max_depth=None,\n",
    "                   n_jobs=-1, random_state=42)),\n",
    "    (\"xgb\",    best_xgb.named_steps[\"xgb\"])    # tuned XGB without extra prep\n",
    "]\n",
    "\n",
    "meta_learner = xgb.XGBRegressor(\n",
    "    n_estimators=200, learning_rate=0.05, max_depth=4,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    objective=\"reg:squarederror\", n_jobs=-1, random_state=42\n",
    ")\n",
    "\n",
    "stack_cv = StackingRegressor(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    passthrough=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "ensemble_pipe = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"stack\", stack_cv)\n",
    "])\n",
    "\n",
    "ensemble_pipe.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6.  Evaluation helper\n",
    "# -----------------------------------------------------------\n",
    "def report(name, model, Xv, yv, Xt, yt):\n",
    "    for tag, X_, y_ in [(\"VAL\", Xv, yv), (\"TEST\", Xt, yt)]:\n",
    "        preds = model.predict(X_)\n",
    "        mse = np.mean((y_ - preds) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae  = mean_absolute_error(y_, preds)\n",
    "        r2   = r2_score(y_, preds)\n",
    "        print(f\"{name:<12}| {tag} | RMSE {rmse:8.2f} | MAE {mae:8.2f} | \"\n",
    "              f\"MSE {mse:10.2f} | R² {r2:6.3f}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 7.  Results\n",
    "# -----------------------------------------------------------\n",
    "print(\"\\n----- Tuned XGBoost -----\")\n",
    "report(\"XGB-Tuned\", best_xgb, X_val, y_val, X_test, y_test)\n",
    "\n",
    "print(\"\\n----- CV Stacked Ensemble -----\")\n",
    "report(\"Stack-CV\",  ensemble_pipe, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7d6d8",
   "metadata": {},
   "source": [
    "## PySpark Data Cleaning & Modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de3e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/local/Cellar/openjdk@11/11.0.28/libexec/openjdk.jdk/Contents/Home\"  # <-- Use your path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c97a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import (\n",
    "    LinearRegression, RandomForestRegressor, GBTRegressor\n",
    ")\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, lower, when, lit, expr, rand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f491d3",
   "metadata": {},
   "source": [
    "### PySpark Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa226732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/05 22:10:24 WARN Utils: Your hostname, Sais-Macbook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.18 instead (on interface en0)\n",
      "25/08/05 22:10:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/05 22:10:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EmployeeCleaning\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ae1b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the datasets \n",
    "employee_df = spark.read.csv(\"Employee_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "salary_df = spark.read.csv(\"employee_salaries.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dfd349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicares in both the dataframes\n",
    "employee_df = employee_df.dropDuplicates()\n",
    "salary_df = salary_df.dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9300a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicates in both the dataframes\n",
    "employee_df = employee_df.na.drop()\n",
    "salary_df = salary_df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "048d10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get distinct jobIds from both DataFrames\n",
    "salary_ids_df   = salary_df.select(\"jobId\").distinct()\n",
    "employee_ids_df = employee_df.select(\"jobId\").distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d61b0218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobIds in Salary Dataset not present in Employee Dataset: 297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ids_not_in_employee = (\n",
    "    salary_ids_df\n",
    "    .join(employee_ids_df, on=\"jobId\", how=\"left_anti\")\n",
    ")\n",
    "\n",
    "num_not_in_employee = ids_not_in_employee.count()\n",
    "\n",
    "print(f\"Number of jobIds in Salary Dataset not present in Employee Dataset: {num_not_in_employee}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57514d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+\n",
      "|           jobId|salaryInThousands|\n",
      "+----------------+-----------------+\n",
      "|JOB1362684407688|              101|\n",
      "|JOB1362684407724|              183|\n",
      "|JOB1362684407739|               88|\n",
      "|JOB1362684407746|              106|\n",
      "|JOB1362684407752|               80|\n",
      "+----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "salary_df_cleaned = salary_df.join(employee_ids_df, on=\"jobId\", how=\"inner\")\n",
    "\n",
    "# Show the result\n",
    "salary_df_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dea8668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = employee_df.join(salary_df_cleaned, on=\"jobId\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6067c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"jobRole\", \"education\", \"major\", \"industry\", \"companyId\"]\n",
    "for c in cat_cols:\n",
    "    df = df.withColumn(c, trim(lower(col(c))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d16e0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.filter(col(\"jobRole\") != \"president\")\n",
    "      .filter((col(\"salaryInThousands\") > 0) & (col(\"salaryInThousands\") <= 1000))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bffc01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "        \"distanceFromCBD\",\n",
    "        when(col(\"distanceFromCBD\") > 60, 60).otherwise(col(\"distanceFromCBD\"))\n",
    "     ).filter(col(\"distanceFromCBD\") >= 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6138d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema after cleaning:\n",
      "root\n",
      " |-- jobId: string (nullable = true)\n",
      " |-- companyId: string (nullable = true)\n",
      " |-- jobRole: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- major: string (nullable = true)\n",
      " |-- industry: string (nullable = true)\n",
      " |-- yearsExperience: integer (nullable = true)\n",
      " |-- distanceFromCBD: integer (nullable = true)\n",
      " |-- salaryInThousands: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count after cleaning: 999,467\n",
      "Sample cleaned rows:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+--------------+-----------+----------+---------+---------------+---------------+-----------------+\n",
      "|jobId           |companyId|jobRole       |education  |major     |industry |yearsExperience|distanceFromCBD|salaryInThousands|\n",
      "+----------------+---------+--------------+-----------+----------+---------+---------------+---------------+-----------------+\n",
      "|JOB1362684407688|comp19   |ceo           |high_school|none      |web      |3              |60             |101              |\n",
      "|JOB1362684407724|comp8    |vice_president|doctoral   |business  |health   |24             |35             |183              |\n",
      "|JOB1362684407739|comp5    |junior        |masters    |biology   |finance  |11             |60             |88               |\n",
      "|JOB1362684407746|comp11   |vice_president|masters    |literature|service  |10             |5              |106              |\n",
      "|JOB1362684407752|comp33   |vice_president|none       |none      |education|1              |36             |80               |\n",
      "+----------------+---------+--------------+-----------+----------+---------+---------------+---------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Schema after cleaning:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(f\"Row count after cleaning: {df.count():,}\")\n",
    "\n",
    "print(\"Sample cleaned rows:\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50b4d13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/05 22:11:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"pyspark_cleaned_master_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b54eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stop the Spark session\n",
    "# spark.stop()\n",
    "# print(\"Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a87dc6",
   "metadata": {},
   "source": [
    "### PySpark Model Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/local/Cellar/openjdk@11/11.0.28/libexec/openjdk.jdk/Contents/Home\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfabe0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/05 20:52:32 WARN Utils: Your hostname, Sais-Macbook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.18 instead (on interface en0)\n",
      "25/08/05 20:52:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/05 20:52:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.appName(\"PySparkML-Training\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cabc3e",
   "metadata": {},
   "source": [
    "#### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6cd5cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled row count: 10,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 10_000\n",
    "df_sample = (\n",
    "    df.orderBy(rand(seed=42))    # reproducible shuffle\n",
    "      .limit(SAMPLE_SIZE)\n",
    ")\n",
    "print(f\"Sampled row count: {df_sample.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2e6b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = df.randomSplit([0.70, 0.15, 0.15], seed=42)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3.  Feature Engineering Pipeline\n",
    "# --------------------------------------------------------------\n",
    "cat_cols = [\"jobRole\", \"education\", \"major\", \"industry\", \"companyId\"]\n",
    "num_cols = [\"yearsExperience\", \"distanceFromCBD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bbd4658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a.  StringIndex → OneHot  for each categorical column\n",
    "indexers  = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "             for c in cat_cols]\n",
    "encoders  = [OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}_ohe\")\n",
    "             for c in cat_cols]\n",
    "\n",
    "# 3b.  Assemble everything into a single feature vector\n",
    "assembler_inputs = [f\"{c}_ohe\" for c in cat_cols] + num_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9cb578",
   "metadata": {},
   "source": [
    "#### Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "924dd177",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr  = LinearRegression(featuresCol=\"features\", labelCol=\"salaryInThousands\")\n",
    "\n",
    "rf  = RandomForestRegressor(featuresCol=\"features\", labelCol=\"salaryInThousands\",\n",
    "                            numTrees=300, maxDepth=10, seed=42)\n",
    "\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"salaryInThousands\",\n",
    "                   maxIter=400, maxDepth=6, stepSize=0.05, seed=42)\n",
    "\n",
    "models = {\"LinearRegression\": lr,\n",
    "          \"RandomForest\":     rf,\n",
    "          \"GBT\":              gbt}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d26242",
   "metadata": {},
   "source": [
    "#### CV Grid Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3aadc857",
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = {\n",
    "    \"LinearRegression\": (ParamGridBuilder()\n",
    "        .addGrid(lr.regParam,        [0.0, 0.1])        # L2 penalty\n",
    "        .addGrid(lr.elasticNetParam, [0.0, 0.5])        # Ridge / Elastic\n",
    "        .build()),\n",
    "\n",
    "    \"RandomForest\": (ParamGridBuilder()\n",
    "        .addGrid(rf.maxDepth, [8, 12])\n",
    "        .addGrid(rf.numTrees, [200, 400])\n",
    "        .build()),\n",
    "\n",
    "    \"GBT\": (ParamGridBuilder()\n",
    "        .addGrid(gbt.maxDepth, [4, 6])\n",
    "        .addGrid(gbt.maxIter,  [200, 400])\n",
    "        .build())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795b1fa7",
   "metadata": {},
   "source": [
    "#### Training & Evaluation Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29e9da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"salaryInThousands\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "def full_metrics(df_preds, label=\"salaryInThousands\"):\n",
    "    \"\"\"Return RMSE, MAE, MSE, R2 for a predictions DataFrame.\"\"\"\n",
    "    rmse = evaluator.evaluate(df_preds, {evaluator.metricName: \"rmse\"})\n",
    "    mae  = evaluator.evaluate(df_preds, {evaluator.metricName: \"mae\"})\n",
    "    mse  = evaluator.evaluate(df_preds, {evaluator.metricName: \"mse\"})\n",
    "    r2   = evaluator.evaluate(df_preds, {evaluator.metricName: \"r2\"})\n",
    "    return rmse, mae, mse, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2821fa7",
   "metadata": {},
   "source": [
    "#### PySpark RandomForest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c0590b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "\n",
    "# for name, est in models.items():\n",
    "#     print(f\"\\n=== {name}: Cross-validating on TRAIN ===\")\n",
    "#     pipeline = Pipeline(stages=indexers + encoders + [assembler, est])\n",
    "\n",
    "#     cv = CrossValidator(\n",
    "#         estimator           = pipeline,\n",
    "#         estimatorParamMaps  = grids[name],\n",
    "#         evaluator           = evaluator,\n",
    "#         numFolds            = 3,\n",
    "#         seed                = 42,\n",
    "#         parallelism         = 4     # adjust to your CPU cores\n",
    "#     )\n",
    "\n",
    "#     cv_model   = cv.fit(train_df)\n",
    "#     best_model = cv_model.bestModel\n",
    "\n",
    "#     # --- Evaluate on VAL ---\n",
    "#     val_preds = best_model.transform(val_df)\n",
    "#     rmse_v, mae_v, mse_v, r2_v = full_metrics(val_preds)\n",
    "\n",
    "#     # --- Evaluate on TEST ---\n",
    "#     test_preds = best_model.transform(test_df)\n",
    "#     rmse_t, mae_t, mse_t, r2_t = full_metrics(test_preds)\n",
    "\n",
    "#     results.append((name, \"VAL\",  rmse_v,  mae_v,  mse_v,  r2_v))\n",
    "#     results.append((name, \"TEST\", rmse_t,  mae_t,  mse_t,  r2_t))\n",
    "\n",
    "#     print(f\"Best params: {best_model.stages[-1]._java_obj.extractParamMap()}\")\n",
    "#     print(f\"VAL  – RMSE {rmse_v:.2f} | MAE {mae_v:.2f} | R² {r2_v:.3f}\")\n",
    "#     print(f\"TEST – RMSE {rmse_t:.2f} | MAE {mae_t:.2f} | R² {r2_t:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d349e62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/05 22:14:59 WARN DAGScheduler: Broadcasting large task binary with size 1642.2 KiB\n",
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/Users/saikeerthan/spark/jars/spark-core_2.13-3.5.5.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "25/08/05 22:16:26 WARN DAGScheduler: Broadcasting large task binary with size 1645.1 KiB\n",
      "25/08/05 22:17:42 WARN MemoryStore: Not enough space to cache rdd_681_4 in memory! (computed 30.2 MiB so far)\n",
      "25/08/05 22:17:42 WARN MemoryStore: Not enough space to cache rdd_681_3 in memory! (computed 30.2 MiB so far)\n",
      "25/08/05 22:17:42 WARN MemoryStore: Not enough space to cache rdd_681_6 in memory! (computed 30.2 MiB so far)\n",
      "25/08/05 22:17:42 WARN MemoryStore: Not enough space to cache rdd_681_1 in memory! (computed 30.2 MiB so far)\n",
      "25/08/05 22:17:42 WARN MemoryStore: Not enough space to cache rdd_681_0 in memory! (computed 30.2 MiB so far)\n",
      "25/08/05 22:17:42 WARN BlockManager: Persisting block rdd_681_3 to disk instead.\n",
      "25/08/05 22:17:42 WARN BlockManager: Persisting block rdd_681_0 to disk instead.\n",
      "25/08/05 22:17:42 WARN BlockManager: Persisting block rdd_681_6 to disk instead.\n",
      "25/08/05 22:17:42 WARN BlockManager: Persisting block rdd_681_1 to disk instead.\n",
      "25/08/05 22:17:42 WARN BlockManager: Persisting block rdd_681_4 to disk instead.\n",
      "25/08/05 22:17:42 WARN MemoryStore: Not enough space to cache rdd_681_2 in memory! (computed 30.2 MiB so far)\n",
      "25/08/05 22:17:42 WARN BlockManager: Persisting block rdd_681_2 to disk instead.\n",
      "25/08/05 22:17:42 WARN MemoryStore: Not enough space to cache rdd_681_5 in memory! (computed 30.2 MiB so far)\n",
      "25/08/05 22:17:42 WARN BlockManager: Persisting block rdd_681_5 to disk instead.\n",
      "25/08/05 22:17:44 WARN MemoryStore: Not enough space to cache rdd_681_3 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:44 WARN MemoryStore: Not enough space to cache rdd_681_2 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:44 WARN MemoryStore: Not enough space to cache rdd_681_0 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:44 WARN MemoryStore: Not enough space to cache rdd_681_1 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:44 WARN MemoryStore: Not enough space to cache rdd_681_4 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:44 WARN MemoryStore: Not enough space to cache rdd_681_6 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:44 WARN MemoryStore: Not enough space to cache rdd_681_5 in memory! (computed 69.0 MiB so far)\n",
      "25/08/05 22:17:48 WARN MemoryStore: Not enough space to cache rdd_681_1 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:48 WARN MemoryStore: Not enough space to cache rdd_681_6 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:48 WARN MemoryStore: Not enough space to cache rdd_681_2 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:48 WARN MemoryStore: Not enough space to cache rdd_681_5 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:48 WARN MemoryStore: Not enough space to cache rdd_681_4 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:48 WARN MemoryStore: Not enough space to cache rdd_681_0 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:48 WARN MemoryStore: Not enough space to cache rdd_681_3 in memory! (computed 69.0 MiB so far)\n",
      "25/08/05 22:17:52 WARN MemoryStore: Not enough space to cache rdd_681_3 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:52 WARN MemoryStore: Not enough space to cache rdd_681_1 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:52 WARN MemoryStore: Not enough space to cache rdd_681_2 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:52 WARN MemoryStore: Not enough space to cache rdd_681_5 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:52 WARN MemoryStore: Not enough space to cache rdd_681_0 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:52 WARN MemoryStore: Not enough space to cache rdd_681_4 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:52 WARN MemoryStore: Not enough space to cache rdd_681_6 in memory! (computed 69.0 MiB so far)\n",
      "25/08/05 22:17:57 WARN MemoryStore: Not enough space to cache rdd_681_5 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:57 WARN MemoryStore: Not enough space to cache rdd_681_2 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:57 WARN MemoryStore: Not enough space to cache rdd_681_3 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:57 WARN MemoryStore: Not enough space to cache rdd_681_1 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:57 WARN MemoryStore: Not enough space to cache rdd_681_4 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:57 WARN MemoryStore: Not enough space to cache rdd_681_6 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:17:58 WARN MemoryStore: Not enough space to cache rdd_681_0 in memory! (computed 69.0 MiB so far)\n",
      "25/08/05 22:18:04 WARN MemoryStore: Not enough space to cache rdd_681_2 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:18:04 WARN MemoryStore: Not enough space to cache rdd_681_4 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:18:04 WARN MemoryStore: Not enough space to cache rdd_681_3 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:18:04 WARN MemoryStore: Not enough space to cache rdd_681_1 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:18:04 WARN MemoryStore: Not enough space to cache rdd_681_0 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:18:04 WARN MemoryStore: Not enough space to cache rdd_681_6 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:18:04 WARN MemoryStore: Not enough space to cache rdd_681_5 in memory! (computed 69.0 MiB so far)\n",
      "25/08/05 22:18:12 WARN DAGScheduler: Broadcasting large task binary with size 1614.7 KiB\n",
      "25/08/05 22:18:13 WARN MemoryStore: Not enough space to cache rdd_681_2 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:18:13 WARN MemoryStore: Not enough space to cache rdd_681_4 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:18:13 WARN MemoryStore: Not enough space to cache rdd_681_1 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:18:13 WARN MemoryStore: Not enough space to cache rdd_681_5 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:18:13 WARN MemoryStore: Not enough space to cache rdd_681_6 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:18:13 WARN MemoryStore: Not enough space to cache rdd_681_0 in memory! (computed 46.0 MiB so far)\n",
      "25/08/05 22:18:13 WARN MemoryStore: Not enough space to cache rdd_681_3 in memory! (computed 69.0 MiB so far)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL: RMSE 24.52 | MAE 19.58 | R2 0.598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 677:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: RMSE 24.62 | MAE 19.63 | R2 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Example: Only run RandomForest (edit for LinearReg or GBT)\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# ---- Pick ONE model ----\n",
    "estimator = rf  # or: lr, gbt\n",
    "\n",
    "# ---- Minimal param grid (ONE or TWO options max) ----\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(estimator.maxDepth, [6]) \\\n",
    "    .addGrid(estimator.numTrees, [100]) \\\n",
    "    .build()\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, estimator])\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=2,      # Only 2-fold for speed/memory!\n",
    "    parallelism=2,   # If 4 crashes, set to 2 or 1\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "cv_model = cv.fit(train_df)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# --- Evaluate on VAL ---\n",
    "val_preds = best_model.transform(val_df)\n",
    "rmse_v, mae_v, mse_v, r2_v = full_metrics(val_preds)\n",
    "print(f\"VAL: RMSE {rmse_v:.2f} | MAE {mae_v:.2f} | R2 {r2_v:.3f}\")\n",
    "\n",
    "# --- Evaluate on TEST ---\n",
    "test_preds = best_model.transform(test_df)\n",
    "rmse_t, mae_t, mse_t, r2_t = full_metrics(test_preds)\n",
    "print(f\"TEST: RMSE {rmse_t:.2f} | MAE {mae_t:.2f} | R2 {r2_t:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4fe0aa",
   "metadata": {},
   "source": [
    "#### PySpark Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae2f75ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/05 22:19:42 WARN Instrumentation: [91633da8] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/08/05 22:19:42 WARN Instrumentation: [406135b3] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/08/05 22:19:43 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/08/05 22:19:46 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "25/08/05 22:19:46 WARN Instrumentation: [91633da8] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/08/05 22:19:47 WARN Instrumentation: [406135b3] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/08/05 22:20:45 WARN Instrumentation: [1925a962] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/08/05 22:20:45 WARN Instrumentation: [b0ad1a2a] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/08/05 22:20:47 WARN Instrumentation: [1925a962] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/08/05 22:20:49 WARN Instrumentation: [b0ad1a2a] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/08/05 22:22:11 WARN Instrumentation: [2b995d34] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/08/05 22:22:15 WARN Instrumentation: [2b995d34] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL: RMSE 19.94 | MAE 16.10 | R2 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1398:>                                                       (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: RMSE 20.03 | MAE 16.16 | R2 0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Example: Only run RandomForest (edit for LinearReg or GBT)\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# ---- Pick ONE model ----\n",
    "# -- For Linear Regression --\n",
    "estimator = lr  # LinearRegression instance\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(estimator.regParam, [0.0, 0.1]) \\\n",
    "    .addGrid(estimator.elasticNetParam, [0.0, 0.5]) \\\n",
    "    .build()\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, estimator])\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=2,      # Only 2-fold for speed/memory!\n",
    "    parallelism=2,   # If 4 crashes, set to 2 or 1\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "cv_model = cv.fit(train_df)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# --- Evaluate on VAL ---\n",
    "val_preds = best_model.transform(val_df)\n",
    "rmse_v, mae_v, mse_v, r2_v = full_metrics(val_preds)\n",
    "print(f\"VAL: RMSE {rmse_v:.2f} | MAE {mae_v:.2f} | R2 {r2_v:.3f}\")\n",
    "\n",
    "# --- Evaluate on TEST ---\n",
    "test_preds = best_model.transform(test_df)\n",
    "rmse_t, mae_t, mse_t, r2_t = full_metrics(test_preds)\n",
    "print(f\"TEST: RMSE {rmse_t:.2f} | MAE {mae_t:.2f} | R2 {r2_t:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed18cd0",
   "metadata": {},
   "source": [
    "#### GBT Regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1044011b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/05 23:11:35 WARN DAGScheduler: Broadcasting large task binary with size 1001.3 KiB\n",
      "25/08/05 23:11:37 WARN DAGScheduler: Broadcasting large task binary with size 1006.4 KiB\n",
      "25/08/05 23:11:39 WARN DAGScheduler: Broadcasting large task binary with size 1010.3 KiB\n",
      "25/08/05 23:11:42 WARN DAGScheduler: Broadcasting large task binary with size 1010.8 KiB\n",
      "25/08/05 23:11:44 WARN DAGScheduler: Broadcasting large task binary with size 1011.4 KiB\n",
      "25/08/05 23:11:45 WARN DAGScheduler: Broadcasting large task binary with size 1012.7 KiB\n",
      "25/08/05 23:11:47 WARN DAGScheduler: Broadcasting large task binary with size 1015.2 KiB\n",
      "25/08/05 23:11:49 WARN DAGScheduler: Broadcasting large task binary with size 1020.1 KiB\n",
      "25/08/05 23:11:51 WARN DAGScheduler: Broadcasting large task binary with size 1024.2 KiB\n",
      "25/08/05 23:11:54 WARN DAGScheduler: Broadcasting large task binary with size 1024.7 KiB\n",
      "25/08/05 23:11:56 WARN DAGScheduler: Broadcasting large task binary with size 1025.3 KiB\n",
      "25/08/05 23:11:58 WARN DAGScheduler: Broadcasting large task binary with size 1026.5 KiB\n",
      "25/08/05 23:11:59 WARN DAGScheduler: Broadcasting large task binary with size 1029.0 KiB\n",
      "25/08/05 23:12:02 WARN DAGScheduler: Broadcasting large task binary with size 1034.0 KiB\n",
      "25/08/05 23:12:04 WARN DAGScheduler: Broadcasting large task binary with size 1038.0 KiB\n",
      "25/08/05 23:12:07 WARN DAGScheduler: Broadcasting large task binary with size 1038.5 KiB\n",
      "25/08/05 23:12:09 WARN DAGScheduler: Broadcasting large task binary with size 1039.2 KiB\n",
      "25/08/05 23:12:10 WARN DAGScheduler: Broadcasting large task binary with size 1040.5 KiB\n",
      "25/08/05 23:12:12 WARN DAGScheduler: Broadcasting large task binary with size 1043.0 KiB\n",
      "25/08/05 23:12:14 WARN DAGScheduler: Broadcasting large task binary with size 1047.8 KiB\n",
      "25/08/05 23:12:17 WARN DAGScheduler: Broadcasting large task binary with size 1051.8 KiB\n",
      "25/08/05 23:12:21 WARN DAGScheduler: Broadcasting large task binary with size 1052.3 KiB\n",
      "25/08/05 23:12:22 WARN DAGScheduler: Broadcasting large task binary with size 1052.8 KiB\n",
      "25/08/05 23:12:24 WARN DAGScheduler: Broadcasting large task binary with size 1054.1 KiB\n",
      "25/08/05 23:12:26 WARN DAGScheduler: Broadcasting large task binary with size 1056.6 KiB\n",
      "25/08/05 23:12:29 WARN DAGScheduler: Broadcasting large task binary with size 1061.4 KiB\n",
      "25/08/05 23:12:31 WARN DAGScheduler: Broadcasting large task binary with size 1065.5 KiB\n",
      "25/08/05 23:12:35 WARN DAGScheduler: Broadcasting large task binary with size 1066.0 KiB\n",
      "25/08/05 23:12:39 WARN DAGScheduler: Broadcasting large task binary with size 1066.7 KiB\n",
      "25/08/05 23:12:42 WARN DAGScheduler: Broadcasting large task binary with size 1068.0 KiB\n",
      "25/08/05 23:12:46 WARN DAGScheduler: Broadcasting large task binary with size 1070.5 KiB\n",
      "25/08/05 23:12:50 WARN DAGScheduler: Broadcasting large task binary with size 1075.3 KiB\n",
      "25/08/05 23:12:54 WARN DAGScheduler: Broadcasting large task binary with size 1079.2 KiB\n",
      "25/08/05 23:13:00 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "25/08/05 23:13:04 WARN DAGScheduler: Broadcasting large task binary with size 1080.2 KiB\n",
      "25/08/05 23:13:08 WARN DAGScheduler: Broadcasting large task binary with size 1081.6 KiB\n",
      "25/08/05 23:13:12 WARN DAGScheduler: Broadcasting large task binary with size 1084.1 KiB\n",
      "25/08/05 23:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1089.2 KiB\n",
      "25/08/05 23:13:20 WARN DAGScheduler: Broadcasting large task binary with size 1093.1 KiB\n",
      "25/08/05 23:13:26 WARN DAGScheduler: Broadcasting large task binary with size 1093.6 KiB\n",
      "25/08/05 23:13:30 WARN DAGScheduler: Broadcasting large task binary with size 1094.2 KiB\n",
      "25/08/05 23:13:34 WARN DAGScheduler: Broadcasting large task binary with size 1095.5 KiB\n",
      "25/08/05 23:13:39 WARN DAGScheduler: Broadcasting large task binary with size 1098.1 KiB\n",
      "25/08/05 23:13:43 WARN DAGScheduler: Broadcasting large task binary with size 1103.2 KiB\n",
      "25/08/05 23:13:47 WARN DAGScheduler: Broadcasting large task binary with size 1107.1 KiB\n",
      "25/08/05 23:13:53 WARN DAGScheduler: Broadcasting large task binary with size 1107.6 KiB\n",
      "25/08/05 23:13:57 WARN DAGScheduler: Broadcasting large task binary with size 1108.2 KiB\n",
      "25/08/05 23:14:01 WARN DAGScheduler: Broadcasting large task binary with size 1109.4 KiB\n",
      "25/08/05 23:14:05 WARN DAGScheduler: Broadcasting large task binary with size 1111.9 KiB\n",
      "25/08/05 23:14:09 WARN DAGScheduler: Broadcasting large task binary with size 1116.8 KiB\n",
      "25/08/05 23:14:14 WARN DAGScheduler: Broadcasting large task binary with size 1120.7 KiB\n",
      "25/08/05 23:14:18 WARN DAGScheduler: Broadcasting large task binary with size 1121.2 KiB\n",
      "25/08/05 23:14:20 WARN DAGScheduler: Broadcasting large task binary with size 1121.8 KiB\n",
      "25/08/05 23:14:22 WARN DAGScheduler: Broadcasting large task binary with size 1123.1 KiB\n",
      "25/08/05 23:14:24 WARN DAGScheduler: Broadcasting large task binary with size 1125.7 KiB\n",
      "25/08/05 23:14:27 WARN DAGScheduler: Broadcasting large task binary with size 1130.7 KiB\n",
      "25/08/05 23:14:30 WARN DAGScheduler: Broadcasting large task binary with size 1134.7 KiB\n",
      "25/08/05 23:14:33 WARN DAGScheduler: Broadcasting large task binary with size 1135.2 KiB\n",
      "25/08/05 23:14:35 WARN DAGScheduler: Broadcasting large task binary with size 1135.8 KiB\n",
      "25/08/05 23:14:37 WARN DAGScheduler: Broadcasting large task binary with size 1137.2 KiB\n",
      "25/08/05 23:14:40 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "25/08/05 23:14:42 WARN DAGScheduler: Broadcasting large task binary with size 1144.7 KiB\n",
      "25/08/05 23:14:45 WARN DAGScheduler: Broadcasting large task binary with size 1148.7 KiB\n",
      "25/08/05 23:14:48 WARN DAGScheduler: Broadcasting large task binary with size 1149.2 KiB\n",
      "25/08/05 23:14:50 WARN DAGScheduler: Broadcasting large task binary with size 1149.8 KiB\n",
      "25/08/05 23:14:52 WARN DAGScheduler: Broadcasting large task binary with size 1151.1 KiB\n",
      "25/08/05 23:14:54 WARN DAGScheduler: Broadcasting large task binary with size 1153.5 KiB\n",
      "25/08/05 23:14:56 WARN DAGScheduler: Broadcasting large task binary with size 1158.4 KiB\n",
      "25/08/05 23:14:59 WARN DAGScheduler: Broadcasting large task binary with size 1162.4 KiB\n",
      "25/08/05 23:15:02 WARN DAGScheduler: Broadcasting large task binary with size 1162.9 KiB\n",
      "25/08/05 23:15:04 WARN DAGScheduler: Broadcasting large task binary with size 1163.5 KiB\n",
      "25/08/05 23:15:06 WARN DAGScheduler: Broadcasting large task binary with size 1164.8 KiB\n",
      "25/08/05 23:15:08 WARN DAGScheduler: Broadcasting large task binary with size 1167.3 KiB\n",
      "25/08/05 23:15:10 WARN DAGScheduler: Broadcasting large task binary with size 1172.3 KiB\n",
      "25/08/05 23:15:13 WARN DAGScheduler: Broadcasting large task binary with size 1176.2 KiB\n",
      "25/08/05 23:15:16 WARN DAGScheduler: Broadcasting large task binary with size 1176.7 KiB\n",
      "25/08/05 23:15:18 WARN DAGScheduler: Broadcasting large task binary with size 1177.4 KiB\n",
      "25/08/05 23:15:20 WARN DAGScheduler: Broadcasting large task binary with size 1178.7 KiB\n",
      "25/08/05 23:15:22 WARN DAGScheduler: Broadcasting large task binary with size 1181.2 KiB\n",
      "25/08/05 23:15:24 WARN DAGScheduler: Broadcasting large task binary with size 1186.0 KiB\n",
      "25/08/05 23:15:27 WARN DAGScheduler: Broadcasting large task binary with size 1189.3 KiB\n",
      "25/08/05 23:15:30 WARN DAGScheduler: Broadcasting large task binary with size 1189.8 KiB\n",
      "25/08/05 23:15:32 WARN DAGScheduler: Broadcasting large task binary with size 1190.4 KiB\n",
      "25/08/05 23:15:34 WARN DAGScheduler: Broadcasting large task binary with size 1191.7 KiB\n",
      "25/08/05 23:15:36 WARN DAGScheduler: Broadcasting large task binary with size 1194.1 KiB\n",
      "25/08/05 23:15:39 WARN DAGScheduler: Broadcasting large task binary with size 1199.1 KiB\n",
      "25/08/05 23:15:41 WARN DAGScheduler: Broadcasting large task binary with size 1203.2 KiB\n",
      "25/08/05 23:15:45 WARN DAGScheduler: Broadcasting large task binary with size 1203.7 KiB\n",
      "25/08/05 23:15:47 WARN DAGScheduler: Broadcasting large task binary with size 1204.3 KiB\n",
      "25/08/05 23:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1205.6 KiB\n",
      "25/08/05 23:15:51 WARN DAGScheduler: Broadcasting large task binary with size 1208.1 KiB\n",
      "25/08/05 23:15:53 WARN DAGScheduler: Broadcasting large task binary with size 1213.1 KiB\n",
      "25/08/05 23:15:56 WARN DAGScheduler: Broadcasting large task binary with size 1217.1 KiB\n",
      "25/08/05 23:16:00 WARN DAGScheduler: Broadcasting large task binary with size 1217.6 KiB\n",
      "25/08/05 23:16:02 WARN DAGScheduler: Broadcasting large task binary with size 1218.3 KiB\n",
      "25/08/05 23:16:04 WARN DAGScheduler: Broadcasting large task binary with size 1219.6 KiB\n",
      "25/08/05 23:16:06 WARN DAGScheduler: Broadcasting large task binary with size 1222.1 KiB\n",
      "25/08/05 23:16:09 WARN DAGScheduler: Broadcasting large task binary with size 1227.0 KiB\n",
      "25/08/05 23:16:11 WARN DAGScheduler: Broadcasting large task binary with size 1230.8 KiB\n",
      "25/08/05 23:16:15 WARN DAGScheduler: Broadcasting large task binary with size 1231.3 KiB\n",
      "25/08/05 23:16:17 WARN DAGScheduler: Broadcasting large task binary with size 1231.9 KiB\n",
      "25/08/05 23:16:19 WARN DAGScheduler: Broadcasting large task binary with size 1233.3 KiB\n",
      "25/08/05 23:16:21 WARN DAGScheduler: Broadcasting large task binary with size 1235.8 KiB\n",
      "25/08/05 23:16:23 WARN DAGScheduler: Broadcasting large task binary with size 1240.5 KiB\n",
      "25/08/05 23:16:26 WARN DAGScheduler: Broadcasting large task binary with size 1244.3 KiB\n",
      "25/08/05 23:16:29 WARN DAGScheduler: Broadcasting large task binary with size 1244.7 KiB\n",
      "25/08/05 23:16:31 WARN DAGScheduler: Broadcasting large task binary with size 1245.3 KiB\n",
      "25/08/05 23:16:34 WARN DAGScheduler: Broadcasting large task binary with size 1246.6 KiB\n",
      "25/08/05 23:16:36 WARN DAGScheduler: Broadcasting large task binary with size 1249.2 KiB\n",
      "25/08/05 23:16:38 WARN DAGScheduler: Broadcasting large task binary with size 1254.2 KiB\n",
      "25/08/05 23:16:41 WARN DAGScheduler: Broadcasting large task binary with size 1258.0 KiB\n",
      "25/08/05 23:16:45 WARN DAGScheduler: Broadcasting large task binary with size 1258.5 KiB\n",
      "25/08/05 23:16:47 WARN DAGScheduler: Broadcasting large task binary with size 1259.1 KiB\n",
      "25/08/05 23:16:49 WARN DAGScheduler: Broadcasting large task binary with size 1260.4 KiB\n",
      "25/08/05 23:16:51 WARN DAGScheduler: Broadcasting large task binary with size 1262.9 KiB\n",
      "25/08/05 23:16:54 WARN DAGScheduler: Broadcasting large task binary with size 1267.9 KiB\n",
      "25/08/05 23:16:57 WARN DAGScheduler: Broadcasting large task binary with size 1271.6 KiB\n",
      "25/08/05 23:17:00 WARN DAGScheduler: Broadcasting large task binary with size 1272.1 KiB\n",
      "25/08/05 23:17:02 WARN DAGScheduler: Broadcasting large task binary with size 1272.8 KiB\n",
      "25/08/05 23:17:04 WARN DAGScheduler: Broadcasting large task binary with size 1274.1 KiB\n",
      "25/08/05 23:17:06 WARN DAGScheduler: Broadcasting large task binary with size 1276.7 KiB\n",
      "25/08/05 23:17:08 WARN DAGScheduler: Broadcasting large task binary with size 1281.5 KiB\n",
      "25/08/05 23:17:11 WARN DAGScheduler: Broadcasting large task binary with size 1285.5 KiB\n",
      "25/08/05 23:17:15 WARN DAGScheduler: Broadcasting large task binary with size 1286.0 KiB\n",
      "25/08/05 23:17:17 WARN DAGScheduler: Broadcasting large task binary with size 1286.6 KiB\n",
      "25/08/05 23:17:20 WARN DAGScheduler: Broadcasting large task binary with size 1287.8 KiB\n",
      "25/08/05 23:17:23 WARN DAGScheduler: Broadcasting large task binary with size 1290.3 KiB\n",
      "25/08/05 23:17:26 WARN DAGScheduler: Broadcasting large task binary with size 1295.1 KiB\n",
      "25/08/05 23:17:29 WARN DAGScheduler: Broadcasting large task binary with size 1299.1 KiB\n",
      "25/08/05 23:17:32 WARN DAGScheduler: Broadcasting large task binary with size 1299.6 KiB\n",
      "25/08/05 23:17:35 WARN DAGScheduler: Broadcasting large task binary with size 1300.2 KiB\n",
      "25/08/05 23:17:37 WARN DAGScheduler: Broadcasting large task binary with size 1301.5 KiB\n",
      "25/08/05 23:17:39 WARN DAGScheduler: Broadcasting large task binary with size 1304.0 KiB\n",
      "25/08/05 23:17:42 WARN DAGScheduler: Broadcasting large task binary with size 1308.9 KiB\n",
      "25/08/05 23:17:45 WARN DAGScheduler: Broadcasting large task binary with size 1312.8 KiB\n",
      "25/08/05 23:17:48 WARN DAGScheduler: Broadcasting large task binary with size 1313.3 KiB\n",
      "25/08/05 23:17:51 WARN DAGScheduler: Broadcasting large task binary with size 1313.9 KiB\n",
      "25/08/05 23:17:53 WARN DAGScheduler: Broadcasting large task binary with size 1315.3 KiB\n",
      "25/08/05 23:17:55 WARN DAGScheduler: Broadcasting large task binary with size 1317.8 KiB\n",
      "25/08/05 23:18:26 WARN DAGScheduler: Broadcasting large task binary with size 1322.8 KiB\n",
      "25/08/05 23:18:29 WARN DAGScheduler: Broadcasting large task binary with size 1326.8 KiB\n",
      "25/08/05 23:18:33 WARN DAGScheduler: Broadcasting large task binary with size 1327.3 KiB\n",
      "25/08/05 23:18:36 WARN DAGScheduler: Broadcasting large task binary with size 1327.9 KiB\n",
      "25/08/05 23:18:38 WARN DAGScheduler: Broadcasting large task binary with size 1329.2 KiB\n",
      "25/08/05 23:18:40 WARN DAGScheduler: Broadcasting large task binary with size 1331.7 KiB\n",
      "25/08/05 23:18:42 WARN DAGScheduler: Broadcasting large task binary with size 1336.6 KiB\n",
      "25/08/05 23:18:44 WARN DAGScheduler: Broadcasting large task binary with size 1340.5 KiB\n",
      "25/08/05 23:18:47 WARN DAGScheduler: Broadcasting large task binary with size 1340.9 KiB\n",
      "25/08/05 23:18:49 WARN DAGScheduler: Broadcasting large task binary with size 1341.5 KiB\n",
      "25/08/05 23:18:51 WARN DAGScheduler: Broadcasting large task binary with size 1342.9 KiB\n",
      "25/08/05 23:18:53 WARN DAGScheduler: Broadcasting large task binary with size 1345.5 KiB\n",
      "25/08/05 23:18:55 WARN DAGScheduler: Broadcasting large task binary with size 1350.5 KiB\n",
      "25/08/05 23:18:57 WARN DAGScheduler: Broadcasting large task binary with size 1354.3 KiB\n",
      "25/08/05 23:19:00 WARN DAGScheduler: Broadcasting large task binary with size 1354.8 KiB\n",
      "25/08/05 23:19:02 WARN DAGScheduler: Broadcasting large task binary with size 1355.4 KiB\n",
      "25/08/05 23:19:04 WARN DAGScheduler: Broadcasting large task binary with size 1356.7 KiB\n",
      "25/08/05 23:19:06 WARN DAGScheduler: Broadcasting large task binary with size 1359.1 KiB\n",
      "25/08/05 23:19:09 WARN DAGScheduler: Broadcasting large task binary with size 1364.0 KiB\n",
      "25/08/05 23:29:34 WARN DAGScheduler: Broadcasting large task binary with size 1368.0 KiB\n",
      "25/08/05 23:29:37 WARN DAGScheduler: Broadcasting large task binary with size 1368.5 KiB\n",
      "25/08/05 23:29:39 WARN DAGScheduler: Broadcasting large task binary with size 1369.1 KiB\n",
      "25/08/05 23:29:41 WARN DAGScheduler: Broadcasting large task binary with size 1370.4 KiB\n",
      "25/08/05 23:29:43 WARN DAGScheduler: Broadcasting large task binary with size 1372.9 KiB\n",
      "25/08/05 23:29:46 WARN DAGScheduler: Broadcasting large task binary with size 1377.9 KiB\n",
      "25/08/05 23:29:48 WARN DAGScheduler: Broadcasting large task binary with size 1381.9 KiB\n",
      "25/08/05 23:29:52 WARN DAGScheduler: Broadcasting large task binary with size 1382.4 KiB\n",
      "25/08/05 23:29:54 WARN DAGScheduler: Broadcasting large task binary with size 1383.1 KiB\n",
      "25/08/05 23:29:57 WARN DAGScheduler: Broadcasting large task binary with size 1384.4 KiB\n",
      "25/08/05 23:30:00 WARN DAGScheduler: Broadcasting large task binary with size 1386.9 KiB\n",
      "25/08/05 23:30:05 WARN DAGScheduler: Broadcasting large task binary with size 1391.8 KiB\n",
      "25/08/05 23:30:08 WARN DAGScheduler: Broadcasting large task binary with size 1395.8 KiB\n",
      "25/08/05 23:30:12 WARN DAGScheduler: Broadcasting large task binary with size 1396.4 KiB\n",
      "25/08/05 23:30:14 WARN DAGScheduler: Broadcasting large task binary with size 1397.0 KiB\n",
      "25/08/05 23:30:16 WARN DAGScheduler: Broadcasting large task binary with size 1398.2 KiB\n",
      "25/08/05 23:30:19 WARN DAGScheduler: Broadcasting large task binary with size 1400.7 KiB\n",
      "25/08/05 23:30:22 WARN DAGScheduler: Broadcasting large task binary with size 1405.7 KiB\n",
      "25/08/05 23:30:26 WARN DAGScheduler: Broadcasting large task binary with size 1409.4 KiB\n",
      "25/08/05 23:30:35 WARN DAGScheduler: Broadcasting large task binary with size 1409.8 KiB\n",
      "25/08/05 23:30:38 WARN DAGScheduler: Broadcasting large task binary with size 1410.4 KiB\n",
      "25/08/05 23:30:47 WARN DAGScheduler: Broadcasting large task binary with size 1411.7 KiB\n",
      "25/08/05 23:30:52 WARN DAGScheduler: Broadcasting large task binary with size 1414.2 KiB\n",
      "25/08/05 23:30:59 WARN DAGScheduler: Broadcasting large task binary with size 1419.3 KiB\n",
      "25/08/05 23:46:53 WARN DAGScheduler: Broadcasting large task binary with size 1423.3 KiB\n",
      "25/08/05 23:46:57 WARN DAGScheduler: Broadcasting large task binary with size 1423.8 KiB\n",
      "25/08/05 23:46:59 WARN DAGScheduler: Broadcasting large task binary with size 1424.4 KiB\n",
      "25/08/05 23:47:01 WARN DAGScheduler: Broadcasting large task binary with size 1425.7 KiB\n",
      "25/08/05 23:47:03 WARN DAGScheduler: Broadcasting large task binary with size 1428.3 KiB\n",
      "25/08/05 23:47:06 WARN DAGScheduler: Broadcasting large task binary with size 1433.3 KiB\n",
      "25/08/05 23:47:08 WARN DAGScheduler: Broadcasting large task binary with size 1437.2 KiB\n",
      "25/08/05 23:47:12 WARN DAGScheduler: Broadcasting large task binary with size 1437.7 KiB\n",
      "25/08/05 23:47:14 WARN DAGScheduler: Broadcasting large task binary with size 1438.3 KiB\n",
      "25/08/05 23:47:17 WARN DAGScheduler: Broadcasting large task binary with size 1439.6 KiB\n",
      "25/08/05 23:47:20 WARN DAGScheduler: Broadcasting large task binary with size 1442.1 KiB\n",
      "25/08/05 23:47:23 WARN DAGScheduler: Broadcasting large task binary with size 1447.0 KiB\n",
      "25/08/05 23:47:27 WARN DAGScheduler: Broadcasting large task binary with size 1450.9 KiB\n",
      "25/08/05 23:47:30 WARN DAGScheduler: Broadcasting large task binary with size 1451.4 KiB\n",
      "25/08/05 23:47:33 WARN DAGScheduler: Broadcasting large task binary with size 1452.0 KiB\n",
      "25/08/05 23:47:35 WARN DAGScheduler: Broadcasting large task binary with size 1453.3 KiB\n",
      "25/08/05 23:47:38 WARN DAGScheduler: Broadcasting large task binary with size 1455.8 KiB\n",
      "25/08/05 23:48:30 WARN DAGScheduler: Broadcasting large task binary with size 1460.7 KiB\n",
      "25/08/05 23:48:33 WARN DAGScheduler: Broadcasting large task binary with size 1464.8 KiB\n",
      "25/08/05 23:48:36 WARN DAGScheduler: Broadcasting large task binary with size 1465.3 KiB\n",
      "25/08/05 23:48:38 WARN DAGScheduler: Broadcasting large task binary with size 1466.0 KiB\n",
      "25/08/05 23:48:40 WARN DAGScheduler: Broadcasting large task binary with size 1467.3 KiB\n",
      "25/08/05 23:48:43 WARN DAGScheduler: Broadcasting large task binary with size 1469.9 KiB\n",
      "25/08/05 23:48:45 WARN DAGScheduler: Broadcasting large task binary with size 1474.9 KiB\n",
      "25/08/05 23:48:48 WARN DAGScheduler: Broadcasting large task binary with size 1478.9 KiB\n",
      "25/08/05 23:48:52 WARN DAGScheduler: Broadcasting large task binary with size 1479.4 KiB\n",
      "25/08/05 23:48:56 WARN DAGScheduler: Broadcasting large task binary with size 1480.0 KiB\n",
      "25/08/05 23:49:02 WARN DAGScheduler: Broadcasting large task binary with size 1481.3 KiB\n",
      "25/08/05 23:49:06 WARN DAGScheduler: Broadcasting large task binary with size 1483.8 KiB\n",
      "25/08/05 23:49:09 WARN DAGScheduler: Broadcasting large task binary with size 1488.8 KiB\n",
      "25/08/05 23:49:13 WARN DAGScheduler: Broadcasting large task binary with size 1492.6 KiB\n",
      "25/08/05 23:56:09 WARN DAGScheduler: Broadcasting large task binary with size 1493.1 KiB\n",
      "25/08/05 23:56:15 WARN DAGScheduler: Broadcasting large task binary with size 1493.8 KiB\n",
      "25/08/05 23:56:20 WARN DAGScheduler: Broadcasting large task binary with size 1495.1 KiB\n",
      "25/08/05 23:56:23 WARN DAGScheduler: Broadcasting large task binary with size 1497.6 KiB\n",
      "25/08/05 23:56:27 WARN DAGScheduler: Broadcasting large task binary with size 1502.7 KiB\n",
      "25/08/05 23:56:30 WARN DAGScheduler: Broadcasting large task binary with size 1506.3 KiB\n",
      "25/08/05 23:56:33 WARN DAGScheduler: Broadcasting large task binary with size 1506.8 KiB\n",
      "25/08/05 23:56:36 WARN DAGScheduler: Broadcasting large task binary with size 1507.4 KiB\n",
      "25/08/05 23:56:39 WARN DAGScheduler: Broadcasting large task binary with size 1508.7 KiB\n",
      "25/08/05 23:56:42 WARN DAGScheduler: Broadcasting large task binary with size 1511.2 KiB\n",
      "25/08/05 23:56:45 WARN DAGScheduler: Broadcasting large task binary with size 1516.1 KiB\n",
      "25/08/05 23:56:47 WARN DAGScheduler: Broadcasting large task binary with size 1519.9 KiB\n",
      "25/08/05 23:56:51 WARN DAGScheduler: Broadcasting large task binary with size 1520.4 KiB\n",
      "25/08/05 23:56:53 WARN DAGScheduler: Broadcasting large task binary with size 1520.9 KiB\n",
      "25/08/05 23:56:55 WARN DAGScheduler: Broadcasting large task binary with size 1522.2 KiB\n",
      "25/08/05 23:56:57 WARN DAGScheduler: Broadcasting large task binary with size 1524.7 KiB\n",
      "25/08/05 23:56:59 WARN DAGScheduler: Broadcasting large task binary with size 1529.7 KiB\n",
      "25/08/05 23:57:02 WARN DAGScheduler: Broadcasting large task binary with size 1533.8 KiB\n",
      "25/08/05 23:57:05 WARN DAGScheduler: Broadcasting large task binary with size 1534.3 KiB\n",
      "25/08/05 23:57:08 WARN DAGScheduler: Broadcasting large task binary with size 1535.0 KiB\n",
      "25/08/05 23:57:10 WARN DAGScheduler: Broadcasting large task binary with size 1536.3 KiB\n",
      "25/08/05 23:57:12 WARN DAGScheduler: Broadcasting large task binary with size 1538.8 KiB\n",
      "25/08/05 23:57:15 WARN DAGScheduler: Broadcasting large task binary with size 1543.9 KiB\n",
      "25/08/05 23:57:18 WARN DAGScheduler: Broadcasting large task binary with size 1547.9 KiB\n",
      "25/08/05 23:57:21 WARN DAGScheduler: Broadcasting large task binary with size 1548.5 KiB\n",
      "25/08/05 23:57:23 WARN DAGScheduler: Broadcasting large task binary with size 1549.1 KiB\n",
      "25/08/05 23:57:26 WARN DAGScheduler: Broadcasting large task binary with size 1550.4 KiB\n",
      "25/08/05 23:57:28 WARN DAGScheduler: Broadcasting large task binary with size 1553.0 KiB\n",
      "25/08/05 23:57:30 WARN DAGScheduler: Broadcasting large task binary with size 1558.0 KiB\n",
      "25/08/05 23:57:33 WARN DAGScheduler: Broadcasting large task binary with size 1562.0 KiB\n",
      "25/08/05 23:57:36 WARN DAGScheduler: Broadcasting large task binary with size 1562.5 KiB\n",
      "25/08/05 23:57:38 WARN DAGScheduler: Broadcasting large task binary with size 1563.1 KiB\n",
      "25/08/05 23:57:40 WARN DAGScheduler: Broadcasting large task binary with size 1564.4 KiB\n",
      "25/08/05 23:57:42 WARN DAGScheduler: Broadcasting large task binary with size 1566.9 KiB\n",
      "25/08/05 23:57:45 WARN DAGScheduler: Broadcasting large task binary with size 1571.9 KiB\n",
      "25/08/05 23:57:48 WARN DAGScheduler: Broadcasting large task binary with size 1575.8 KiB\n",
      "25/08/05 23:57:51 WARN DAGScheduler: Broadcasting large task binary with size 1576.2 KiB\n",
      "25/08/05 23:57:53 WARN DAGScheduler: Broadcasting large task binary with size 1576.9 KiB\n",
      "25/08/05 23:57:55 WARN DAGScheduler: Broadcasting large task binary with size 1578.2 KiB\n",
      "25/08/05 23:57:58 WARN DAGScheduler: Broadcasting large task binary with size 1580.7 KiB\n",
      "25/08/05 23:58:00 WARN DAGScheduler: Broadcasting large task binary with size 1585.8 KiB\n",
      "25/08/05 23:58:03 WARN DAGScheduler: Broadcasting large task binary with size 1589.6 KiB\n",
      "25/08/05 23:58:06 WARN DAGScheduler: Broadcasting large task binary with size 1590.1 KiB\n",
      "25/08/05 23:58:09 WARN DAGScheduler: Broadcasting large task binary with size 1590.7 KiB\n",
      "25/08/05 23:58:11 WARN DAGScheduler: Broadcasting large task binary with size 1592.0 KiB\n",
      "25/08/05 23:58:14 WARN DAGScheduler: Broadcasting large task binary with size 1594.5 KiB\n",
      "25/08/05 23:58:17 WARN DAGScheduler: Broadcasting large task binary with size 1599.5 KiB\n",
      "25/08/05 23:58:20 WARN DAGScheduler: Broadcasting large task binary with size 1603.6 KiB\n",
      "25/08/05 23:58:23 WARN DAGScheduler: Broadcasting large task binary with size 1604.2 KiB\n",
      "25/08/05 23:58:25 WARN DAGScheduler: Broadcasting large task binary with size 1604.8 KiB\n",
      "25/08/05 23:58:28 WARN DAGScheduler: Broadcasting large task binary with size 1606.1 KiB\n",
      "25/08/05 23:58:30 WARN DAGScheduler: Broadcasting large task binary with size 1608.7 KiB\n",
      "25/08/05 23:58:33 WARN DAGScheduler: Broadcasting large task binary with size 1613.6 KiB\n",
      "25/08/06 00:41:46 WARN DAGScheduler: Broadcasting large task binary with size 1002.0 KiB\n",
      "25/08/06 00:41:50 WARN DAGScheduler: Broadcasting large task binary with size 1006.0 KiB\n",
      "25/08/06 00:41:52 WARN DAGScheduler: Broadcasting large task binary with size 1006.5 KiB\n",
      "25/08/06 00:41:54 WARN DAGScheduler: Broadcasting large task binary with size 1007.2 KiB\n",
      "25/08/06 00:41:56 WARN DAGScheduler: Broadcasting large task binary with size 1008.5 KiB\n",
      "25/08/06 00:41:58 WARN DAGScheduler: Broadcasting large task binary with size 1011.0 KiB\n",
      "25/08/06 00:42:00 WARN DAGScheduler: Broadcasting large task binary with size 1016.1 KiB\n",
      "25/08/06 00:42:03 WARN DAGScheduler: Broadcasting large task binary with size 1019.7 KiB\n",
      "25/08/06 00:42:06 WARN DAGScheduler: Broadcasting large task binary with size 1020.2 KiB\n",
      "25/08/06 00:42:08 WARN DAGScheduler: Broadcasting large task binary with size 1020.8 KiB\n",
      "25/08/06 00:42:10 WARN DAGScheduler: Broadcasting large task binary with size 1022.0 KiB\n",
      "25/08/06 00:42:11 WARN DAGScheduler: Broadcasting large task binary with size 1024.5 KiB\n",
      "25/08/06 00:42:14 WARN DAGScheduler: Broadcasting large task binary with size 1029.5 KiB\n",
      "25/08/06 00:42:17 WARN DAGScheduler: Broadcasting large task binary with size 1033.6 KiB\n",
      "25/08/06 00:42:20 WARN DAGScheduler: Broadcasting large task binary with size 1034.0 KiB\n",
      "25/08/06 00:42:21 WARN DAGScheduler: Broadcasting large task binary with size 1034.6 KiB\n",
      "25/08/06 00:42:23 WARN DAGScheduler: Broadcasting large task binary with size 1035.9 KiB\n",
      "25/08/06 00:42:25 WARN DAGScheduler: Broadcasting large task binary with size 1038.4 KiB\n",
      "25/08/06 00:42:27 WARN DAGScheduler: Broadcasting large task binary with size 1043.3 KiB\n",
      "25/08/06 00:42:30 WARN DAGScheduler: Broadcasting large task binary with size 1047.4 KiB\n",
      "25/08/06 00:42:33 WARN DAGScheduler: Broadcasting large task binary with size 1047.9 KiB\n",
      "25/08/06 00:42:35 WARN DAGScheduler: Broadcasting large task binary with size 1048.5 KiB\n",
      "25/08/06 00:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1049.8 KiB\n",
      "25/08/06 00:42:39 WARN DAGScheduler: Broadcasting large task binary with size 1052.4 KiB\n",
      "25/08/06 00:42:41 WARN DAGScheduler: Broadcasting large task binary with size 1057.4 KiB\n",
      "25/08/06 00:42:44 WARN DAGScheduler: Broadcasting large task binary with size 1061.3 KiB\n",
      "25/08/06 00:42:47 WARN DAGScheduler: Broadcasting large task binary with size 1061.8 KiB\n",
      "25/08/06 00:42:49 WARN DAGScheduler: Broadcasting large task binary with size 1062.4 KiB\n",
      "25/08/06 00:42:51 WARN DAGScheduler: Broadcasting large task binary with size 1063.8 KiB\n",
      "25/08/06 00:42:53 WARN DAGScheduler: Broadcasting large task binary with size 1066.3 KiB\n",
      "25/08/06 00:42:55 WARN DAGScheduler: Broadcasting large task binary with size 1071.4 KiB\n",
      "25/08/06 00:42:58 WARN DAGScheduler: Broadcasting large task binary with size 1075.2 KiB\n",
      "25/08/06 00:43:02 WARN DAGScheduler: Broadcasting large task binary with size 1075.7 KiB\n",
      "25/08/06 00:43:04 WARN DAGScheduler: Broadcasting large task binary with size 1076.3 KiB\n",
      "25/08/06 00:43:06 WARN DAGScheduler: Broadcasting large task binary with size 1077.6 KiB\n",
      "25/08/06 00:43:08 WARN DAGScheduler: Broadcasting large task binary with size 1080.0 KiB\n",
      "25/08/06 00:43:10 WARN DAGScheduler: Broadcasting large task binary with size 1085.1 KiB\n",
      "25/08/06 00:43:13 WARN DAGScheduler: Broadcasting large task binary with size 1089.3 KiB\n",
      "25/08/06 00:43:16 WARN DAGScheduler: Broadcasting large task binary with size 1089.8 KiB\n",
      "25/08/06 00:43:18 WARN DAGScheduler: Broadcasting large task binary with size 1090.4 KiB\n",
      "25/08/06 00:43:20 WARN DAGScheduler: Broadcasting large task binary with size 1091.6 KiB\n",
      "25/08/06 00:43:22 WARN DAGScheduler: Broadcasting large task binary with size 1094.1 KiB\n",
      "25/08/06 00:43:24 WARN DAGScheduler: Broadcasting large task binary with size 1099.0 KiB\n",
      "25/08/06 00:43:27 WARN DAGScheduler: Broadcasting large task binary with size 1103.0 KiB\n",
      "25/08/06 00:43:30 WARN DAGScheduler: Broadcasting large task binary with size 1103.5 KiB\n",
      "25/08/06 00:43:32 WARN DAGScheduler: Broadcasting large task binary with size 1104.1 KiB\n",
      "25/08/06 00:43:34 WARN DAGScheduler: Broadcasting large task binary with size 1105.3 KiB\n",
      "25/08/06 00:43:36 WARN DAGScheduler: Broadcasting large task binary with size 1107.9 KiB\n",
      "25/08/06 00:43:38 WARN DAGScheduler: Broadcasting large task binary with size 1112.9 KiB\n",
      "25/08/06 00:43:41 WARN DAGScheduler: Broadcasting large task binary with size 1116.9 KiB\n",
      "25/08/06 00:43:44 WARN DAGScheduler: Broadcasting large task binary with size 1117.4 KiB\n",
      "25/08/06 00:43:46 WARN DAGScheduler: Broadcasting large task binary with size 1118.0 KiB\n",
      "25/08/06 00:43:48 WARN DAGScheduler: Broadcasting large task binary with size 1119.3 KiB\n",
      "25/08/06 00:43:50 WARN DAGScheduler: Broadcasting large task binary with size 1121.8 KiB\n",
      "25/08/06 00:43:52 WARN DAGScheduler: Broadcasting large task binary with size 1126.9 KiB\n",
      "25/08/06 00:43:55 WARN DAGScheduler: Broadcasting large task binary with size 1130.9 KiB\n",
      "25/08/06 00:43:58 WARN DAGScheduler: Broadcasting large task binary with size 1131.4 KiB\n",
      "25/08/06 00:44:00 WARN DAGScheduler: Broadcasting large task binary with size 1131.9 KiB\n",
      "25/08/06 00:44:03 WARN DAGScheduler: Broadcasting large task binary with size 1133.2 KiB\n",
      "25/08/06 00:44:05 WARN DAGScheduler: Broadcasting large task binary with size 1135.7 KiB\n",
      "25/08/06 00:44:08 WARN DAGScheduler: Broadcasting large task binary with size 1140.6 KiB\n",
      "25/08/06 00:44:11 WARN DAGScheduler: Broadcasting large task binary with size 1144.6 KiB\n",
      "25/08/06 00:44:14 WARN DAGScheduler: Broadcasting large task binary with size 1145.0 KiB\n",
      "25/08/06 00:44:17 WARN DAGScheduler: Broadcasting large task binary with size 1145.6 KiB\n",
      "25/08/06 00:44:18 WARN DAGScheduler: Broadcasting large task binary with size 1146.9 KiB\n",
      "25/08/06 00:44:21 WARN DAGScheduler: Broadcasting large task binary with size 1149.4 KiB\n",
      "25/08/06 00:44:24 WARN DAGScheduler: Broadcasting large task binary with size 1154.5 KiB\n",
      "25/08/06 00:44:26 WARN DAGScheduler: Broadcasting large task binary with size 1158.5 KiB\n",
      "25/08/06 00:44:30 WARN DAGScheduler: Broadcasting large task binary with size 1159.0 KiB\n",
      "25/08/06 00:44:32 WARN DAGScheduler: Broadcasting large task binary with size 1159.7 KiB\n",
      "25/08/06 00:44:34 WARN DAGScheduler: Broadcasting large task binary with size 1161.0 KiB\n",
      "25/08/06 00:44:36 WARN DAGScheduler: Broadcasting large task binary with size 1163.4 KiB\n",
      "25/08/06 00:44:38 WARN DAGScheduler: Broadcasting large task binary with size 1168.0 KiB\n",
      "25/08/06 00:44:42 WARN DAGScheduler: Broadcasting large task binary with size 1171.6 KiB\n",
      "25/08/06 00:44:48 WARN DAGScheduler: Broadcasting large task binary with size 1172.1 KiB\n",
      "25/08/06 00:44:51 WARN DAGScheduler: Broadcasting large task binary with size 1172.8 KiB\n",
      "25/08/06 00:44:53 WARN DAGScheduler: Broadcasting large task binary with size 1174.1 KiB\n",
      "25/08/06 00:44:56 WARN DAGScheduler: Broadcasting large task binary with size 1176.5 KiB\n",
      "25/08/06 00:44:59 WARN DAGScheduler: Broadcasting large task binary with size 1181.5 KiB\n",
      "25/08/06 00:45:03 WARN DAGScheduler: Broadcasting large task binary with size 1185.6 KiB\n",
      "25/08/06 00:45:06 WARN DAGScheduler: Broadcasting large task binary with size 1186.1 KiB\n",
      "25/08/06 00:45:08 WARN DAGScheduler: Broadcasting large task binary with size 1186.7 KiB\n",
      "25/08/06 00:45:11 WARN DAGScheduler: Broadcasting large task binary with size 1188.0 KiB\n",
      "25/08/06 00:45:13 WARN DAGScheduler: Broadcasting large task binary with size 1190.5 KiB\n",
      "25/08/06 00:45:16 WARN DAGScheduler: Broadcasting large task binary with size 1195.3 KiB\n",
      "25/08/06 00:45:19 WARN DAGScheduler: Broadcasting large task binary with size 1199.1 KiB\n",
      "25/08/06 00:45:23 WARN DAGScheduler: Broadcasting large task binary with size 1199.6 KiB\n",
      "25/08/06 00:45:25 WARN DAGScheduler: Broadcasting large task binary with size 1200.3 KiB\n",
      "25/08/06 00:45:28 WARN DAGScheduler: Broadcasting large task binary with size 1201.6 KiB\n",
      "25/08/06 00:45:30 WARN DAGScheduler: Broadcasting large task binary with size 1204.1 KiB\n",
      "25/08/06 00:45:33 WARN DAGScheduler: Broadcasting large task binary with size 1209.1 KiB\n",
      "25/08/06 00:45:36 WARN DAGScheduler: Broadcasting large task binary with size 1213.1 KiB\n",
      "25/08/06 00:45:40 WARN DAGScheduler: Broadcasting large task binary with size 1213.6 KiB\n",
      "25/08/06 00:45:42 WARN DAGScheduler: Broadcasting large task binary with size 1214.3 KiB\n",
      "25/08/06 00:45:44 WARN DAGScheduler: Broadcasting large task binary with size 1215.6 KiB\n",
      "25/08/06 00:45:47 WARN DAGScheduler: Broadcasting large task binary with size 1218.2 KiB\n",
      "25/08/06 00:45:49 WARN DAGScheduler: Broadcasting large task binary with size 1223.1 KiB\n",
      "25/08/06 00:45:52 WARN DAGScheduler: Broadcasting large task binary with size 1227.1 KiB\n",
      "25/08/06 00:45:56 WARN DAGScheduler: Broadcasting large task binary with size 1227.6 KiB\n",
      "25/08/06 00:45:58 WARN DAGScheduler: Broadcasting large task binary with size 1228.2 KiB\n",
      "25/08/06 00:46:00 WARN DAGScheduler: Broadcasting large task binary with size 1229.4 KiB\n",
      "25/08/06 00:46:03 WARN DAGScheduler: Broadcasting large task binary with size 1231.9 KiB\n",
      "25/08/06 00:46:05 WARN DAGScheduler: Broadcasting large task binary with size 1236.8 KiB\n",
      "25/08/06 00:46:08 WARN DAGScheduler: Broadcasting large task binary with size 1240.9 KiB\n",
      "25/08/06 00:46:11 WARN DAGScheduler: Broadcasting large task binary with size 1241.3 KiB\n",
      "25/08/06 00:46:13 WARN DAGScheduler: Broadcasting large task binary with size 1241.9 KiB\n",
      "25/08/06 00:46:16 WARN DAGScheduler: Broadcasting large task binary with size 1243.2 KiB\n",
      "25/08/06 00:46:18 WARN DAGScheduler: Broadcasting large task binary with size 1245.7 KiB\n",
      "25/08/06 00:46:20 WARN DAGScheduler: Broadcasting large task binary with size 1250.7 KiB\n",
      "25/08/06 00:46:23 WARN DAGScheduler: Broadcasting large task binary with size 1254.8 KiB\n",
      "25/08/06 00:46:26 WARN DAGScheduler: Broadcasting large task binary with size 1255.3 KiB\n",
      "25/08/06 00:46:28 WARN DAGScheduler: Broadcasting large task binary with size 1255.9 KiB\n",
      "25/08/06 00:46:30 WARN DAGScheduler: Broadcasting large task binary with size 1257.3 KiB\n",
      "25/08/06 00:46:33 WARN DAGScheduler: Broadcasting large task binary with size 1259.8 KiB\n",
      "25/08/06 00:46:35 WARN DAGScheduler: Broadcasting large task binary with size 1264.9 KiB\n",
      "25/08/06 00:46:38 WARN DAGScheduler: Broadcasting large task binary with size 1268.9 KiB\n",
      "25/08/06 00:46:42 WARN DAGScheduler: Broadcasting large task binary with size 1269.4 KiB\n",
      "25/08/06 00:46:44 WARN DAGScheduler: Broadcasting large task binary with size 1270.0 KiB\n",
      "25/08/06 00:46:46 WARN DAGScheduler: Broadcasting large task binary with size 1271.3 KiB\n",
      "25/08/06 00:46:49 WARN DAGScheduler: Broadcasting large task binary with size 1273.7 KiB\n",
      "25/08/06 00:46:51 WARN DAGScheduler: Broadcasting large task binary with size 1278.3 KiB\n",
      "25/08/06 00:46:54 WARN DAGScheduler: Broadcasting large task binary with size 1282.0 KiB\n",
      "25/08/06 00:46:58 WARN DAGScheduler: Broadcasting large task binary with size 1282.5 KiB\n",
      "25/08/06 00:47:00 WARN DAGScheduler: Broadcasting large task binary with size 1283.2 KiB\n",
      "25/08/06 00:47:02 WARN DAGScheduler: Broadcasting large task binary with size 1284.5 KiB\n",
      "25/08/06 00:47:05 WARN DAGScheduler: Broadcasting large task binary with size 1287.0 KiB\n",
      "25/08/06 00:47:07 WARN DAGScheduler: Broadcasting large task binary with size 1292.0 KiB\n",
      "25/08/06 00:47:10 WARN DAGScheduler: Broadcasting large task binary with size 1295.6 KiB\n",
      "25/08/06 00:47:13 WARN DAGScheduler: Broadcasting large task binary with size 1296.0 KiB\n",
      "25/08/06 00:47:16 WARN DAGScheduler: Broadcasting large task binary with size 1296.6 KiB\n",
      "25/08/06 00:47:18 WARN DAGScheduler: Broadcasting large task binary with size 1297.9 KiB\n",
      "25/08/06 00:47:20 WARN DAGScheduler: Broadcasting large task binary with size 1300.5 KiB\n",
      "25/08/06 00:47:23 WARN DAGScheduler: Broadcasting large task binary with size 1305.4 KiB\n",
      "25/08/06 00:47:26 WARN DAGScheduler: Broadcasting large task binary with size 1309.5 KiB\n",
      "25/08/06 00:47:30 WARN DAGScheduler: Broadcasting large task binary with size 1310.0 KiB\n",
      "25/08/06 00:47:32 WARN DAGScheduler: Broadcasting large task binary with size 1310.7 KiB\n",
      "25/08/06 00:47:34 WARN DAGScheduler: Broadcasting large task binary with size 1312.0 KiB\n",
      "25/08/06 00:47:37 WARN DAGScheduler: Broadcasting large task binary with size 1314.5 KiB\n",
      "25/08/06 00:47:39 WARN DAGScheduler: Broadcasting large task binary with size 1319.5 KiB\n",
      "25/08/06 00:47:43 WARN DAGScheduler: Broadcasting large task binary with size 1323.3 KiB\n",
      "25/08/06 00:47:46 WARN DAGScheduler: Broadcasting large task binary with size 1323.8 KiB\n",
      "25/08/06 00:47:49 WARN DAGScheduler: Broadcasting large task binary with size 1324.4 KiB\n",
      "25/08/06 00:47:51 WARN DAGScheduler: Broadcasting large task binary with size 1325.7 KiB\n",
      "25/08/06 00:47:53 WARN DAGScheduler: Broadcasting large task binary with size 1328.3 KiB\n",
      "25/08/06 00:47:56 WARN DAGScheduler: Broadcasting large task binary with size 1333.4 KiB\n",
      "25/08/06 00:47:58 WARN DAGScheduler: Broadcasting large task binary with size 1337.3 KiB\n",
      "25/08/06 00:48:02 WARN DAGScheduler: Broadcasting large task binary with size 1337.8 KiB\n",
      "25/08/06 00:48:04 WARN DAGScheduler: Broadcasting large task binary with size 1338.4 KiB\n",
      "25/08/06 00:48:06 WARN DAGScheduler: Broadcasting large task binary with size 1339.6 KiB\n",
      "25/08/06 00:48:08 WARN DAGScheduler: Broadcasting large task binary with size 1342.1 KiB\n",
      "25/08/06 00:48:11 WARN DAGScheduler: Broadcasting large task binary with size 1347.1 KiB\n",
      "25/08/06 00:48:13 WARN DAGScheduler: Broadcasting large task binary with size 1350.7 KiB\n",
      "25/08/06 00:48:18 WARN DAGScheduler: Broadcasting large task binary with size 1351.2 KiB\n",
      "25/08/06 00:48:20 WARN DAGScheduler: Broadcasting large task binary with size 1351.9 KiB\n",
      "25/08/06 00:48:22 WARN DAGScheduler: Broadcasting large task binary with size 1353.2 KiB\n",
      "25/08/06 00:48:25 WARN DAGScheduler: Broadcasting large task binary with size 1355.7 KiB\n",
      "25/08/06 00:48:27 WARN DAGScheduler: Broadcasting large task binary with size 1360.6 KiB\n",
      "25/08/06 00:48:30 WARN DAGScheduler: Broadcasting large task binary with size 1364.6 KiB\n",
      "25/08/06 00:48:34 WARN DAGScheduler: Broadcasting large task binary with size 1365.1 KiB\n",
      "25/08/06 00:48:36 WARN DAGScheduler: Broadcasting large task binary with size 1365.7 KiB\n",
      "25/08/06 00:48:38 WARN DAGScheduler: Broadcasting large task binary with size 1367.0 KiB\n",
      "25/08/06 00:48:40 WARN DAGScheduler: Broadcasting large task binary with size 1369.5 KiB\n",
      "25/08/06 00:48:43 WARN DAGScheduler: Broadcasting large task binary with size 1374.6 KiB\n",
      "25/08/06 00:48:46 WARN DAGScheduler: Broadcasting large task binary with size 1378.7 KiB\n",
      "25/08/06 00:48:49 WARN DAGScheduler: Broadcasting large task binary with size 1379.2 KiB\n",
      "25/08/06 00:48:51 WARN DAGScheduler: Broadcasting large task binary with size 1379.8 KiB\n",
      "25/08/06 00:48:53 WARN DAGScheduler: Broadcasting large task binary with size 1381.1 KiB\n",
      "25/08/06 00:48:56 WARN DAGScheduler: Broadcasting large task binary with size 1383.7 KiB\n",
      "25/08/06 00:48:58 WARN DAGScheduler: Broadcasting large task binary with size 1388.7 KiB\n",
      "25/08/06 00:49:01 WARN DAGScheduler: Broadcasting large task binary with size 1392.5 KiB\n",
      "25/08/06 00:49:05 WARN DAGScheduler: Broadcasting large task binary with size 1393.0 KiB\n",
      "25/08/06 00:49:07 WARN DAGScheduler: Broadcasting large task binary with size 1393.6 KiB\n",
      "25/08/06 00:49:09 WARN DAGScheduler: Broadcasting large task binary with size 1394.9 KiB\n",
      "25/08/06 00:49:11 WARN DAGScheduler: Broadcasting large task binary with size 1397.4 KiB\n",
      "25/08/06 00:49:14 WARN DAGScheduler: Broadcasting large task binary with size 1402.3 KiB\n",
      "25/08/06 00:49:17 WARN DAGScheduler: Broadcasting large task binary with size 1406.4 KiB\n",
      "25/08/06 00:49:21 WARN DAGScheduler: Broadcasting large task binary with size 1406.9 KiB\n",
      "25/08/06 00:49:23 WARN DAGScheduler: Broadcasting large task binary with size 1407.5 KiB\n",
      "25/08/06 00:49:25 WARN DAGScheduler: Broadcasting large task binary with size 1408.9 KiB\n",
      "25/08/06 00:49:27 WARN DAGScheduler: Broadcasting large task binary with size 1411.5 KiB\n",
      "25/08/06 00:49:30 WARN DAGScheduler: Broadcasting large task binary with size 1416.4 KiB\n",
      "25/08/06 00:49:33 WARN DAGScheduler: Broadcasting large task binary with size 1420.2 KiB\n",
      "25/08/06 00:49:37 WARN DAGScheduler: Broadcasting large task binary with size 1420.7 KiB\n",
      "25/08/06 00:49:39 WARN DAGScheduler: Broadcasting large task binary with size 1421.3 KiB\n",
      "25/08/06 00:49:42 WARN DAGScheduler: Broadcasting large task binary with size 1422.7 KiB\n",
      "25/08/06 00:49:44 WARN DAGScheduler: Broadcasting large task binary with size 1425.2 KiB\n",
      "25/08/06 00:49:46 WARN DAGScheduler: Broadcasting large task binary with size 1430.0 KiB\n",
      "25/08/06 00:49:49 WARN DAGScheduler: Broadcasting large task binary with size 1433.6 KiB\n",
      "25/08/06 00:49:53 WARN DAGScheduler: Broadcasting large task binary with size 1434.0 KiB\n",
      "25/08/06 00:49:56 WARN DAGScheduler: Broadcasting large task binary with size 1434.7 KiB\n",
      "25/08/06 00:49:58 WARN DAGScheduler: Broadcasting large task binary with size 1436.0 KiB\n",
      "25/08/06 00:50:00 WARN DAGScheduler: Broadcasting large task binary with size 1438.5 KiB\n",
      "25/08/06 00:50:03 WARN DAGScheduler: Broadcasting large task binary with size 1443.4 KiB\n",
      "25/08/06 00:50:06 WARN DAGScheduler: Broadcasting large task binary with size 1447.3 KiB\n",
      "25/08/06 00:50:10 WARN DAGScheduler: Broadcasting large task binary with size 1447.8 KiB\n",
      "25/08/06 00:50:12 WARN DAGScheduler: Broadcasting large task binary with size 1448.4 KiB\n",
      "25/08/06 00:50:14 WARN DAGScheduler: Broadcasting large task binary with size 1449.7 KiB\n",
      "25/08/06 00:50:17 WARN DAGScheduler: Broadcasting large task binary with size 1452.3 KiB\n",
      "25/08/06 00:50:20 WARN DAGScheduler: Broadcasting large task binary with size 1456.9 KiB\n",
      "25/08/06 00:50:23 WARN DAGScheduler: Broadcasting large task binary with size 1460.6 KiB\n",
      "25/08/06 00:50:26 WARN DAGScheduler: Broadcasting large task binary with size 1461.2 KiB\n",
      "25/08/06 00:50:28 WARN DAGScheduler: Broadcasting large task binary with size 1461.8 KiB\n",
      "25/08/06 00:50:31 WARN DAGScheduler: Broadcasting large task binary with size 1463.1 KiB\n",
      "25/08/06 00:50:34 WARN DAGScheduler: Broadcasting large task binary with size 1465.6 KiB\n",
      "25/08/06 00:50:37 WARN DAGScheduler: Broadcasting large task binary with size 1470.6 KiB\n",
      "25/08/06 00:50:40 WARN DAGScheduler: Broadcasting large task binary with size 1474.3 KiB\n",
      "25/08/06 00:50:43 WARN DAGScheduler: Broadcasting large task binary with size 1474.8 KiB\n",
      "25/08/06 00:50:46 WARN DAGScheduler: Broadcasting large task binary with size 1475.4 KiB\n",
      "25/08/06 00:50:48 WARN DAGScheduler: Broadcasting large task binary with size 1476.8 KiB\n",
      "25/08/06 00:50:51 WARN DAGScheduler: Broadcasting large task binary with size 1479.4 KiB\n",
      "25/08/06 00:50:53 WARN DAGScheduler: Broadcasting large task binary with size 1484.4 KiB\n",
      "25/08/06 00:50:56 WARN DAGScheduler: Broadcasting large task binary with size 1487.9 KiB\n",
      "25/08/06 00:51:00 WARN DAGScheduler: Broadcasting large task binary with size 1488.4 KiB\n",
      "25/08/06 00:51:03 WARN DAGScheduler: Broadcasting large task binary with size 1489.0 KiB\n",
      "25/08/06 00:51:06 WARN DAGScheduler: Broadcasting large task binary with size 1490.3 KiB\n",
      "25/08/06 00:51:09 WARN DAGScheduler: Broadcasting large task binary with size 1492.8 KiB\n",
      "25/08/06 00:51:12 WARN DAGScheduler: Broadcasting large task binary with size 1497.7 KiB\n",
      "25/08/06 00:51:15 WARN DAGScheduler: Broadcasting large task binary with size 1501.8 KiB\n",
      "25/08/06 00:51:19 WARN DAGScheduler: Broadcasting large task binary with size 1502.3 KiB\n",
      "25/08/06 00:51:22 WARN DAGScheduler: Broadcasting large task binary with size 1502.8 KiB\n",
      "25/08/06 00:51:24 WARN DAGScheduler: Broadcasting large task binary with size 1504.1 KiB\n",
      "25/08/06 00:51:26 WARN DAGScheduler: Broadcasting large task binary with size 1506.6 KiB\n",
      "25/08/06 00:51:29 WARN DAGScheduler: Broadcasting large task binary with size 1511.5 KiB\n",
      "25/08/06 00:51:32 WARN DAGScheduler: Broadcasting large task binary with size 1515.7 KiB\n",
      "25/08/06 00:51:36 WARN DAGScheduler: Broadcasting large task binary with size 1516.2 KiB\n",
      "25/08/06 00:51:38 WARN DAGScheduler: Broadcasting large task binary with size 1516.8 KiB\n",
      "25/08/06 00:51:41 WARN DAGScheduler: Broadcasting large task binary with size 1518.1 KiB\n",
      "25/08/06 00:51:44 WARN DAGScheduler: Broadcasting large task binary with size 1520.6 KiB\n",
      "25/08/06 00:51:47 WARN DAGScheduler: Broadcasting large task binary with size 1525.7 KiB\n",
      "25/08/06 00:51:50 WARN DAGScheduler: Broadcasting large task binary with size 1529.6 KiB\n",
      "25/08/06 00:51:54 WARN DAGScheduler: Broadcasting large task binary with size 1530.2 KiB\n",
      "25/08/06 00:51:56 WARN DAGScheduler: Broadcasting large task binary with size 1530.8 KiB\n",
      "25/08/06 00:51:59 WARN DAGScheduler: Broadcasting large task binary with size 1532.1 KiB\n",
      "25/08/06 00:52:02 WARN DAGScheduler: Broadcasting large task binary with size 1534.6 KiB\n",
      "25/08/06 00:52:05 WARN DAGScheduler: Broadcasting large task binary with size 1539.5 KiB\n",
      "25/08/06 00:52:08 WARN DAGScheduler: Broadcasting large task binary with size 1543.5 KiB\n",
      "25/08/06 00:52:12 WARN DAGScheduler: Broadcasting large task binary with size 1544.0 KiB\n",
      "25/08/06 00:52:14 WARN DAGScheduler: Broadcasting large task binary with size 1544.6 KiB\n",
      "25/08/06 00:52:17 WARN DAGScheduler: Broadcasting large task binary with size 1546.0 KiB\n",
      "25/08/06 00:52:20 WARN DAGScheduler: Broadcasting large task binary with size 1548.5 KiB\n",
      "25/08/06 00:52:22 WARN DAGScheduler: Broadcasting large task binary with size 1553.5 KiB\n",
      "25/08/06 00:52:26 WARN DAGScheduler: Broadcasting large task binary with size 1557.2 KiB\n",
      "25/08/06 00:52:29 WARN DAGScheduler: Broadcasting large task binary with size 1557.8 KiB\n",
      "25/08/06 00:52:32 WARN DAGScheduler: Broadcasting large task binary with size 1558.3 KiB\n",
      "25/08/06 00:52:35 WARN DAGScheduler: Broadcasting large task binary with size 1559.6 KiB\n",
      "25/08/06 00:52:37 WARN DAGScheduler: Broadcasting large task binary with size 1562.1 KiB\n",
      "25/08/06 00:52:40 WARN DAGScheduler: Broadcasting large task binary with size 1567.2 KiB\n",
      "25/08/06 00:52:43 WARN DAGScheduler: Broadcasting large task binary with size 1571.4 KiB\n",
      "25/08/06 00:52:47 WARN DAGScheduler: Broadcasting large task binary with size 1571.9 KiB\n",
      "25/08/06 00:52:50 WARN DAGScheduler: Broadcasting large task binary with size 1572.5 KiB\n",
      "25/08/06 00:52:53 WARN DAGScheduler: Broadcasting large task binary with size 1573.7 KiB\n",
      "25/08/06 00:52:56 WARN DAGScheduler: Broadcasting large task binary with size 1576.2 KiB\n",
      "25/08/06 00:52:59 WARN DAGScheduler: Broadcasting large task binary with size 1581.1 KiB\n",
      "25/08/06 00:53:02 WARN DAGScheduler: Broadcasting large task binary with size 1585.2 KiB\n",
      "25/08/06 00:53:06 WARN DAGScheduler: Broadcasting large task binary with size 1585.7 KiB\n",
      "25/08/06 00:53:09 WARN DAGScheduler: Broadcasting large task binary with size 1586.3 KiB\n",
      "25/08/06 00:53:12 WARN DAGScheduler: Broadcasting large task binary with size 1587.7 KiB\n",
      "25/08/06 00:53:15 WARN DAGScheduler: Broadcasting large task binary with size 1590.2 KiB\n",
      "25/08/06 00:53:17 WARN DAGScheduler: Broadcasting large task binary with size 1595.2 KiB\n",
      "25/08/06 00:53:21 WARN DAGScheduler: Broadcasting large task binary with size 1598.9 KiB\n",
      "25/08/06 00:53:24 WARN DAGScheduler: Broadcasting large task binary with size 1599.4 KiB\n",
      "25/08/06 00:53:27 WARN DAGScheduler: Broadcasting large task binary with size 1600.1 KiB\n",
      "25/08/06 00:53:30 WARN DAGScheduler: Broadcasting large task binary with size 1601.4 KiB\n",
      "25/08/06 00:53:33 WARN DAGScheduler: Broadcasting large task binary with size 1603.9 KiB\n",
      "25/08/06 00:53:36 WARN DAGScheduler: Broadcasting large task binary with size 1608.8 KiB\n",
      "25/08/06 00:55:32 WARN MemoryStore: Not enough space to cache rdd_16499_5 in memory! (computed 29.6 MiB so far)\n",
      "25/08/06 00:55:32 WARN MemoryStore: Not enough space to cache rdd_16499_2 in memory! (computed 29.6 MiB so far)\n",
      "25/08/06 00:55:32 WARN MemoryStore: Not enough space to cache rdd_16499_0 in memory! (computed 29.6 MiB so far)\n",
      "25/08/06 00:55:32 WARN MemoryStore: Not enough space to cache rdd_16499_3 in memory! (computed 29.6 MiB so far)\n",
      "25/08/06 00:55:32 WARN MemoryStore: Not enough space to cache rdd_16499_7 in memory! (computed 29.6 MiB so far)\n",
      "25/08/06 00:55:32 WARN BlockManager: Persisting block rdd_16499_5 to disk instead.\n",
      "25/08/06 00:55:32 WARN BlockManager: Persisting block rdd_16499_7 to disk instead.\n",
      "25/08/06 00:55:32 WARN BlockManager: Persisting block rdd_16499_3 to disk instead.\n",
      "25/08/06 00:55:32 WARN BlockManager: Persisting block rdd_16499_2 to disk instead.\n",
      "25/08/06 00:55:32 WARN BlockManager: Persisting block rdd_16499_0 to disk instead.\n",
      "25/08/06 00:57:27 WARN DAGScheduler: Broadcasting large task binary with size 1001.7 KiB\n",
      "25/08/06 00:57:27 WARN DAGScheduler: Broadcasting large task binary with size 1006.7 KiB\n",
      "25/08/06 00:57:28 WARN DAGScheduler: Broadcasting large task binary with size 1010.7 KiB\n",
      "25/08/06 00:57:28 WARN DAGScheduler: Broadcasting large task binary with size 1011.2 KiB\n",
      "25/08/06 00:57:29 WARN DAGScheduler: Broadcasting large task binary with size 1011.7 KiB\n",
      "25/08/06 00:57:29 WARN DAGScheduler: Broadcasting large task binary with size 1013.0 KiB\n",
      "25/08/06 00:57:29 WARN DAGScheduler: Broadcasting large task binary with size 1015.5 KiB\n",
      "25/08/06 00:57:30 WARN DAGScheduler: Broadcasting large task binary with size 1020.6 KiB\n",
      "25/08/06 00:57:30 WARN DAGScheduler: Broadcasting large task binary with size 1024.7 KiB\n",
      "25/08/06 00:57:31 WARN DAGScheduler: Broadcasting large task binary with size 1025.2 KiB\n",
      "25/08/06 00:57:31 WARN DAGScheduler: Broadcasting large task binary with size 1025.8 KiB\n",
      "25/08/06 00:57:31 WARN DAGScheduler: Broadcasting large task binary with size 1027.2 KiB\n",
      "25/08/06 00:57:32 WARN DAGScheduler: Broadcasting large task binary with size 1029.7 KiB\n",
      "25/08/06 00:57:32 WARN DAGScheduler: Broadcasting large task binary with size 1034.7 KiB\n",
      "25/08/06 00:57:32 WARN DAGScheduler: Broadcasting large task binary with size 1038.6 KiB\n",
      "25/08/06 00:57:33 WARN DAGScheduler: Broadcasting large task binary with size 1039.1 KiB\n",
      "25/08/06 00:57:33 WARN DAGScheduler: Broadcasting large task binary with size 1039.7 KiB\n",
      "25/08/06 00:57:34 WARN DAGScheduler: Broadcasting large task binary with size 1041.0 KiB\n",
      "25/08/06 00:57:34 WARN DAGScheduler: Broadcasting large task binary with size 1043.4 KiB\n",
      "25/08/06 00:57:34 WARN DAGScheduler: Broadcasting large task binary with size 1048.4 KiB\n",
      "25/08/06 00:57:34 WARN DAGScheduler: Broadcasting large task binary with size 1052.4 KiB\n",
      "25/08/06 00:57:35 WARN DAGScheduler: Broadcasting large task binary with size 1052.8 KiB\n",
      "25/08/06 00:57:36 WARN DAGScheduler: Broadcasting large task binary with size 1053.4 KiB\n",
      "25/08/06 00:57:36 WARN DAGScheduler: Broadcasting large task binary with size 1054.7 KiB\n",
      "25/08/06 00:57:36 WARN DAGScheduler: Broadcasting large task binary with size 1057.1 KiB\n",
      "25/08/06 00:57:37 WARN DAGScheduler: Broadcasting large task binary with size 1062.0 KiB\n",
      "25/08/06 00:57:37 WARN DAGScheduler: Broadcasting large task binary with size 1066.1 KiB\n",
      "25/08/06 00:57:38 WARN DAGScheduler: Broadcasting large task binary with size 1066.6 KiB\n",
      "25/08/06 00:57:38 WARN DAGScheduler: Broadcasting large task binary with size 1067.2 KiB\n",
      "25/08/06 00:57:38 WARN DAGScheduler: Broadcasting large task binary with size 1068.5 KiB\n",
      "25/08/06 00:57:39 WARN DAGScheduler: Broadcasting large task binary with size 1071.1 KiB\n",
      "25/08/06 00:57:39 WARN DAGScheduler: Broadcasting large task binary with size 1076.1 KiB\n",
      "25/08/06 00:57:39 WARN DAGScheduler: Broadcasting large task binary with size 1079.9 KiB\n",
      "25/08/06 00:57:40 WARN DAGScheduler: Broadcasting large task binary with size 1080.4 KiB\n",
      "25/08/06 00:57:40 WARN DAGScheduler: Broadcasting large task binary with size 1081.0 KiB\n",
      "25/08/06 00:57:41 WARN DAGScheduler: Broadcasting large task binary with size 1082.4 KiB\n",
      "25/08/06 00:57:41 WARN DAGScheduler: Broadcasting large task binary with size 1084.8 KiB\n",
      "25/08/06 00:57:41 WARN DAGScheduler: Broadcasting large task binary with size 1089.8 KiB\n",
      "25/08/06 00:57:41 WARN DAGScheduler: Broadcasting large task binary with size 1093.6 KiB\n",
      "25/08/06 00:57:42 WARN DAGScheduler: Broadcasting large task binary with size 1094.1 KiB\n",
      "25/08/06 00:57:42 WARN DAGScheduler: Broadcasting large task binary with size 1094.7 KiB\n",
      "25/08/06 00:57:42 WARN DAGScheduler: Broadcasting large task binary with size 1095.9 KiB\n",
      "25/08/06 00:57:43 WARN DAGScheduler: Broadcasting large task binary with size 1098.5 KiB\n",
      "25/08/06 00:57:43 WARN DAGScheduler: Broadcasting large task binary with size 1103.5 KiB\n",
      "25/08/06 00:57:43 WARN DAGScheduler: Broadcasting large task binary with size 1107.4 KiB\n",
      "25/08/06 00:57:44 WARN DAGScheduler: Broadcasting large task binary with size 1107.9 KiB\n",
      "25/08/06 00:57:44 WARN DAGScheduler: Broadcasting large task binary with size 1108.5 KiB\n",
      "25/08/06 00:57:44 WARN DAGScheduler: Broadcasting large task binary with size 1109.7 KiB\n",
      "25/08/06 00:57:45 WARN DAGScheduler: Broadcasting large task binary with size 1112.1 KiB\n",
      "25/08/06 00:57:45 WARN DAGScheduler: Broadcasting large task binary with size 1116.9 KiB\n",
      "25/08/06 00:57:46 WARN DAGScheduler: Broadcasting large task binary with size 1120.9 KiB\n",
      "25/08/06 00:57:47 WARN DAGScheduler: Broadcasting large task binary with size 1121.4 KiB\n",
      "25/08/06 00:57:47 WARN DAGScheduler: Broadcasting large task binary with size 1122.0 KiB\n",
      "25/08/06 00:57:47 WARN DAGScheduler: Broadcasting large task binary with size 1123.3 KiB\n",
      "25/08/06 00:57:47 WARN DAGScheduler: Broadcasting large task binary with size 1125.9 KiB\n",
      "25/08/06 00:57:48 WARN DAGScheduler: Broadcasting large task binary with size 1131.0 KiB\n",
      "25/08/06 00:57:48 WARN DAGScheduler: Broadcasting large task binary with size 1135.0 KiB\n",
      "25/08/06 00:57:49 WARN DAGScheduler: Broadcasting large task binary with size 1135.5 KiB\n",
      "25/08/06 00:57:49 WARN DAGScheduler: Broadcasting large task binary with size 1136.1 KiB\n",
      "25/08/06 00:57:49 WARN DAGScheduler: Broadcasting large task binary with size 1137.4 KiB\n",
      "25/08/06 00:57:50 WARN DAGScheduler: Broadcasting large task binary with size 1140.0 KiB\n",
      "25/08/06 00:57:50 WARN DAGScheduler: Broadcasting large task binary with size 1145.0 KiB\n",
      "25/08/06 00:57:50 WARN DAGScheduler: Broadcasting large task binary with size 1149.1 KiB\n",
      "25/08/06 00:57:51 WARN DAGScheduler: Broadcasting large task binary with size 1149.6 KiB\n",
      "25/08/06 00:57:51 WARN DAGScheduler: Broadcasting large task binary with size 1150.3 KiB\n",
      "25/08/06 00:57:51 WARN DAGScheduler: Broadcasting large task binary with size 1151.6 KiB\n",
      "25/08/06 00:57:52 WARN DAGScheduler: Broadcasting large task binary with size 1154.2 KiB\n",
      "25/08/06 00:57:52 WARN DAGScheduler: Broadcasting large task binary with size 1159.1 KiB\n",
      "25/08/06 00:57:52 WARN DAGScheduler: Broadcasting large task binary with size 1163.1 KiB\n",
      "25/08/06 00:57:53 WARN DAGScheduler: Broadcasting large task binary with size 1163.5 KiB\n",
      "25/08/06 00:57:53 WARN DAGScheduler: Broadcasting large task binary with size 1164.1 KiB\n",
      "25/08/06 00:57:54 WARN DAGScheduler: Broadcasting large task binary with size 1165.4 KiB\n",
      "25/08/06 00:57:54 WARN DAGScheduler: Broadcasting large task binary with size 1167.9 KiB\n",
      "25/08/06 00:57:55 WARN DAGScheduler: Broadcasting large task binary with size 1172.9 KiB\n",
      "25/08/06 00:57:56 WARN DAGScheduler: Broadcasting large task binary with size 1177.1 KiB\n",
      "25/08/06 00:57:56 WARN DAGScheduler: Broadcasting large task binary with size 1177.6 KiB\n",
      "25/08/06 00:57:57 WARN DAGScheduler: Broadcasting large task binary with size 1178.3 KiB\n",
      "25/08/06 00:57:57 WARN DAGScheduler: Broadcasting large task binary with size 1179.6 KiB\n",
      "25/08/06 00:57:57 WARN DAGScheduler: Broadcasting large task binary with size 1182.1 KiB\n",
      "25/08/06 00:57:57 WARN DAGScheduler: Broadcasting large task binary with size 1187.1 KiB\n",
      "25/08/06 00:57:58 WARN DAGScheduler: Broadcasting large task binary with size 1190.8 KiB\n",
      "25/08/06 00:57:58 WARN DAGScheduler: Broadcasting large task binary with size 1191.3 KiB\n",
      "25/08/06 00:57:59 WARN DAGScheduler: Broadcasting large task binary with size 1191.9 KiB\n",
      "25/08/06 00:57:59 WARN DAGScheduler: Broadcasting large task binary with size 1193.2 KiB\n",
      "25/08/06 00:57:59 WARN DAGScheduler: Broadcasting large task binary with size 1195.6 KiB\n",
      "25/08/06 00:57:59 WARN DAGScheduler: Broadcasting large task binary with size 1200.4 KiB\n",
      "25/08/06 00:58:00 WARN DAGScheduler: Broadcasting large task binary with size 1204.4 KiB\n",
      "25/08/06 00:58:00 WARN DAGScheduler: Broadcasting large task binary with size 1204.9 KiB\n",
      "25/08/06 00:58:01 WARN DAGScheduler: Broadcasting large task binary with size 1205.5 KiB\n",
      "25/08/06 00:58:01 WARN DAGScheduler: Broadcasting large task binary with size 1206.9 KiB\n",
      "25/08/06 00:58:01 WARN DAGScheduler: Broadcasting large task binary with size 1209.4 KiB\n",
      "25/08/06 00:58:02 WARN DAGScheduler: Broadcasting large task binary with size 1214.3 KiB\n",
      "25/08/06 00:58:02 WARN DAGScheduler: Broadcasting large task binary with size 1218.1 KiB\n",
      "25/08/06 00:58:02 WARN DAGScheduler: Broadcasting large task binary with size 1218.5 KiB\n",
      "25/08/06 00:58:03 WARN DAGScheduler: Broadcasting large task binary with size 1219.2 KiB\n",
      "25/08/06 00:58:03 WARN DAGScheduler: Broadcasting large task binary with size 1220.5 KiB\n",
      "25/08/06 00:58:03 WARN DAGScheduler: Broadcasting large task binary with size 1223.0 KiB\n",
      "25/08/06 00:58:03 WARN DAGScheduler: Broadcasting large task binary with size 1227.7 KiB\n",
      "25/08/06 00:58:04 WARN DAGScheduler: Broadcasting large task binary with size 1231.2 KiB\n",
      "25/08/06 00:58:05 WARN DAGScheduler: Broadcasting large task binary with size 1231.7 KiB\n",
      "25/08/06 00:58:05 WARN DAGScheduler: Broadcasting large task binary with size 1232.3 KiB\n",
      "25/08/06 00:58:05 WARN DAGScheduler: Broadcasting large task binary with size 1233.6 KiB\n",
      "25/08/06 00:58:05 WARN DAGScheduler: Broadcasting large task binary with size 1236.0 KiB\n",
      "25/08/06 00:58:06 WARN DAGScheduler: Broadcasting large task binary with size 1241.0 KiB\n",
      "25/08/06 00:58:06 WARN DAGScheduler: Broadcasting large task binary with size 1245.1 KiB\n",
      "25/08/06 00:58:07 WARN DAGScheduler: Broadcasting large task binary with size 1245.6 KiB\n",
      "25/08/06 00:58:07 WARN DAGScheduler: Broadcasting large task binary with size 1246.2 KiB\n",
      "25/08/06 00:58:08 WARN DAGScheduler: Broadcasting large task binary with size 1247.4 KiB\n",
      "25/08/06 00:58:08 WARN DAGScheduler: Broadcasting large task binary with size 1249.9 KiB\n",
      "25/08/06 00:58:09 WARN DAGScheduler: Broadcasting large task binary with size 1254.9 KiB\n",
      "25/08/06 00:58:09 WARN DAGScheduler: Broadcasting large task binary with size 1259.0 KiB\n",
      "25/08/06 00:58:10 WARN DAGScheduler: Broadcasting large task binary with size 1259.4 KiB\n",
      "25/08/06 00:58:10 WARN DAGScheduler: Broadcasting large task binary with size 1260.0 KiB\n",
      "25/08/06 00:58:10 WARN DAGScheduler: Broadcasting large task binary with size 1261.3 KiB\n",
      "25/08/06 00:58:10 WARN DAGScheduler: Broadcasting large task binary with size 1263.8 KiB\n",
      "25/08/06 00:58:11 WARN DAGScheduler: Broadcasting large task binary with size 1268.8 KiB\n",
      "25/08/06 00:58:11 WARN DAGScheduler: Broadcasting large task binary with size 1272.6 KiB\n",
      "25/08/06 00:58:12 WARN DAGScheduler: Broadcasting large task binary with size 1273.1 KiB\n",
      "25/08/06 00:58:12 WARN DAGScheduler: Broadcasting large task binary with size 1273.8 KiB\n",
      "25/08/06 00:58:13 WARN DAGScheduler: Broadcasting large task binary with size 1275.1 KiB\n",
      "25/08/06 00:58:13 WARN DAGScheduler: Broadcasting large task binary with size 1277.7 KiB\n",
      "25/08/06 00:58:14 WARN DAGScheduler: Broadcasting large task binary with size 1282.5 KiB\n",
      "25/08/06 00:58:14 WARN DAGScheduler: Broadcasting large task binary with size 1286.3 KiB\n",
      "25/08/06 00:58:15 WARN DAGScheduler: Broadcasting large task binary with size 1286.9 KiB\n",
      "25/08/06 00:58:15 WARN DAGScheduler: Broadcasting large task binary with size 1287.5 KiB\n",
      "25/08/06 00:58:16 WARN DAGScheduler: Broadcasting large task binary with size 1288.8 KiB\n",
      "25/08/06 00:58:16 WARN DAGScheduler: Broadcasting large task binary with size 1291.4 KiB\n",
      "25/08/06 00:58:16 WARN DAGScheduler: Broadcasting large task binary with size 1296.4 KiB\n",
      "25/08/06 00:58:17 WARN DAGScheduler: Broadcasting large task binary with size 1300.4 KiB\n",
      "25/08/06 00:58:17 WARN DAGScheduler: Broadcasting large task binary with size 1300.8 KiB\n",
      "25/08/06 00:58:17 WARN DAGScheduler: Broadcasting large task binary with size 1301.4 KiB\n",
      "25/08/06 00:58:18 WARN DAGScheduler: Broadcasting large task binary with size 1302.7 KiB\n",
      "25/08/06 00:58:18 WARN DAGScheduler: Broadcasting large task binary with size 1305.2 KiB\n",
      "25/08/06 00:58:18 WARN DAGScheduler: Broadcasting large task binary with size 1310.2 KiB\n",
      "25/08/06 00:58:19 WARN DAGScheduler: Broadcasting large task binary with size 1314.2 KiB\n",
      "25/08/06 00:58:19 WARN DAGScheduler: Broadcasting large task binary with size 1314.7 KiB\n",
      "25/08/06 00:58:20 WARN DAGScheduler: Broadcasting large task binary with size 1315.3 KiB\n",
      "25/08/06 00:58:20 WARN DAGScheduler: Broadcasting large task binary with size 1316.6 KiB\n",
      "25/08/06 00:58:20 WARN DAGScheduler: Broadcasting large task binary with size 1319.0 KiB\n",
      "25/08/06 00:58:21 WARN DAGScheduler: Broadcasting large task binary with size 1323.9 KiB\n",
      "25/08/06 00:58:21 WARN DAGScheduler: Broadcasting large task binary with size 1328.1 KiB\n",
      "25/08/06 00:58:22 WARN DAGScheduler: Broadcasting large task binary with size 1328.6 KiB\n",
      "25/08/06 00:58:22 WARN DAGScheduler: Broadcasting large task binary with size 1329.3 KiB\n",
      "25/08/06 00:58:23 WARN DAGScheduler: Broadcasting large task binary with size 1330.6 KiB\n",
      "25/08/06 00:58:23 WARN DAGScheduler: Broadcasting large task binary with size 1333.2 KiB\n",
      "25/08/06 00:58:23 WARN DAGScheduler: Broadcasting large task binary with size 1338.2 KiB\n",
      "25/08/06 00:58:24 WARN DAGScheduler: Broadcasting large task binary with size 1342.2 KiB\n",
      "25/08/06 00:58:25 WARN DAGScheduler: Broadcasting large task binary with size 1342.7 KiB\n",
      "25/08/06 00:58:25 WARN DAGScheduler: Broadcasting large task binary with size 1343.2 KiB\n",
      "25/08/06 00:58:25 WARN DAGScheduler: Broadcasting large task binary with size 1344.5 KiB\n",
      "25/08/06 00:58:26 WARN DAGScheduler: Broadcasting large task binary with size 1347.0 KiB\n",
      "25/08/06 00:58:26 WARN DAGScheduler: Broadcasting large task binary with size 1352.0 KiB\n",
      "25/08/06 00:58:26 WARN DAGScheduler: Broadcasting large task binary with size 1355.8 KiB\n",
      "25/08/06 00:58:27 WARN DAGScheduler: Broadcasting large task binary with size 1356.3 KiB\n",
      "25/08/06 00:58:27 WARN DAGScheduler: Broadcasting large task binary with size 1356.9 KiB\n",
      "25/08/06 00:58:28 WARN DAGScheduler: Broadcasting large task binary with size 1358.2 KiB\n",
      "25/08/06 00:58:28 WARN DAGScheduler: Broadcasting large task binary with size 1360.8 KiB\n",
      "25/08/06 00:58:28 WARN DAGScheduler: Broadcasting large task binary with size 1365.8 KiB\n",
      "25/08/06 00:58:29 WARN DAGScheduler: Broadcasting large task binary with size 1369.7 KiB\n",
      "25/08/06 00:58:30 WARN DAGScheduler: Broadcasting large task binary with size 1370.2 KiB\n",
      "25/08/06 00:58:30 WARN DAGScheduler: Broadcasting large task binary with size 1370.8 KiB\n",
      "25/08/06 00:58:31 WARN DAGScheduler: Broadcasting large task binary with size 1372.1 KiB\n",
      "25/08/06 00:58:31 WARN DAGScheduler: Broadcasting large task binary with size 1374.7 KiB\n",
      "25/08/06 00:58:32 WARN DAGScheduler: Broadcasting large task binary with size 1379.8 KiB\n",
      "25/08/06 00:58:32 WARN DAGScheduler: Broadcasting large task binary with size 1383.8 KiB\n",
      "25/08/06 00:58:33 WARN DAGScheduler: Broadcasting large task binary with size 1384.3 KiB\n",
      "25/08/06 00:58:33 WARN DAGScheduler: Broadcasting large task binary with size 1384.9 KiB\n",
      "25/08/06 00:58:34 WARN DAGScheduler: Broadcasting large task binary with size 1386.2 KiB\n",
      "25/08/06 00:58:34 WARN DAGScheduler: Broadcasting large task binary with size 1388.7 KiB\n",
      "25/08/06 00:58:35 WARN DAGScheduler: Broadcasting large task binary with size 1393.6 KiB\n",
      "25/08/06 00:58:35 WARN DAGScheduler: Broadcasting large task binary with size 1397.7 KiB\n",
      "25/08/06 00:58:36 WARN DAGScheduler: Broadcasting large task binary with size 1398.2 KiB\n",
      "25/08/06 00:58:36 WARN DAGScheduler: Broadcasting large task binary with size 1398.8 KiB\n",
      "25/08/06 00:58:37 WARN DAGScheduler: Broadcasting large task binary with size 1400.1 KiB\n",
      "25/08/06 00:58:37 WARN DAGScheduler: Broadcasting large task binary with size 1402.7 KiB\n",
      "25/08/06 00:58:37 WARN DAGScheduler: Broadcasting large task binary with size 1407.6 KiB\n",
      "25/08/06 00:58:37 WARN DAGScheduler: Broadcasting large task binary with size 1411.4 KiB\n",
      "25/08/06 00:58:38 WARN DAGScheduler: Broadcasting large task binary with size 1411.9 KiB\n",
      "25/08/06 00:58:38 WARN DAGScheduler: Broadcasting large task binary with size 1412.5 KiB\n",
      "25/08/06 00:58:39 WARN DAGScheduler: Broadcasting large task binary with size 1413.8 KiB\n",
      "25/08/06 00:58:39 WARN DAGScheduler: Broadcasting large task binary with size 1416.4 KiB\n",
      "25/08/06 00:58:40 WARN DAGScheduler: Broadcasting large task binary with size 1421.4 KiB\n",
      "25/08/06 00:58:40 WARN DAGScheduler: Broadcasting large task binary with size 1425.6 KiB\n",
      "25/08/06 00:58:41 WARN DAGScheduler: Broadcasting large task binary with size 1426.0 KiB\n",
      "25/08/06 00:58:41 WARN DAGScheduler: Broadcasting large task binary with size 1426.6 KiB\n",
      "25/08/06 00:58:41 WARN DAGScheduler: Broadcasting large task binary with size 1427.8 KiB\n",
      "25/08/06 00:58:42 WARN DAGScheduler: Broadcasting large task binary with size 1430.3 KiB\n",
      "25/08/06 00:58:42 WARN DAGScheduler: Broadcasting large task binary with size 1435.2 KiB\n",
      "25/08/06 00:58:43 WARN DAGScheduler: Broadcasting large task binary with size 1439.2 KiB\n",
      "25/08/06 00:58:43 WARN DAGScheduler: Broadcasting large task binary with size 1439.7 KiB\n",
      "25/08/06 00:58:44 WARN DAGScheduler: Broadcasting large task binary with size 1440.4 KiB\n",
      "25/08/06 00:58:44 WARN DAGScheduler: Broadcasting large task binary with size 1441.7 KiB\n",
      "25/08/06 00:58:44 WARN DAGScheduler: Broadcasting large task binary with size 1444.3 KiB\n",
      "25/08/06 00:58:45 WARN DAGScheduler: Broadcasting large task binary with size 1449.3 KiB\n",
      "25/08/06 00:58:45 WARN DAGScheduler: Broadcasting large task binary with size 1453.3 KiB\n",
      "25/08/06 00:58:46 WARN DAGScheduler: Broadcasting large task binary with size 1453.8 KiB\n",
      "25/08/06 00:58:46 WARN DAGScheduler: Broadcasting large task binary with size 1454.4 KiB\n",
      "25/08/06 00:58:46 WARN DAGScheduler: Broadcasting large task binary with size 1455.7 KiB\n",
      "25/08/06 00:58:47 WARN DAGScheduler: Broadcasting large task binary with size 1458.2 KiB\n",
      "25/08/06 00:58:47 WARN DAGScheduler: Broadcasting large task binary with size 1463.3 KiB\n",
      "25/08/06 00:58:48 WARN DAGScheduler: Broadcasting large task binary with size 1467.2 KiB\n",
      "25/08/06 00:58:49 WARN DAGScheduler: Broadcasting large task binary with size 1467.7 KiB\n",
      "25/08/06 00:58:49 WARN DAGScheduler: Broadcasting large task binary with size 1468.3 KiB\n",
      "25/08/06 00:58:49 WARN DAGScheduler: Broadcasting large task binary with size 1469.6 KiB\n",
      "25/08/06 00:58:49 WARN DAGScheduler: Broadcasting large task binary with size 1472.1 KiB\n",
      "25/08/06 00:58:50 WARN DAGScheduler: Broadcasting large task binary with size 1477.1 KiB\n",
      "25/08/06 00:58:50 WARN DAGScheduler: Broadcasting large task binary with size 1481.1 KiB\n",
      "25/08/06 00:58:51 WARN DAGScheduler: Broadcasting large task binary with size 1481.6 KiB\n",
      "25/08/06 00:58:51 WARN DAGScheduler: Broadcasting large task binary with size 1482.2 KiB\n",
      "25/08/06 00:58:51 WARN DAGScheduler: Broadcasting large task binary with size 1483.6 KiB\n",
      "25/08/06 00:58:52 WARN DAGScheduler: Broadcasting large task binary with size 1486.2 KiB\n",
      "25/08/06 00:58:52 WARN DAGScheduler: Broadcasting large task binary with size 1491.1 KiB\n",
      "25/08/06 00:58:52 WARN DAGScheduler: Broadcasting large task binary with size 1494.9 KiB\n",
      "25/08/06 00:58:53 WARN DAGScheduler: Broadcasting large task binary with size 1495.4 KiB\n",
      "25/08/06 00:58:53 WARN DAGScheduler: Broadcasting large task binary with size 1496.0 KiB\n",
      "25/08/06 00:58:54 WARN DAGScheduler: Broadcasting large task binary with size 1497.3 KiB\n",
      "25/08/06 00:58:54 WARN DAGScheduler: Broadcasting large task binary with size 1499.8 KiB\n",
      "25/08/06 00:58:54 WARN DAGScheduler: Broadcasting large task binary with size 1504.9 KiB\n",
      "25/08/06 00:58:55 WARN DAGScheduler: Broadcasting large task binary with size 1508.7 KiB\n",
      "25/08/06 00:58:55 WARN DAGScheduler: Broadcasting large task binary with size 1509.2 KiB\n",
      "25/08/06 00:58:56 WARN DAGScheduler: Broadcasting large task binary with size 1509.8 KiB\n",
      "25/08/06 00:58:56 WARN DAGScheduler: Broadcasting large task binary with size 1511.0 KiB\n",
      "25/08/06 00:58:56 WARN DAGScheduler: Broadcasting large task binary with size 1513.6 KiB\n",
      "25/08/06 00:58:57 WARN DAGScheduler: Broadcasting large task binary with size 1518.7 KiB\n",
      "25/08/06 00:58:57 WARN DAGScheduler: Broadcasting large task binary with size 1522.8 KiB\n",
      "25/08/06 00:58:58 WARN DAGScheduler: Broadcasting large task binary with size 1523.3 KiB\n",
      "25/08/06 00:58:58 WARN DAGScheduler: Broadcasting large task binary with size 1523.9 KiB\n",
      "25/08/06 00:58:58 WARN DAGScheduler: Broadcasting large task binary with size 1525.3 KiB\n",
      "25/08/06 00:58:59 WARN DAGScheduler: Broadcasting large task binary with size 1527.8 KiB\n",
      "25/08/06 00:58:59 WARN DAGScheduler: Broadcasting large task binary with size 1532.9 KiB\n",
      "25/08/06 00:58:59 WARN DAGScheduler: Broadcasting large task binary with size 1536.8 KiB\n",
      "25/08/06 00:59:00 WARN DAGScheduler: Broadcasting large task binary with size 1537.3 KiB\n",
      "25/08/06 00:59:00 WARN DAGScheduler: Broadcasting large task binary with size 1538.0 KiB\n",
      "25/08/06 00:59:01 WARN DAGScheduler: Broadcasting large task binary with size 1539.3 KiB\n",
      "25/08/06 00:59:01 WARN DAGScheduler: Broadcasting large task binary with size 1541.8 KiB\n",
      "25/08/06 00:59:01 WARN DAGScheduler: Broadcasting large task binary with size 1546.9 KiB\n",
      "25/08/06 00:59:02 WARN DAGScheduler: Broadcasting large task binary with size 1550.8 KiB\n",
      "25/08/06 00:59:02 WARN DAGScheduler: Broadcasting large task binary with size 1551.3 KiB\n",
      "25/08/06 00:59:03 WARN DAGScheduler: Broadcasting large task binary with size 1551.9 KiB\n",
      "25/08/06 00:59:03 WARN DAGScheduler: Broadcasting large task binary with size 1553.2 KiB\n",
      "25/08/06 00:59:03 WARN DAGScheduler: Broadcasting large task binary with size 1555.7 KiB\n",
      "25/08/06 00:59:04 WARN DAGScheduler: Broadcasting large task binary with size 1560.7 KiB\n",
      "25/08/06 00:59:04 WARN DAGScheduler: Broadcasting large task binary with size 1564.2 KiB\n",
      "25/08/06 00:59:05 WARN DAGScheduler: Broadcasting large task binary with size 1564.8 KiB\n",
      "25/08/06 00:59:05 WARN DAGScheduler: Broadcasting large task binary with size 1565.4 KiB\n",
      "25/08/06 00:59:06 WARN DAGScheduler: Broadcasting large task binary with size 1566.7 KiB\n",
      "25/08/06 00:59:06 WARN DAGScheduler: Broadcasting large task binary with size 1569.3 KiB\n",
      "25/08/06 00:59:06 WARN DAGScheduler: Broadcasting large task binary with size 1574.2 KiB\n",
      "25/08/06 00:59:07 WARN DAGScheduler: Broadcasting large task binary with size 1578.1 KiB\n",
      "25/08/06 00:59:07 WARN DAGScheduler: Broadcasting large task binary with size 1578.6 KiB\n",
      "25/08/06 00:59:08 WARN DAGScheduler: Broadcasting large task binary with size 1579.2 KiB\n",
      "25/08/06 00:59:08 WARN DAGScheduler: Broadcasting large task binary with size 1580.5 KiB\n",
      "25/08/06 00:59:08 WARN DAGScheduler: Broadcasting large task binary with size 1583.0 KiB\n",
      "25/08/06 00:59:09 WARN DAGScheduler: Broadcasting large task binary with size 1587.9 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL: RMSE 19.27 | MAE 15.67 | R2 0.752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17620:================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: RMSE 19.33 | MAE 15.69 | R2 0.751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 02:15:27 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 885771 ms exceeds timeout 120000 ms\n",
      "25/08/06 02:15:27 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "25/08/06 02:15:31 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:15:31 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:15:41 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:15:41 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:15:51 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:15:51 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:16:01 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:16:01 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:16:11 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:16:11 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:18:43 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:18:43 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:18:53 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:18:53 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:19:03 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:19:03 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:19:13 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:19:13 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:19:39 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:19:39 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:19:49 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:19:49 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:19:59 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:19:59 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:20:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:20:09 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:20:19 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:20:19 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:20:29 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:20:29 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:20:39 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:20:39 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:20:49 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:20:49 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:20:59 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:20:59 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:21:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:21:09 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:21:19 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:21:19 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:21:29 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:21:29 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:21:39 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:21:39 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:21:49 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:21:49 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:28:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:28:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:28:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:28:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:28:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:28:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:28:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:28:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:28:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:28:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:28:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:28:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:29:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:29:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:29:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/06 02:29:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.18:51150\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "# Example: Only run RandomForest (edit for LinearReg or GBT)\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# ---- Pick ONE model ----\n",
    "estimator = gbt\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(estimator.maxDepth, [4, 6]) \\\n",
    "    .addGrid(estimator.maxIter, [50, 100]) \\\n",
    "    .build()\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, estimator])\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=2,      # Only 2-fold for speed/memory!\n",
    "    parallelism=2,   # If 4 crashes, set to 2 or 1\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "cv_model = cv.fit(train_df)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# --- Evaluate on VAL ---\n",
    "val_preds = best_model.transform(val_df)\n",
    "rmse_v, mae_v, mse_v, r2_v = full_metrics(val_preds)\n",
    "print(f\"VAL: RMSE {rmse_v:.2f} | MAE {mae_v:.2f} | R2 {r2_v:.3f}\")\n",
    "\n",
    "# --- Evaluate on TEST ---\n",
    "test_preds = best_model.transform(test_df)\n",
    "rmse_t, mae_t, mse_t, r2_t = full_metrics(test_preds)\n",
    "print(f\"TEST: RMSE {rmse_t:.2f} | MAE {mae_t:.2f} | R2 {r2_t:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93758471",
   "metadata": {},
   "source": [
    "gbt trained for 400 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08711e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # If you have just one model's results:\n",
    "# metrics_pdf = (\n",
    "#     spark.createDataFrame(results, [\"Model\", \"Split\", \"RMSE\", \"MAE\", \"MSE\", \"R2\"])\n",
    "#     .toPandas()\n",
    "#     .pivot(index=\"Model\", columns=\"Split\")\n",
    "# )\n",
    "\n",
    "# from IPython.display import display\n",
    "# display(metrics_pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_y3s1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
